
## OLD: Descriptive assessment {#sec-aa-descriptive}


<!--  
- [ ] Consider using this approach for the new descriptive statistics section
-->

The way to statistically summarize a variable into a single measure is to derive a **measure of central tendency**. For a continuous variable the most common measure is the (arithmetic) *mean*, or average, which is simply the sum of all the values divided by the number of values. As a measure of central tendency, however, the mean can be less-than-reliable as it is sensitive to outliers which is to say that data points in the variable that are extreme relative to the overall distribution of the other values in the variable affect the value of the mean depending on how extreme the deviate. One way to assess the effects of outliers is to calculate a **measure of dispersion**. The most common of these is the *standard deviation* which estimates the average amount of variability between the values in a continuous variable. Another way to assess, or rather side-step, outliers is to calculate another measure of central tendency, the *median*. A median is calculated by sorting all the values in the variable and then selecting the value which falls in the middle of all the other values. A median is less sensitive to outliers as extreme values (if there are few) only indirectly affect the selection of the middle value. Another measure of dispersion is to calculate quantiles. A *quantile* slices the data in four percentile ranges providing a five value numeric summary of the spread of the values in a continuous variable. The spread between the first and third quantile is known as the Interquartile Range (IQR) and is also used as a single statistic to summarize variability between values in a continuous variable.

Below is a list of central tendency and dispersion scores for the continuous variables in the BELC dataset @tbl-summaries-continuous-measures-belc.

```{r}
#| label: tbl-summaries-continuous-measures-belc
#| tbl-cap: "Central tendency and dispersion measures for the continuous variables in the BELC dataset."
#| eval: false

# my_skim <- skim_with(numeric = sfl(hist = NULL, # remove hist from my_skim
#                                    iqr = IQR)) # add IQR to my_skim
# belc |> 
#   select(num_tokens:ttr) |> # select only the continuous variables
#   my_skim() |> # apply my_skim to the BELC dataset
#   yank("numeric") |>  # select the "numeric" columns 
#   select(-n_missing, -complete_rate) |> # remove n_missing and complete_rate columns
#   data.frame() |> # remove the skimr object class
#   kable(booktabs = TRUE) # convert to a kable table
```

In the above summary, we see the mean, standard deviation (sd), and the quantiles (the five-number summary, p0, p25, p50, p75, and p100). The middle quantile (p50) is the median and the IQR is listed last.

These are important measures for assessing the central tendency and dispersion and will be useful for reporting purposes, but to get a better feel of how a variable is distributed, nothing beats a visual summary. A boxplot graphically summarizes many of these metrics. In @fig-summaries-boxplots-belc we see the same three continuous variables, but now in graphical form.

```{r}
#| label: fig-summaries-boxplots-belc
#| fig-cap: 'Boxplots for each of the continuous variables in the BELC dataset.'
#| fig-alt: | 
#|  Boxplots for each of the continuous variables in the BELC dataset.
#|  The boxplots show the median (horizontal line), the interquartile range (box), and the range of the data (whiskers).
#|  The boxplots also show the distribution of the data with points for each observation.
#| eval: false
# p1 <- 
#   belc |> 
#   ggplot(aes(y = num_tokens)) + # select the num_tokens variable
#   geom_boxplot() + # create a boxplot
#   theme(axis.title.x=element_blank(), axis.text.x = element_blank()) + # remove x-axis labels
#   labs(y = "Number of tokens") # add y-axis label

# p2 <- 
#   belc |> 
#   ggplot(aes(y = num_types)) + # select the num_types variable
#   geom_boxplot() + # create a boxplot
#   theme(axis.title.x=element_blank(), axis.text.x = element_blank()) + # remove x-axis labels
#   labs(y = "Number of types") # add y-axis label

# p3 <- 
#   belc |> 
#   ggplot(aes(y = ttr)) + # select the ttr variable
#   geom_boxplot() + # create a boxplot
#   theme(axis.title.x=element_blank(), axis.text.x = element_blank()) + # remove x-axis labels
#   labs(y = "Type-Token Ratio")

# p1 + p2 + p3 # combine plots
```

In a boxplot, the bold line is the median. The surrounding box around the median is the interquantile range. The extending lines above and below the IQR mark the largest and lowest value that is within 1.5 times either the 3rd (top of the box) or 1st (bottom of the box). Any values that fall outside, above or below, the extending lines are considered statistical outliers and are marked as dots. [^1]

[^1]: Note that each of these three variables are to be considered separately here (vertically). Later we will see the use of boxplots to compare a continuous variable across levels of a categorical variable (horizontally).

Boxplots provide a robust and visually intuitive way of assessing central tendency and variability in a continuous variable but this type of plot can be complemented by looking at the overall distribution of the values in terms of their frequencies. A histogram provides a visualization of the frequency (and density in this case with the blue overlay) of the values across a continuous variable binned at regular intervals.

In @fig-summaries-histograms-belc I've plotted histograms in the top row and density plots in the bottom row for the same three continuous variables from the BELC dataset.

```{r}
#| label: fig-summaries-histograms-belc
#| fig-cap: 'Histograms and density plots for the continuous variables in the BELC dataset.'
#| eval: false
# p1 <- 
#   belc |> 
#   ggplot(aes(x = num_tokens)) + 
#   geom_histogram(binwidth = 20, color="black", fill = "white") + 
#   labs(x = "Number of tokens")

# p2 <- 
#   belc |> 
#   ggplot(aes(x = num_types)) + 
#   geom_histogram(binwidth = 10, color="black", fill = "white") +
#   labs(x = "Number of types")

# p3 <- 
#   belc |> 
#   ggplot(aes(x = ttr)) + 
#   geom_histogram(binwidth = .05, color="black", fill = "white") + 
#   labs(x = "Type-Token Ratio")

# p4 <- 
#   belc |> 
#   ggplot(aes(x = num_tokens)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Number of tokens", y = "Density")

# p5 <- 
#   belc |> 
#   ggplot(aes(x = num_types)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Number of types", y = "Density")

# p6 <- 
#   belc |> 
#   ggplot(aes(x = ttr)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Type-Token Ratio", y = "Density")

# gridExtra::grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3, nrow = 2)
```


```{r}
#| label: fig-histogram-boxplot-sim
#| fig-cap: 'Histogram and boxplot for a simulated normal distribution.'
#| fig-alt: 'A histogram and boxplot for a simulated normal distribution.'
#| fig-pos: 'h'
#| echo: false
#| eval: false
# data <- data.frame(values = rnorm(1000, mean = 0, sd = 1))

# # Calculate the Q1, Q2, and Q3 quantiles
# quantiles <- quantile(data$values, probs = c(0.25, 0.5, 0.75))

# # Create the histogram with lines at Q1, Q2, and Q3
# histogram <- ggplot(data, aes(x = values)) +
#   geom_histogram(aes(y = after_stat(density)), bins = 30, fill = "lightblue", color = "black", alpha = 0.7) +
#   geom_vline(aes(xintercept = quantiles[[2]]), color = "red", linetype = "dashed") +
#   geom_vline(aes(xintercept = quantiles[[1]]), color = "blue", linetype = "dashed") +
#   geom_vline(aes(xintercept = quantiles[[3]]), color = "blue", linetype = "dashed") +
#   labs(title = "Histogram with Quantile Lines")

# # Create the boxplot
# boxplot <- ggplot(data, aes(x = 0, y = values)) +
#   geom_boxplot(fill = "lightblue", color = "black", alpha = 0.7) +
#   labs(title = "Boxplot") + 
#   geom_rug() +
#   coord_flip()

# # Combine the plots using patchwork
# combined_plot <- histogram / boxplot + plot_layout(guides = "collect")
# print(combined_plot)
```

Histograms provide insight into the distribution of the data. For our three continuous variables, the distributions happen not to be too strikingly distinct. They are, however, not the same either. When we explore continuous variables with histograms we are often trying to assess whether there is skew or not. There are three general types of skew, visualized in @fig-summaries-skew-graphic.

```{r}
#| label: fig-summaries-skew-graphic
#| fig-cap: 'Examples of skew types in density plots.'
#| eval: false
#  knitr::include_graphics("figures/approaching-analysis/skew-types-paper.png")
```

```{r}
#| label: fig-summaries-skew-graphic-2
#| fig-cap: 'Examples of skew types in density plots.' 

# Generate 1 million samples of three types of data
# left_values: left skewed data
# normal_values: data following normal distribution
# right_values: right skewed data
#| eval: false
# set.seed(1234) # set seed for reproducibility

# # Create tibble with three columns of skew data
# skew_examples <- 
#   tibble(left_values = rbeta(10000000, 5, 2), 
#          normal_values = rbeta(10000000, 5, 5),
#          right_values = rbeta(10000000, 2, 5))

# # Function to calculate the mode
# get_mode <- function(x) {
#   ux <- unique(x)
#   ux[which.max(tabulate(match(x, ux)))]
# }

# # Plot left skew in a density plot with lines for mean, median, and mode
# p1 <- 
#   skew_examples |> 
#   ggplot(aes(x = left_values)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Value", y = "Density") + 
#   geom_vline(aes(xintercept = mean(left_values)), color = "red", linetype = "dashed") + 
#   geom_vline(aes(xintercept = median(left_values)), color = "blue", linetype = "dashed") + 
#   geom_vline(aes(xintercept = get_mode(left_values)), color = "green", linetype = "dashed")

# # Plot normal distribution in a density plot with lines for mean, median, and mode
# p2 <- 
#   skew_examples |> 
#   ggplot(aes(x = normal_values)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Value", y = "Density") + 
#   geom_vline(aes(xintercept = mean(normal_values)), color = "red", linetype = "dashed") + 
#   geom_vline(aes(xintercept = median(normal_values)), color = "blue", linetype = "dashed") + 
#   geom_vline(aes(xintercept = get_mode(normal_values)), color = "green", linetype = "dashed")

# # Plot right skew in a density plot with lines for mean, median, and mode
# p3 <- 
#   skew_examples |> 
#   ggplot(aes(x = right_values)) + 
#   geom_density(alpha=.2, fill="#7dced8", color = "#7dced8") + 
#   labs(x = "Value", y = "Density") + 
#   geom_vline(aes(xintercept = mean(right_values)), color = "red", linetype = "dashed") + 
#   geom_vline(aes(xintercept = median(right_values)), color = "blue", linetype = "dashed") + 
#   geom_vline(aes(xintercept = get_mode(right_values)), color = "green", linetype = "dashed")

# p1 + p2 + p3 # arrange plots in a row
```

In histograms/ density plots in which the distribution is either left or right, the median and the mean are not aligned. The *mode*, which indicates the most frequent value in the variable is also not aligned with the other two measures. In a left-skewed distribution the mean will be to the left of the median which is left of the mode whereas in a right-skewed distribution the opposite occurs. In a distribution with absolutely no skew these three measures are the same. In practice these measures rarely align perfectly but it is very typical for these three measures to approximate alignment. It is common enough that this distribution is called the Normal Distribution [^2] as it is very common in real-world data.

[^2]: formally known as a Gaussian Distribution

----


Now let's turn to the descriptive assessment of categorical variables. For categorical variables, central tendency can be calculated as well but only a subset of measures given the reduced informational value of categorical variables. For nominal variables where there is no relationship between the levels the central tendency is simply the mode. The levels of ordinal variables, however, are relational and therefore the median, in addition to the mode, can also be used as a measure of central tendency. Note that a variable with one mode is unimodal, two modes, bimmodal, and in variables that have two or more modes multimodal.

Below is a list of the central tendency metrics for the categorical variables in the BELC dataset.

```{r}
#| label: summaries-categorical-measures-belc
#| eval: false

# belc |> 
#   select(participant_id:sex) |> # select only the categorical variables
#   skim() |> # get the summary statistics
#   yank("factor") |> # get the factor variables
#   select(-n_missing, -complete_rate) # remove the n_missing and complete_rate columns
```

In practice when a categorical variable has few levels it is common to simply summarize the counts of each level in a table to get an overview of the variable. With ordinal variables with more numerous levels, the five-score summary (quantiles) can be useful to summarize the distribution. In contrast to continuous variables where a graphical representation is very helpful to get perspective on the shape of the distribution of the values, the exploration of single categorical variables is rarely enhanced by plots.

#### Multiple variables

In addition to the single variable summaries (univariate), it is very useful to understand how two (bivariate) or more variables (multivariate) are related to add to our understanding of the shape of the relationships in the dataset. Just as with univariate summaries, the informational values of the variables frame our approach.

To explore the relationship between two continuous variables we can statistically summarize a relationship with a **coefficient of correlation** which is a measure of **effect size** between continuous variables. If the continuous variables approximate the normal distribution *Pearson's r* is used, if not *Kendall's tau* is the appropriate measure. A correlation coefficient ranges from -1 to 1 where 0 is no correlation and -1 or 1 is perfect correlation (either negative or positive). Let's assess the correlation coefficient for the variables `num_tokens` and `ttr`. Since these variables are not normally distributed, we use Kendall's tau. Using this measure the correlation coefficient is $`r cor(belc$num_tokens, belc$ttr, method = "kendall")`$ suggesting there is a correlation, but not a particularly strong one.

Correlation measures are important for reporting but to really appreciate a relationship it is best to graphically represent the variables in a *scatterplot*. In @fig-summaries-bivariate-scatterplot-belc we see the relationship between `num_tokens` and `ttr`.

```{r}
#| label: fig-summaries-bivariate-scatterplot-belc
#| fig-cap: 'Scatterplot...'
#| eval: false

# p1 <- 
#   belc |> 
#   ggplot(aes(x = num_tokens, y = ttr)) + 
#   geom_point() +
#   labs(x = "Number of tokens", y = "Type-Token Ratio", title = "Data points")

# p2 <- p1 + geom_smooth(method = "lm") + labs(title = "Linear trend line and confidence interval", y = "")

# gridExtra::grid.arrange(p1, p2, ncol = 2)
```

In both plots `ttr` is on the y-axis and `num_tokens` on the x-axis. The points correspond to the intersection between these variables for each single observation. In the left pane only the points are represented. Visually (and given the correlation coefficient) we can see that there is a negative relationship between the number of tokens and the Type-Token ratio: in other words, the more tokens a composition has the lower the Type-Token Ratio. In this case this trend is quite apparent, but in other cases is may not be. To provide an additional visual cue a trend line is often added to a scatterplot. In the right pane I've added a linear trend line. This line demarcates the optimal central tendency across the relationship, assuming a linear relationship. The steeper the line, or slope, the more likely the correlation is strong. The band, or ribbon, around this trend line indicates the **confidence interval** which means that real central tendency could fall anywhere within this space. The wider the ribbon, the larger the variation between the observations. In this case we see that the ribbon widens when the number of tokens is either low or high. This means that the trend line could be potentially be drawn either steeper (more strongly correlated) or flatter (less strongly correlated).

::: {.callout}
**{{< fa regular hand-point-up >}} Tip**

In plots comparing two or more variables, the choice of which variable to plot on the x- and y-axis is contingent on the research question and/ or the statistical approach. The language varies between statistical approaches: in inferential methods the x-axis is used to plot what is known as the dependent variable and the y-axis an independent variable. In predictive methods the dependent variable is known as the outcome and the independent variable a predictor. Exploratory methods do not draw distinctions between variables along these lines so the choice between which variable to plot along the x- and y-axis is often arbitrary.
:::

Let's add another variable to the mix, in this case the categorical variable `sex`, taking our bivariate exploration to a multivariate exploration. Again each point corresponds to an observation where the values for `num_tokens` and `ttr` intersect. But now each of these points is given a color that reflects which level of `sex` it is associated with.

```{r}
#| label: fig-summaries-multivariate-scatterplot-belc
#| fig-cap: 'Scatterplot visualizing the relationship between `num_tokens` and `ttr`.'
#| eval: false
# p1 <- 
#   belc |> 
#   ggplot(aes(x = num_tokens, y = ttr, color = sex)) + 
#   geom_point() + 
#   labs(x = "Number of tokens", y = "Type-Token Ratio", color = "Sex")

# p2 <- p1 + geom_smooth(method = "lm") + labs(y = "")

# p1 <- p1 + theme(legend.position = "none")

# gridExtra::grid.arrange(p1, p2, ncol = 2)
```

In this multivariate case, the scatterplot without the trend line is more difficult to interpret. The trend lines for the levels of `sex` help visually understand the variation of the relationship of `num_tokens`and `ttr` much better. But it is important to note that when there are multiple trend lines there is more than one slope to evaluate. The correlation coefficient can be calculated for each level of `sex` (i.e. 'male' and 'female') independently but the relationship between the each slope can be visually inspected and provide important information regarding each level's relative distribution. If the trend lines are parallel (ignoring the ribbons for the moment), as it appears in this case, this suggests that the relationship between the continuous variables is stable across the levels of the categorical variable, with males showing more lexical diversity than females declining at a similar rate. If the lines were to cross, or suggest that they would cross at some point, then there would be a potentially important difference between the levels of the categorical variable (known as an interaction). Now let's consider the meaning of the ribbons. Since the ribbons reflect the range in which the real trend line could fall, and these ribbons overlap, the differences between the levels of our categorical variable are likely not distinct. So at a descriptive level, this visual summary would suggest that there are no differences between the relationship between `num_tokens` and `ttr` for the distinct levels of `sex`.

Characterizing the relationship between two continuous variables, as we have seen is either performed through a correlation coefficient metric or visually. The approach for summarizing a bivariate relationship which combines a continuous and categorical variable is distinct. Since a categorical variable is by definition a class-oriented variable, a descriptive evaluation can include a tabular representation, with some type of summary statistic. For example, if we consider the relationship between `num_tokens` and `age_group` we can calculate the mean for `num_tokens` for each level of `age_group`. To provide a metric of dispersion we can include either the standard error of the mean (SEM) and/ or the confidence interval (CI).

```{r}
#| label: tbl-summarize-bivariate-cont-cat-belc
#| tbl-cap: 'Summary table for `tokens` by `age_group`.'
#| eval: false
# belc |> 
#   group_by(age_group) |> 
#   summarize(mean_num_tokens = mean(num_tokens),
#             sem = sd(num_tokens)/ sqrt(n()),
#             ci = qnorm(.95)*(sd(num_tokens)/ sqrt(n()))) |> 
#   ungroup() |> 
#   kable(booktabs = TRUE)
```

The SEM is a metric which summarizes variation based on the number of values and the CI, as we have seen, summarizes the potential range of in which the mean may fall given a likelihood criterion (usually the same as the $p$-value, .05).

Because we are assessing a categorical variable in combination with a continuous variable a table is an available visual summary. But as I have said before, a graphic summary is hard to beat. In the following figure (@fig-summaries-bivariate-barplot-belc) a barplot is provided which includes the means of `num_tokens` for each level of `age_group`. The overlaid bars represent the confidence interval for each mean score.

```{r}
#| label: fig-summaries-bivariate-barplot-belc
#| fig-cap: 'Barplot comparing the mean `num_tokens` by `age_group` from the BELC dataset.'
#| eval: false

# p1 <- 
#   belc |> 
#   group_by(age_group) |> 
#   summarize(mean_num_tokens = mean(num_tokens),
#             sem = sd(num_tokens)/ sqrt(n()),
#             ci = qnorm(.95)*(sd(num_tokens)/ sqrt(n()))) |> 
#   ungroup() |> 
#   ggplot(aes(x = age_group, y = mean_num_tokens)) + 
#   geom_col() +
#   ylim(0, 140) +
#   labs(x = "Age group", y = "Number of tokens (mean)")

# p2 <- 
#   belc |> 
#   group_by(age_group) |> 
#   summarize(mean_num_tokens = mean(num_tokens),
#             sem = sd(num_tokens)/ sqrt(n()),
#             ci = qnorm(.95)*(sd(num_tokens)/ sqrt(n()))) |> 
#   ungroup() |> 
#   ggplot(aes(x = age_group, y = mean_num_tokens)) + 
#   geom_col() +
#   geom_errorbar(aes(ymin = mean_num_tokens-ci, ymax = mean_num_tokens+ci),
#                 width = .5) +
#   ylim(0, 140) +
#   labs(x = "Age group", y = "")

# gridExtra::grid.arrange(p1, p2, ncol = 2)
```

When CI ranges overlap, just as with ribbons in scatterplots, the likelihood that the differences between levels are 'real' is diminished.

To gauge the effect size of this relationship we can use *Kendall's Ï„* for rank-based coefficients. The score is `r cor(as.numeric(belc$age_group), belc$num_tokens, method = "spearman")` indicating that the relationship between `age_group` and `num_tokens` is quite strong. [^3]

[^3]: To calculate effect sizes for the difference between two means, *Cohen's d* is used.

Now, if we want to explore a multivariate relationship and add `sex` to the current descriptive summary, we can create a summary table, but let's jump straight to a barplot.

```{r}
#| label: fig-summaries-multivariate-barplot-belc
#| fig-cap: 'Barplot comparing the mean `num_tokens` by `age_group` and `sex` from the BELC dataset.'
#| eval: false
# belc |> 
#   group_by(age_group, sex) |> 
#   summarize(mean_num_tokens = mean(num_tokens),
#             sem = sd(num_tokens)/ sqrt(n()),
#             ci = qnorm(.95)*(sd(num_tokens)/ sqrt(n()))) |> 
#   ungroup() |> 
#   ggplot(aes(x = age_group, y = mean_num_tokens, fill = sex)) + 
#   geom_col(position = position_dodge()) +
#   geom_errorbar(aes(ymin = mean_num_tokens-ci, ymax = mean_num_tokens+ci),
#                 width = .5,
#                 position = position_dodge(.9)) +
#   labs(x = "Age group", y = "Number of tokens (mean)", fill = "Sex")
```

We see in @fig-summaries-multivariate-barplot-belc that on the whole, the appears to be general trend towards more tokens in a composition for more advanced learner levels. However, the non-overlap in CI bars for the '12-year-olds' for the levels of `sex` ('male' and 'female') suggest that 12-year-old females may produce more tokens per composition than males --a potential divergence from the overall trend.

Barplots are a familiar and common visualization for summaries of continuous variables across levels of categorical variables, but a boxplot is another useful visualization of this type of relationship.

```{r}
#| label: fig-summaries-bivariate-boxplots-belc
#| fig-cap: 'Boxplot of the relationship between `age_group` and `num_tokens` from the BELC dataset.'
#| eval: false
# p1 <- 
#   belc |> 
#   ggplot(aes(x = age_group, y = num_tokens)) + 
#   geom_boxplot(outlier.color = "red", notch = FALSE) +
#   labs(x = "Age group", y = "Number of tokens")

# p2 <- 
#   belc |> 
#   ggplot(aes(x = age_group, y = num_tokens)) + 
#   geom_boxplot(outlier.color = "red", notch = TRUE) +
#   labs(x = "Age group", y = "")

# gridExtra::grid.arrange(p1, p2, ncol = 2)
```

As seen when summarizing single continuous variables, boxplots provide a rich set of information concerning the distribution of a continuous variable. In this case we can visually compare the continuous variable `num_tokens` with the categorical variable `age_group`. The plot in the right pane includes 'notches'. Notches represent the confidence interval, in boxplots this interval surrounds the median. When compared horizontally across levels of a categorical variable the overlap of notched spaces suggest that the true median may be within the same range. Additionally, when the confidence interval goes outside the interquantile range (the box) the notches hinge back to the either the 1st (lower) or the 3rd (higher) IQR range and suggests that the variability is high.

We can also add a third variable to our exploration. As in the barplot in @fig-summaries-multivariate-barplot-belc, the boxplot in @fig-summaries-multivariate-boxplots-belc suggests that there is an overall trend towards more tokens per composition as a learner advances in experience, except at the '12-year-old' level where there appears to be a difference between 'males' and 'females'.

```{r}
#| label: fig-summaries-multivariate-boxplots-belc
#| fig-cap: 'Boxplot of the relationship between `age_group`, `num_tokens` and `sex` from the BELC dataset.'
#| eval: false
  # belc |> 
  # ggplot(aes(x = age_group, y = num_tokens, fill = sex)) + 
  # geom_boxplot(outlier.color = "red", notch = TRUE) +
  # labs(x = "Age group", y = "Number of tokens", fill = "Sex")
```

Up to this point in our exploration of multiple variables we have always included at least one continuous variable. The central tendency for continuous variables can be summarized in multiple ways (mean, median, and mode) and when calculating means and medians, measures of dispersion are also provide helpful information summarize variability. When working with categorical variables, however, measures of central tendency and dispersion are more limited. For ordinal variables central tendency can be summarized by the median or mode and dispersion can be assessed with an interquantile range. For nominal variables the mode is the only measure of central tendency and dispersion is not applicable. For this reason relationships between categorical variables are typically summarized using **contingency tables** which provide cross-variable counts for each level of the target categorical variables.

Let's explore the relationship between the categorical variables `sex` and `age_group`. In @tbl-summaries-bivariate-categorical-table-belc we see the contingency table with summary counts and percentages.

```{r}
#| label: tbl-summaries-bivariate-categorical-table-belc
#| tbl-cap: "Contingency table for `age_group` and `sex`."
#| eval: false
# belc |> 
#   tabyl(sex, age_group) |> 
#   adorn_totals(c("row", "col")) |>
#   adorn_percentages("col") |> 
#   adorn_pct_formatting(rounding = "half up", digits = 0) |>
#   adorn_ns() |>
#   adorn_title("combined") |> 
#   kable(booktabs = TRUE)
```

As the size of the contingency table increases, visual inspection becomes more difficult. As we have seen, a graphical summary often proves more helpful to detect patterns.

```{r}
#| label: fig-summaries-bivariate-categorical-barplot-belc
#| fig-cap: 'Barplot...'
#| eval: false
# p1 <- 
#   belc |> 
#   ggplot(aes(x = age_group, fill = sex)) +
#   geom_bar() +
#   labs(y = "Count", x = "Age group")

# p2 <- 
#   belc |> 
#   ggplot(aes(x = age_group, fill = sex)) +
#   geom_bar(position = "fill") +
#   labs(y = "Proportion", x = "Age group", fill = "Sex")

# p1 <- p1 + theme(legend.position = "none") 

# chi_test <- 
#   table(belc$sex, belc$age_group) |> 
#   chisq.test()
# cv <- effectsize(chi_test)

# gridExtra::grid.arrange(p1, p2, ncol = 2)
```

In @fig-summaries-bivariate-categorical-barplot-belc the left pane shows the counts. Counts alone can be tricky to evaluate and adjusting the barplot to account for the proportions of males to females in each group, as shown in the right pane, provides a clearer picture of the relationship. From these barplots we can see there were more females in the study overall and particularly in the 12-year-olds and 17-year-olds groups. To gauge the association strength between `sex` and `age_group` we can calculate *Cramer's V* which, in spirit, is like our correlation coefficients for the relationship between continuous variables. The Cramer's V score for this relationship is `r cv$Cramers_v` which is low, suggesting that there is not a strong association between `sex` and `age_group` --in other words, the relationship is stable.

Let's look at a more complex case in which we have three categorical variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous `num_tokens` variable as a categorical variable if we bin the scores into groups. I've binned tokens into three score groups with equal ranges in a new variable called `rank_tokens`.

Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. Our numerical summary will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable, in this case `sex`.

```{r}
#| label: tbl-summaries-multivariate-categorical-table-belc-female
#| tbl-cap: 'Contingency table for `age_group`, `rank_tokens`, and `sex` (female).'
#| eval: false
# tab <- 
#   belc |> 
#   mutate(rank_tokens = cut(num_tokens, breaks = 3, labels = c("low", "mid", "high"))) |> 
#   select(participant_id, age_group, sex, rank_tokens) |> 
#   tabyl(rank_tokens, age_group, sex) |> 
#   adorn_totals(c("row", "col")) |>
#   adorn_percentages("all") |> 
#   adorn_pct_formatting(rounding = "half up", digits = 0) |>
#   adorn_ns() |>
#   adorn_title("combined")

# tab$female |> 
#   kable(booktabs = TRUE)
```

```{r}
#| label: tbl-summaries-multivariate-categorical-table-belc-male
#| tbl-cap: 'Contingency table for `age_group`, `rank_tokens`, and `sex` (male).'
#| eval: false
tab$male |> 
    kable(booktabs = TRUE)
```

Contingency tables with this many levels are notoriously difficult to interpret. A plot that is often used for three-way contingency table summaries is a mosaic plot. In @fig-summaries-multivariate-mosaic-belc I have created a mosaic plot for the three categorical variables in the previous contingency tables.

```{r}
#| label: fig-summaries-multivariate-mosaic-belc
#| fig-cap: 'Mosaic plot for three categorical variables `age_group`, `rank_tokens`, and `sex` in the BELC dataset.'
#| eval: false

# belc_cats <- 
#   belc |> 
#   mutate(rank_tokens = cut(num_tokens, breaks = 3, labels = c("low", "mid", "high")))

# mosaicplot(~ sex + age_group + rank_tokens, data = belc_cats, xlab = c("Sex"), ylab = c("Age group"), main = "")
```

The mosaic plot suggests that the number of tokens per composition increase as the learner age group increases and that females show more tokens earlier.