---
execute:
  echo: false
---

# Analysis {#sec-approaching-analysis}

```{r}
#| label: setup-options
#| child: "_common.qmd"
#| cache: false
```

> Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.
>
> --- H.G. Wells

::: {.callout}
**{{< fa regular list-alt >}} Outcomes**

- Recall the fundamental concepts and principles of statistics in data analysis.
- Articulate the roles of diagnostic, analytic, and interpretive statistics in quantitative analysis.
- Compare the similarities and differences between analytic approaches to data analysis.
:::

```{r}
#| label: aa-packages
pacman::p_load(skimr, janitor, effectsize, tidytext, forcats)
```

The aim of analysis is to derive knowledge from information, the next step in the DIKI Hierarchy. Where the creation of information from data involves human intervention and conscious decisions, as we have seen, deriving knowledge from information involves another level of intervention. The goal is to break down complex information into simpler components which are more readily interpretable.

In what follows, we will cover the main steps in the process of analysis. The first is to inspect the data to ensure its quality and understand its characteristics. The second is to interrogate the data to uncover patterns and relationships and interpret the findings. To conclude this chapter, I will outline methods to and the importance of communicating the analysis results and procedure in a transparent and reproducible manner.

::: {.callout}
**{{< fa terminal >}} Lessons**

**What**: [Summarizing data, Visual summaries](https://github.com/qtalr/lessons)\
**How**: In the R Console pane load `swirl`, run `swirl()`, and follow prompts to select the lesson.\
**Why**: To showcase methods for statistical summaries of vectors and data frames and to create informative graphics that enhance data interpretation and analysis.
:::

<!-- Set up the dataset/ dictionary -->

```{r}
#| label: aa-belc
#| eval: false

# TalkBank API
library(TBDBr)

# Talkbank: BELC
corpus_name <- "slabank"
corpora <- c("slabank", "English", "BELC", "1-written(4t)_10-16")

# Get tokens ----
belc_tokens_tbl <-
  getTokens(
    corpusName = corpus_name, # corpus name
    corpora = corpora
  ) |> # corpus path
  unnest(everything()) # unnest variables

# Get participants ----
belc_participants_tbl <-
  getParticipants(
    corpusName = corpus_name, # corpus name
    corpora = corpora
  ) |> # corpus path
  unnest(everything()) # unnest variables

# Join tokens and participants ----
belc_parts_tokens_tbl <-
  left_join(belc_participants_tbl, belc_tokens_tbl)

# Wrangling steps ----
belc_tbl <-
  belc_parts_tokens_tbl |>
  # separate time_group and part_id
  separate(filename, c("time_group", "part_id"), sep = "c") |>
  # replace "A" with "T" in time_group labels
  mutate(time_group = str_remove(time_group, "A")) |>
  # filter out time_group "2B"
  filter(time_group != "2B") |>
  mutate(time_group = str_c("T", time_group, sep = "")) |>
  # remove redundant variables
  select(-path, -who, -name, -role, -language, -age) |>
  # select names and order
  select(part_id, sex,
    group = time_group,
    month_age = monthage, num_words = numwords,
    num_utts = numutts, avg_utt = avgutt, median_utt = medianutt,
    utt_id = uid, word_id = wordnum, word, lemma = stem, pos
  ) |>
  # subset columns
  select(part_id:month_age, utt_id:pos) |>
  # arrange observations
  arrange(part_id, utt_id, word_id) |>
  # impute missing values
    mutate(lemma = case_when(
    pos == "n" & is.na(lemma) ~ word,
    TRUE ~ lemma
  )) |>
  # lemma not: I, football, basketball, english
  mutate(pos = case_when(
    is.na(pos) & !(word %in% c("I", "football", "basketball", "english")) ~ "L2",
    is.na(pos) ~ "n",
    TRUE ~ pos
  )) |>
  # remaining empty lemmas from word
  mutate(lemma = case_when(
    is.na(lemma) ~ word,
    TRUE ~ lemma
  )) |>
  # adjust numberic variables
    # adjust utt_id and word_id
  mutate(
    utt_id = utt_id + 1,
    word_id = word_id + 1
  )

belc_essay_tbl <-
  belc_tbl |>
  # group by essay
  group_by(part_id, sex, group) |>
  # summarize data
  summarize(
    # number of words
    tokens = n(),
    # number of unique words
    types = n_distinct(word),
    # number l1 tokens
    l1_tokens = sum(str_count(pos, "L2"))
  ) |>
  # ungroup by essay
  ungroup() |>
  mutate(
    # proportion of L2 to L1 words
    prop_l2 = 1 - round((l1_tokens / tokens), 3),
    # type/ token ratio
    ttr = round((types / tokens), 3),
    # assign number to essays
    essay_id = str_c("E", row_number(), sep = "")
  ) |>
  # select variables
  select(essay_id, part_id:types, ttr, prop_l2)

belc_essay_tbl <-
  belc_essay_tbl |>
  mutate(
    across(
      where(is.character),
      factor
    )
  ) |>
  mutate(group = fct_inorder(group, ordered = TRUE)) |>
  mutate(group = fct_relevel(group, "T1", "T2", "T3", "T4"))

# Write data ----
write_rds(belc_essay_tbl, "data/aa-belc_essay_tbl.rds")

# Create data dictionary ----
create_data_dictionary(belc_essay_tbl, "data/aa-belc_essay_tbl_dd.csv", model = "gpt-3.5-turbo")
```

<!-- Load dataset/ origin/ dictionary -->

```{r}
#| label: aa-belc-dataset-data-dictionary
#| message: false

# Dataset
belc_essay_tbl <- read_rds("data/aa-belc_essay_tbl.rds")

# Data dictionary
belc_essay_dd <- read_csv("data/aa-belc_essay_tbl_dd.csv")
```

## Describe {#sec-aa-describe}

<!-- Purpose -->

The goal of descriptive statistics is to summarize the data in order to understand and prepare the data for the analysis approach to be performed. This is accomplished through a combination of statistic measures and/ or tabular or graphic summaries. The choice of descriptive statistics is guided by the type of data, as well as the question(s) being asked of the data.

In descriptive statistics, there are four basic questions that are asked of each of the variables in the dataset. Each correspond to a different type of descriptive measure.

1. **Central Tendency**: Where do the data points tend to be located?
2. **Dispersion**: How spread out are the data points?
3. **Distribution**: What is the overall shape of of the data points?
4. **Association**: How are these data points related to other data points?

<!-- Dataset used to groud the discussion -->

To ground this discussion I will introduce a new dataset. This dataset is drawn from the Barcelona English Language Corpus (BELC) [@Munoz2006], which is found in the TalkBank repository. I've selected the "Written composition" task from this corpus which contains 80 writing samples from 36 second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of "Me: my past, present and future". Data was collected for participants from one to three times over the course of seven years (at 10, 12, 16, and 17 years of age).


In @tbl-aa-belc-dd we see the data dictionary for the BELC dataset which reflects structural and transformational steps I've done so we start with a tidy dataset with `essay_id` as the unit of observation.

```{r}
#| label: tbl-aa-belc-dd
#| tbl-cap: "Data dictionary for the BELC dataset."
#| tbl-colwidths: [13, 17, 15, 55]

# Data dictionary ----
belc_essay_dd |>
  tt(width = 1)
```

The data dictionary provides a easily accessible overview of the dataset. This includes a human-readable mapping from variable names to variable descriptions. Further, it provides information about the type of variable (e.g., categorical, ordinal, numeric). As we will see the informational type of variables is key to descriptive measures, as well as all other components of analysis.

Now, let's take a look a the first few observations of the BELC dataset to get another perspective on the dataset as we view the values of the dataset.

```{r}
#| label: tbl-aa-belc-overview
#| tbl-cap: "First 5 observations of the BELC dataset."
#| tbl-colwidths: [12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5, 12.5]

# View data ----
belc_essay_tbl |>
  # select first 10 observations
  slice_head(n = 5) |>
  # print table
  tt(width = 1)
```

In @tbl-aa-belc-overview, each of the variable are attributes or measures of the `essay_id` variable. `tokens` is the number of total words, `types` is the number of unique words, `ttr` is the ratio of unique words to total words. This is known as the Type-Token Ratio and it is a standard metric for measuring lexical diversity. Finally, the proportion of L2 words (English) to the total words (tokens) is provided in `prop_l2`.

::: {.callout}
**{{< fa regular file-alt >}} Case study**

Type-Token Ratio is a standard metric for measuring lexical diversity, but it is not without its flaws. Most importantly, TTR is highly sensitive to the word length of the text. @Duran2004 discuss this limitation, and the limitations of other lexical diversity measures and propose a new measure $D$ which shows a stronger correlation with language proficiency in their comparative studies.
:::

Let's now turn our attention to exploring descriptive measures using the BELC dataset.

### Central tendency {#sec-aa-central-tendency}

<!-- Location -->
<!-- - Central tendency (mean, median, mode) -->

```{r}
#| label: aa-belc-descriptive-functions

# Function: calculate the mode ----
calculate_mode <- function(x) {
  x |>
    # convert to tibble
    as_tibble() |>
    # count values
    count(value) |>
    # select most frequent value
    filter(n == max(n)) |>
    # pull value
    pull(value)
}

# Function: calculate the normalized entropy ----
calculate_norm_entropy <- function(x) {
  # add NA to x
  x <- addNA(x, ifany = TRUE)
  # get value proportions
  prop <- prop.table(table(x))
  # calculate entropy
  entropy <- -sum(prop * log2(prop))
  # calculate max entropy
  max_entropy <- log2(length(prop))
  # calculate normalized entropy
  normalized_entropy <- entropy / max_entropy
  return(normalized_entropy)
}

# Skim function ----
aa_skim <- skim_with(
  factor = sfl(top_counts = top_counts, norm_entropy = calculate_norm_entropy),
  character = sfl(top_counts = top_counts, norm_entropy = calculate_norm_entropy),
  numeric = sfl(mean = mean, median = median, sd = sd, iqr = IQR),
  append = FALSE
)
```

The central tendency is measure which aims to summarize the data points in a variable as the most representative, middle or most typical value. There are three common measures of central tendency: the mode, mean and median. Each differ in how they summarize the data points.

The **mode** is the value, or values, that appears most frequently in a set of values. If there are multiple values with the highest frequency, then the variable is said to be multimodal. The most versatile of the central tendency measures as it can be applied to all levels of measurement, although the mode is not often used for numeric variables as it is not as informative as other measures.

::: {.callout}
**{{< fa lightbulb >}} Consider this**

::: {layout="[60, -3, 40]" layout-valign="top" layout-align="left"}

::: {#first}
@Grieve2018 compiled a 8.9 billion-word corpus of geotagged posts from Twitter between 2013-2014 in the United States. The authors provide a [search interface](https://isogloss.shinyapps.io/isogloss/) to explore relationship between lexical usage and geographic location. Explore this corpus searching for terms related to slang ("hella", "wicked"), geographical ("mountain", "river"), meteorological ("snow", "rain"), and/ or any other term types. What types of patterns do you find? What are the benefits and/ or limitations of this type of data, data summarization, and/ or interface?
:::

::: {#second}
```{r}
#| label: fig-ud-word-mapper
#| fig-cap: "Example distribution of the term 'Ya'll' the Word Mapper project."
#| out-width: '80%'
#| echo: false

include_graphics("figures/ud-word-mapper.png")
```
:::

:::

:::


The more common measures for numeric variables are the mean and the median. The **mean** is a summary statistic calculated by summing all the values and dividing by the number of values. The **median** is calculated by sorting all the values in the variable and then selecting the middle value. Given that the mean and median are calculated differently, they will not always yield the same result. Differences that appear between the mean and median will be of interest to us later in this chapter.

### Dispersion

The mean, median, and mode provide summary information where data points tend to be located. However, they do not provide us with any understanding as to how representative this value is. To provide this context, the spread of the values around the central tendency, or **dispersion**, is calculated.

For categorical variables, the spread is framed in terms of how balanced the values are across the levels. One way to do this is to calculate the (normalized) entropy. **Entropy** is a measure of uncertainty. The more balanced the values are across the levels, the closer entropy is 1. In practice, however, proportions are often used to assess the balance of the values across the levels. The **proportion** of each level is the frequency of the level divided by the total number of values.

The most common measure of dispersion for numeric variables is the **standard deviation**. The standard deviation is calculated by taking the square root of the variance. The **variance** is the average of the squared differences from the mean. So, more succinctly, the standard deviation is a measure of the spread of the values around the mean. Where the standard deviation is anchored to the mean, the **interquartile range** (IQR) is tied to the median. The median represents the sorted middle of the values, in other words the 50th percentile. The IQR is the difference between the 75th percentile and the 25th percentile. Again, just as the mean and the median, the standard deviation and the IQR are calculated in different ways, they are not always the same.

Let's now consider the relevant central tendency and dispersion of the variables in the BELC dataset. In @tbl-aa-belc-descriptive-stats-categorical, we see the categorical measures.

```{r}
#| label: tbl-aa-belc-descriptive-stats-categorical
#| tbl-cap: "Central tendency and dispersion of the categorical variables"

belc_essay_tbl |>
  # custom skim function
  aa_skim() |>
  yank("factor") |>
  select(-n_missing, -complete_rate) |>
  select(variable = skim_variable, everything()) |>
  tibble() |>
  tt(width = 1, digits = 2)
```


In @tbl-aa-belc-descriptive-stats-categorical, the `top_counts` measure gives us a short list of the most frequent levels of the variable. From `top_count` we can gather whether the variable has one mode or is multimodel. Both `essay_id` and `part_id` have the same most frequent value for the levels listed. On the other hand, `sex` and `group` have a single mode. We can also appreciate the dispersion of these variables based on the `norm_entropy` of each variable. `essay_id` is completely balanced across the levels, so it has a normalized entropy of 1. the other variables are not as balanced, but still quite balanced as the normalized entropy is close to 1.

In @tbl-aa-belc-descriptive-stats-numeric, we see the numeric measures.

```{r}
#| label: tbl-aa-belc-descriptive-stats-numeric
#| tbl-cap: "Central tendency and dispersion for numeric variables."

belc_essay_tbl |>
  aa_skim() |>
  yank("numeric") |>
  select(-n_missing, -complete_rate) |>
  select(variable = skim_variable, everything()) |>
  tibble() |>
  select(variable, mean, sd, median, iqr) |>
  tt(width = 1, digits = 2)
```

In @tbl-aa-belc-descriptive-stats-numeric, we have meansures for the mean, median, standard deviation, and IQR for each variable. The variable `tokens` has a larger difference between the mean and median than the other variables and the standard deviation is relatively large suggesting that the values are more spread out around the mean. In the case of `ttr` the mean and median are quite close and the standard deviation is relatively small suggesting that the values are more tightly clustered around the mean.

When interpreting these numeric summary values, it is important to only directly compare column-wise. That is, focusing only on a single variable, not across variables. Each variable, as is, is measured on a different scale and only relative to itself can we make sense of the values.

::: {.callout}
**{{< fa medal >}} Dive deeper**

The inability to compare summary statistics across variables is a key reason why **standardization** is often applied before submiting a dataset for analysis [@Johnson2008; @Baayen2008a].

Standardization is a scale-based transformation that changes the scale of the values to a common scale, or *z-scores*. The result of this transformation puts data points of each variable on the same scale and allows for direct comparison. Furthermore, standardization also mitigates the influence of variables with large values relative to other variables. This is particularly important in multivariate analysis where the influence of variables with large values can be magnified.

The caveat is that standardization masks the original meaning of the data. That is, if we consider token frequency, before standardization, we can say that a value of 1000 tokens is 1000 tokens. After standardization, we can only say that a value of 1 is 1 standard deviation from the mean. This is why standardization is often applied after the descriptive phase of analysis.
:::

Another thing to note in @tbl-aa-belc-descriptive-stats-numeric is that the mean and median are not the same. They are both measures of central tendency, but the mean is more sensitive to extreme values than the median. The difference between the mean and median is a numeric representation that gives us a first glimpse of the distribution of the data. With this in mind, let's turn to the distribution of the variables.

### Distributions

<!-- - Distributions  -->

Summary statistics of the central tendency and dispersion of a variable provide a sense of the most representative value and how spread out the data is around this value. However, to gain a more comprehensive understanding of the variable, it is key to consider the frequencies of all the data points. The **distribution** of a variable is the pattern or shape of the data that emerges when the frequencies of all data points are considered. This can reveal patterns that might not be immediately apparent from summary statistics alone.

When assessing the distribution of categorical variables, we can use a frequency table or bar plot. A **frequency table** is a useful method to display the frequency and proportion of each level in a categorical variable in a clear and concise manner. In @tbl-aa-belc-frequency-table we see the frequency table for the variable `sex`.

<!-- sex frequency table -->

```{r}
#| label: tbl-aa-belc-frequency-table
#| tbl-cap: "Frequency table for the variable `sex`."

belc_essay_tbl |>
  tabyl(sex) |>
  tibble() |>
  select(sex, frequency = n, proportion = percent) |>
  tt(width = 1)
```

A **bar plot** is a type of plot where the x-axis is a categorical variable and the y-axis is the frequency of the values. The frequency is represented by the height of the bar. The variables can be ordered by frequency, alphabetically, or some other order. @fig-aa-belc-barplots is a bar chart for the variables `sex` and `group` ordered alphabetically.

<!-- sex, group, and part_id barplots -->

```{r}
#| label: fig-aa-belc-barplots
#| fig-cap: "Bar plots for categorical variables `sex` and `group`."
#| fig-subcap:
#|   - "Sex"
#|   - "Time group"
#| fig-height: 3
#| layout-ncol: 2

# Function to create bar plots ----
create_barplot <- function(data, variable, x_lab = NULL, y_lab = NULL) {
  # Create a frequency table for the variable
  freq_table <- data |> count({{ variable }})

  # Set the y-limits to 0 to the sum of the variable frequencies
  ylim <- c(0, sum(freq_table$n))

  # Create the bar plot
  ggplot(freq_table, aes(x = {{ variable }}, y = n)) +
    geom_bar(stat = "identity") +
    ylim(ylim) +
    labs(x = x_lab, y = y_lab) +
    theme_qtalr(font_size = 13)
}

# Bar plots for categorical variables ----

# Bar plot `sex` ----
belc_essay_tbl |>
  create_barplot(sex, "Sex", "Frequency")

# Bar plot `group` ----
belc_essay_tbl |>
  create_barplot(group, "Time group", "Frequency")
```

So for a frequency table or barplot, we can see the frequency of each level of a categorical variable. This gives us some knowledge about the BELC dataset: there are more girls in the dataset and more essays appear in first and third time groups. If we were to see any clearly loopsided categories, this would be a sign of imbalance in the data and we would need to consider how this might impact our analysis.

::: {.callout}
**{{< fa regular lightbulb >}} Consider this**

The goal of descriptive statistics is to summarize the data in a way that is meaningful and interpretable. With this in mind, compare the frequency table in [-@tbl-aa-belc-frequency-table] and bar plot in [-@fig-aa-belc-barplots-1]. Does one provide a more interpretable summary of the data? Why or why not? Are there any other ways you might communicate this distribution more effectively?
:::

For numeric variables, a frequency table, as in @tbl-aa-belc-frequency-table, does not summarize a distibution well. Instead, the distribution of a numeric variable is best understood visually. Furthermore, we aim to assess the distribution of the variable in terms of the shape of the distribution and the presence of outliers.

The most common visualizations of the distribution of a numeric variable are histograms and density plots. **Histograms** are a type of bar plot where the x-axis is a numeric variable and the y-axis is the frequency of the values falling within a determined range of values, or bins. The frequency of values within each bin is represented by the height of the bars.

**Density plots** are a smoothed version of histograms. The y-axis of a density plot is the probability of the values. When frequent values appear closely together, the plot line is higher. When the frequency of values is lower or more spread out, the plot line is lower.

An example of these plots is show in @fig-aa-belc-histogram-density-tokens for the variable `tokens`.

```{r}
#| label: fig-aa-belc-histogram-density-tokens
#| fig-cap: "Distribution plots for the variable `tokens`."
#| fig-subcap:
#|  - "Histogram"
#|  - "Density plot"
#| fig-height: 3
#| layout-ncol: 2

# Define range for x-axis
x_range <- belc_essay_tbl |>
  pull(tokens) |>
  range()

# Histogram ----
belc_essay_tbl |>
  ggplot(aes(x = tokens)) +
  geom_histogram(bins = 30, color = "black", fill = "white") +
  labs(x = "Number of tokens", y = "Frequency") +
  scale_x_continuous(limits = x_range)

# Density plot ----
belc_essay_tbl |>
  ggplot(aes(x = tokens)) +
  geom_density() +
  labs(x = "Number of tokens", y = "Probability") +
  scale_x_continuous(limits = x_range)
```

Both the histogram in @fig-aa-belc-histogram-density-tokens-1 and the density plot in @fig-aa-belc-histogram-density-tokens-2 show the distribution of the variable `tokens` in slightly different ways which translate into trade-offs in terms of interpretability.

The histogram shows the frequency of the values in bins. The number of bins and/ or binwidth can be changed for more or less granularity. A rough grain histogram shows the general shape of the distribution, but it is difficult to see the details of the distribution. A fine grain histogram shows the details of the distribution, but it is difficult to see the general shape of the distribution. The density plot shows the general shape of the distribution, but it hides the details of the distribution. Given this trade-off, it is often useful explore outliers with histograms and the overall shape of the distribution with density plots.

In @fig-aa-belc-histograms we see both histograms and density plots combined for the variables `tokens`, `types`, and `ttr`.

```{r}
#| label: fig-aa-belc-histograms
#| fig-cap: "Histograms for numeric variables `tokens`, `types`, and `ttr`."
#| fig-subcap:
#|  - "Number of tokens"
#|  - "Number of types"
#|  - "Type-token ratio score"
#| fig-height: 3
#| layout-ncol: 3

# Histograms ----
belc_essay_tbl |>
  ggplot(aes(x = tokens)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, color = "black", fill = "white") +
  geom_density() +
  labs(x = "", y = "") +
  theme(axis.text.y = element_blank()) +
  theme_qtalr(font_size = 13)

belc_essay_tbl |>
  ggplot(aes(x = types)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, color = "black", fill = "white") +
  geom_density() +
  labs(x = "", y = "") +
  theme(axis.text.y = element_blank()) +
  theme_qtalr(font_size = 13)

belc_essay_tbl |>
  ggplot(aes(x = ttr)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, color = "black", fill = "white") +
  geom_density() +
  labs(x = "", y = "") +
  theme(axis.text.y = element_blank()) +
  theme_qtalr(font_size = 13)
```

Focusing on the details captured in the histogram we are better able to detect potential outliers. Outliers can reflect valid values that are simply extreme or they can reflect something erroneous in the data. To distinguish between these two possibilities, it is important to know the context of the data. Take, for example, @fig-aa-belc-histograms-3. We see that there is a bin near the value 1.0. Given that the type-token ratio is a ratio of the number of types to the number of tokens, it is unlikely that the type-token ratio would be exactly 1.0 as this would mean that every word in an essay is unique. Another, less dramatic, example is the bin to the far right of @fig-aa-belc-histograms-1. In this case, the bin represents the number of tokens in an essay. An uptick in the number of essays with a large number of tokens is not surprising and would not typically be considered an outlier. On the other hand, consider the bin near the value 0 in the same plot. It is unlikely that a true essay would have 0, or near 0, words and therefore a closer look at the data is warranted.

It is important to recognize that outliers contribute undue influence to overall measures of central tendency and dispersion. To appreciate this, let's consider another helpful visualization called a **boxplot**. A boxplot is a visual representation which aims to represent the central tendency, dispersion, and distribution of a numeric variable in one plot.

<!-- [ ] PDF chokes on this layout -->

::: {#fig-aa-belc-boxplots layout="[[ 40, 60 ]]"}

```{r}
#| label: fig-aa-belc-boxplot
#| fig-cap: "Boxplot"
#| fig-width: 4
#| fig-height: 4.9

p1 <-
  belc_essay_tbl |>
  ggplot(aes(y = ttr)) +
  geom_boxplot() +
  labs(y = "Type-Token Ratio Score", x = "") +
  theme(axis.text.x = element_blank()) +
  scale_y_continuous(limits = c(0.4, 1.02))

p1
```

```{r}
#| label: fig-aa-belc-histogram-boxplot
#| fig-subcap:
#|  - "Histogram"
#|  - "Boxplot (horizontal)"
#| fig-width: 6
#| fig-asp: 0.283
#| layout-nrow: 2

# Calculate quantiles and mean
quants <-
  belc_essay_tbl |>
  pull(ttr) |>
  quantile(probs = c(0.25, 0.5, 0.75))

mean_val <-
  belc_essay_tbl |>
  pull(ttr) |>
  mean()

# Histogram plot ----
p2 <-
  belc_essay_tbl |>
  ggplot(aes(x = ttr)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30, color = "#BDBDBD", fill = "white") +
  # geom_density() +
  geom_vline(aes(xintercept = quants[[1]]), linetype = "solid") + # first quartile
  geom_vline(aes(xintercept = quants[[2]]), linetype = "solid", linewidth = 1) + # median
  geom_vline(aes(xintercept = mean_val), linetype = "dashed", linewidth = 1) + # mean
  geom_vline(aes(xintercept = quants[[3]]), linetype = "solid") + # third quartile
  labs(x = "", y = "") +
  theme(axis.text.y = element_blank()) +
  scale_x_continuous(limits = c(0.4, 1.02))

p2

# Boxplot ----
p3 <-
  p1 +
  # add X annotation for mean
  geom_segment(aes(x = -0.38, xend = 0.38, y = mean(ttr), yend = mean(ttr)), linetype = "dashed", linewidth = 1) +
  # geom_rug() +
  coord_flip() +
  labs(y = "", x = "") +
  theme(axis.text.y = element_blank())

p3
```

Understanding boxplots
:::

In @fig-aa-belc-boxplot we see a boxplot for `ttr` variable. The box in the middle of the plot represents the interquartile range (IQR) which is the range of values between the first quartile and the third quartile. The solid line in the middle of the box represents the median. The lines extending from the box are called 'whiskers' and provide the range of values which are within 1.5 times the IQR. Values outside of this range are plotted as individual points.

Now let's consider boxplots from another angle. In @fig-aa-belc-histogram-boxplot-2 I've plotted the boxplot horizontally, right below the histogram in @fig-aa-belc-histogram-boxplot-1. In this view, we can see that a boxplot is a simplifed histogram augmented with central tendency and dispersion statistics. While histograms focus on the frequency distribution of data points, boxplots focus on the data's quartiles and potential outliers.

I've added a dashed line in @fig-aa-belc-histogram-boxplot-1 and @fig-aa-belc-histogram-boxplot-2 to signal the mean in this set of plots, but it is not typically included. I include the dashed line to make a point: the mean is more sensitive to outliers than the median. As I pointed out in @sec-aa-central-tendency, the mean is the sum of all values divided by the number of values. If there are extreme values, the mean will be pulled in the direction of the extreme values. The median, however, is the middle value and a few extreme values have less effect. So, when central tendency is reported, if there is a sizeable difference between the mean and the median, measures of dispersion will be larger and the direction of the difference can be used to infer the presence of outliers.

Returning to outliers, it is important to address them to safeguard the accuracy of the analysis. There are two main ways to address outliers: 1) transform the data and 2) eliminate observations with outliers (**trimming**). Trimming is more extreme as it removes data but can be the best approach for true outliers. Transforming the data is an approach to mitigating the influence of extreme but valid values. **Transformation** involves applying a mathematical function to the data which changes the scale and/ or shape of the distribution, but does not remove data nor does it change the relative order of the values.

In @fig-aa-belc-boxplot-trimmed, we see two boxplots. @fig-aa-belc-boxplot-trimmed-1 is the original `ttr` data and @fig-aa-belc-boxplot-trimmed-2 reflects the data trimmed to remove outliers. In this case, we have removed essays with a type-token ratio of 1.

```{r}
#| label: fig-aa-belc-boxplot-trimmed
#| fig-cap: "Boxplots for `ttr` before and after trimming."
#| fig-subcap:
#|  - "Type-token ratio score"
#|  - "Type-token ratio score (trimmed)"
#| layout-ncol: 2

# Boxplot (original) ----

ttr_mean <-
  belc_essay_tbl |>
  pull(ttr) |>
  mean()
ttr_median <-
  belc_essay_tbl |>
  pull(ttr) |>
  median()

belc_essay_tbl |>
  ggplot(aes(y = ttr)) +
  geom_boxplot() +
  # add X annotation for mean
  geom_segment(aes(x = -0.38, xend = 0.38, y = ttr_mean, yend = ttr_mean), linetype = "dashed") +
  annotate("text", x = -0.09, y = ttr_mean, label = paste("Mean:", round(ttr_mean, 2)), hjust = 0, vjust = -0.4) +
  annotate("text", x = -0.1, y = ttr_median, label = paste("Median:", round(ttr_median, 2)), hjust = 0, vjust = 1.4) +
  labs(y = "Type-Token Ratio Score", x = "") +
  theme(axis.text.x = element_blank())

# Boxplot (trimmed) ----
belc_essay_tbl <-
  belc_essay_tbl |>
  # Filter out essays with less than 10 tokens or a type-token ratio of 1
  filter(ttr != 1)

ttr_mean <-
  belc_essay_tbl |>
  pull(ttr) |>
  mean()
ttr_median <-
  belc_essay_tbl |>
  pull(ttr) |>
  median()


belc_essay_tbl |>
  ggplot(aes(y = ttr)) +
  geom_boxplot() +
  # add X annotation for mean
  geom_segment(aes(x = -0.38, xend = 0.38, y = mean(ttr), yend = mean(ttr)), linetype = "dashed") +
  annotate("text", x = -0.09, y = ttr_mean, label = paste("Mean:", round(ttr_mean, 2)), hjust = 0, vjust = -0.4) +
  annotate("text", x = -0.1, y = ttr_median, label = paste("Median:", round(ttr_median, 2)), hjust = 0, vjust = 1.4) +
  labs(y = "Type-Token Ratio Score", x = "") +
  # remove x-axis labels
  theme(axis.text.x = element_blank()) +
  ylim(0.4, 1.0)
```

We can now appreciate the relatively larger effect that the outliers had on the mean value of the `ttr` variable. As outliers are removed as the difference between the mean and median will become smaller.

<!-- Normal distribution/ skewed distributions -->

The exploration the data points with histograms and boxplots has helped us to identify outliers. Now we turn to the question of the overall shape of the distribution. The key question is whether the observed distribution of each variable groups around a central tendency in a symmetrical manner or if the distribution is skewed in one direction or another.

When values are symmetrically dispersed around the central tendency, the distribution is said to be normal. The **Normal Distribution** is characterized by a distribution where the mean and median are the same. The Normal Distribution has a key role in theoretical statistics and is the foundation for many statistical tests. This distribution is also known as the Gaussian Distribution or the Bell Curve for the hallmark bell shape of the distribution. In a normal distribution, extreme values are less likely than values near the center.

When values are not symmetrically dispersed around the central tendency, the distribution is said to be skewed. A distribution in which values tend to disperse to the left of the central tendency is **left skewed** and a distribution in which values tend to disperse to the right of the central tendency is **right skewed**.

Stepping away from our BELC dataset, I've created simulated data that fit normal and non-normal, or skewed, distributions. I present each of these distributions as density plots with mean and median line overlays in @fig-aa-distributions.

```{r}
#| label: fig-aa-distributions
#| fig-cap: "Mean and median for normal and skewed distributions."
#| fig-subcap:
#|  - "Left skewed distribution"
#|  - "Normal distribution"
#|  - "Right skewed distribution"
#| fig-height: 3
#| layout-ncol: 3

shape1 <- 6
shape2 <- 2

# Left skewed distribution ----
set.seed(123)
left_skew_data <- tibble(value = rbeta(1000, shape1, shape2)) # left skewed

ggplot(left_skew_data, aes(x = value)) +
  geom_function(fun = dbeta, args = list(shape1 = shape1, shape2 = shape2), color = "black", size = 1) +
  geom_vline(aes(xintercept = mean(value)), linetype = "dashed", linewidth = 1) +
  geom_vline(aes(xintercept = median(value)), linetype = "solid", linewidth = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "Values", y = "Density") +
  theme_qtalr(font_size = 13)

# Normal distribution ----
set.seed(123)
norm_data <- tibble(value = rnorm(1000))

ggplot(norm_data, aes(x = value)) +
  geom_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "black", size = 1) +
  geom_vline(aes(xintercept = mean(value)), linetype = "dashed", linewidth = 1) +
  geom_vline(aes(xintercept = median(value)), linetype = "solid", linewidth = 1) +
  labs(x = "Values", y = "Density") +
  theme(axis.text = element_blank()) +
  theme_qtalr(font_size = 13)


# Right skewed distribution ----
set.seed(123)
right_skew_data <- tibble(value = rbeta(1000, shape2, shape1)) # right skewed

ggplot(right_skew_data, aes(x = value)) +
  geom_function(fun = dbeta, args = list(shape1 = shape2, shape2 = shape1), color = "black", size = 1) +
  geom_vline(aes(xintercept = mean(value)), linetype = "dashed", linewidth = 1) +
  geom_vline(aes(xintercept = median(value)), linetype = "solid", linewidth = 1) +
  theme(axis.text = element_blank()) +
  labs(x = "Values", y = "Density") +
  theme_qtalr(font_size = 13)

```

Assessing the distribution of a variable is important for two reasons. First, the distribution of a variable can inform the choice of statistical test in theory-based hypothesis testing. Data that are normally, or near-normally distributed are often analyzed using parametric tests while data that exhibit a skewed distributed are often analyzed using non-parametric tests. Second, highly skewed distributions have the effect of compressing the range of values. This can lead to a loss of information and can make it difficult to detect patterns in the data.

::: {.callout}
**{{< fa medal >}} Dive deeper**

Another approach for visually summarizing a single numeric variable is the Empirical Cumulative Distribution Function, or *ECDF*. An ECDF plot is a summary of the cummulative proportion of each of the values of a numeric variable. In addition to providing insight into the distribution of a variable, ECDF plots can be useful in determing what proportion of the values fall above or below a certain percentage of the data.
:::

Let's return to the BELC dataset and assess the distribution of the variables `tokens`, `types` and `ttr` in @fig-aa-belc-histogram-density-trimmed to the three distributions in @fig-aa-distributions, we see that all three numeric variables in the BELC dataset are skewed to some degree, but not as extreme as the simulated data.

```{r}
#| label: fig-aa-belc-histogram-density-trimmed
#| fig-cap: "Histogram/ Density plots for numeric variables in the BELC dataset."
#| fig-subcap:
#| - "Number of tokens"
#| - "Number of types"
#| - "Type-token ratio score"
#| fig-height: 3
#| layout-ncol: 3

belc_essay_tbl <-
  belc_essay_tbl |>
  # Filter out essays with less than 10 tokens
  filter(tokens >= 10) |>
  anti_join(find_outliers(belc_essay_tbl, prop_l2), by = "essay_id")

# Density plots ----

# Type-token ratio score ----
belc_essay_tbl |>
  ggplot(aes(x = tokens)) +
  geom_density() +
  labs(x = "", y = "") +
  theme_qtalr(font_size = 13)

# Number of types ----
belc_essay_tbl |>
  ggplot(aes(x = types)) +
  geom_density() +
  labs(x = "", y = "") +
  theme_qtalr(font_size = 13)

# Proportion of L2 words ----
belc_essay_tbl |>
  ggplot(aes(x = ttr)) +
  geom_density() +
  labs(x = "", y = "") +
  theme_qtalr(font_size = 13)
```

@fig-aa-belc-histogram-density-trimmed-1 and [-@fig-aa-belc-histogram-density-trimmed-2] for `tokens` and `types` has a bit more right skewing than `ttr` seen in @fig-aa-belc-histogram-density-trimmed-3. This is consistent with the summary statistics for the central tendency in @tbl-aa-belc-descriptive-stats-numeric. The mean and median for `tokens` and `types` are not the same and the mean is larger than the median. For `ttr`, the mean and median is much closer.

In the case that a variable is highly skewed, it is often useful to attempt transform the variable to reduce the skewness. In contrast to scale-based transformations (*e.g.* centering and scaling), shape-based transformations change the scale and the shape of the distribution. The most common shape-based transformation is the logarithmic transformation. The **logarithmic transformation** (log-transformation) takes the log (typically base 10) of each value in a variable. The log-transformation is useful for reducing the skewness of a variable as it compresses large values and expands small values. If the skewness is due to these factors, the log-transformation can help.

It is important to note, however, that if scale-based transformations are to be applied to a variable, they should be applied after the log-transformation as the log of negative values is undefined.

### Association

<!-- Purpose: nature and strength -->

We have covered the first three of the four questions we are interested in asking in a descriptive analysis. The fourth, and last, question is whether there is an association between variables. If so, what is the directionality and what is the apparent magnitude of the dependence? Knowing the answers to these questions will help frame our approach to analysis.

To assess association, the number and information types of the variables under consideration are important. Let's start by considering two variables. If we are working with two variables, we are dealing with a **bivariate** relationship. Given there are three informational types (categorical, ordinal, and numeric), there are six logical bivariate combinations: categorical-categorical, categorical-ordinal, categorical-numeric, ordinal-ordinal, ordinal-numeric, and numeric-numeric.

The directionality of a relationship will take the form of a tabular or graphic summary depending on the informational value of the variables involved. In @tbl-aa-summary-types, we see the appropriate summary types for each of the six bivariate combinations.

|                 | Categorical       | Ordinal                     | Numeric              |
|-----------------|-------------------|-----------------------------|----------------------|
| **Categorical** | Contingency table | Contingency table/ Bar plot | Pivot table/ Boxplot |
| **Ordinal**     | -                 | Contingency table/ Bar plot | Pivot table/ Boxplot |
| **Numeric**     | -                 | -                           | Scatterplot          |

: Appropriate summary types for different combinations of variable types. {#tbl-aa-summary-types}{tbl-colwidths="[14, 22, 30, 22]" .sm .striped}

<!-- Nominal + ? -->

Let's first start with the combinations that include a categorical or ordinal variable. Categorical and ordinal variables reflect measures of class-type information, with add meaningful ranks to ordinal variables. To assess a relationship with these variable types, a table is always a good place to start. When combined together, a contingency table is the appropriate table. A **contingency table** is a cross-tabulation of two class-type variables, basically a two-way frequency table. This means that three of the six bivariate combinations are assessed with a contingency table: categorical-categorical, categorical-ordinal, and ordinal-ordinal.

In @tbl-aa-belc-contingency-tables we see contingency tables for the categorical variable `sex` and ordinal variable `group` in the BELC dataset.

<!-- sex + group contingency table -->

```{r}
#| label: tbl-aa-belc-contingency-tables
#| tbl-cap: "Contingency tables for categorical variable `sex` and ordinal variable `group` in the BELC dataset."
#| tbl-subcap:
#|  - "Counts"
#|  - "Percentages"
#| layout-ncol: 2

belc_essay_tbl |>
  tabyl(group, sex) |>
  adorn_totals(c("row", "col")) |>
  tt(width = 1, digits = 0)

belc_essay_tbl |>
  tabyl(group, sex) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 2) |>
  tt(width = 1, digits = 0)
```

A contingency table may include only counts, as in @tbl-aa-belc-contingency-tables-1, or may include proportions or percentages in an effort to normalize the counts and make them more comparable, as in @tbl-aa-belc-contingency-tables-2.

It is sometimes helpful to visualize a contingency table as a bar plot when there are a larger number of levels in either or both of the variables. Again, looking at the relationship between `sex` and `group`, we see that we can plot the counts or the proportions. In @fig-aa-belc-bar-plots, we see both.

<!-- sex + group bar plots (counts/ proportions) -->

```{r}
#| label: fig-aa-belc-bar-plots
#| fig-cap: "Bar plots for the relationship between `sex` and `group` in the BELC dataset."
#| fig-subcap:
#| - "Counts"
#| - "Proportions"
#| fig-height: 3
#| layout-ncol: 2

belc_essay_tbl |>
  ggplot(aes(x = sex, y = after_stat(count), fill = group)) +
  geom_bar(position = "stack", color = "black") +
  scale_fill_brewer(palette = "Greys") +
  labs(x = "Sex", y = "Frequency", fill = "Group") +
  ylim(0, 80)

belc_essay_tbl |>
  ggplot(aes(x = sex, fill = group)) +
  geom_bar(position = "fill", color = "black") +
  scale_fill_brewer(palette = "Greys") +
  labs(x = "Sex", y = "Proportion", fill = "Group")
```

To summarize and assess the relationship between a categorical or an ordinal variable and a numeric variable, we cannot use a contingency table. Instead, this type of relationship is best summarized in a table using a summary statistic in a **pivot table**. A pivot table is a table in which a class-type variable is used to group a numeric variable by some summary statistic appropriate for numeric variables, *e.g.* mean, median, standard deviation, *etc.*

In @tbl-aa-belc-pivot-table, we see a pivot table for the relationship between `group` and `tokens` in the BELC dataset. Specifically, we see the mean number of tokens by group.

<!-- group + tokens pivot table (mean) -->

```{r}
#| label: tbl-aa-belc-pivot-table
#| tbl-cap: "Pivot table for the relationship between `group` and `tokens` in the BELC dataset."

belc_essay_tbl |>
  group_by(group) |>
  summarise(mean_tokens = mean(tokens)) |>
  tt(width = 1)
```

We see that the mean number of tokens increases from Group T1 to T4, which is consistent with the idea that the students in the higher groups are writing longer essays.

Although a pivot table may be appropriate for targeted numeric summaries, a visualization is often more informative for assessing the dispersion and distribution of a numeric variable by a categorical or ordinal variable. There are two main types of visualizations for this type of relationship: a boxplot and a **violin plot**. A violin plot is a visualization that summarizes the distribution of a numeric variable by a categorical or ordinal variable, adding the overall shape of the distribution, much as a density plot does for histograms.

In @fig-aa-belc-boxplot-violin-plot, we see both a boxplot and a violin plot for the relationship between `group` and `tokens` in the BELC dataset.

<!-- group + tokens boxplot/ voilin plot -->

```{r}
#| label: fig-aa-belc-boxplot-violin-plot
#| fig-cap: "Boxplot and violin plot for the relationship between `group` and `tokens` in the BELC dataset."
#| fig-subcap:
#| - "Boxplot"
#| - "Violin plot"
#| fig-height: 3
#| layout-ncol: 2

belc_essay_tbl |>
  ggplot(aes(x = group, y = tokens)) +
  geom_boxplot(color = "black") +
  labs(x = "Group", y = "Tokens")

belc_essay_tbl |>
  ggplot(aes(x = group, y = tokens)) +
  geom_violin(color = "black") +
  labs(x = "Group", y = "Tokens")
```

From the boxplot in @fig-aa-belc-boxplot-violin-plot-1, we see that the general trend towards more tokens used by students in higher groups. But we can also appreciate the dispersion of the data within each group looking at the boxes and whiskers. On the surface it appears that the data for groups T1 and T3 are closer to each other than groups T2 and T4, in which there is more variability within these groups. Furthermore, we can see outliers in groups T1 and T3, but not in groups T2 and T4. From the violin plot in @fig-aa-belc-boxplot-violin-plot-2, we can see the same information, but we can also see the overall shape of the distribution of tokens within each group. In this plot, it is very clear that group T4 includes a wide range of token counts.

<!-- Numeric + Numeric -->

The last bivariate combination is numeric-numeric. To summarize this type of relationship a scatterplot is used. A **scatterplot** is a visualization that plots each data point as a point in a two-dimensional space, with one numeric variable on the x-axis and the other numeric variable on the y-axis. Depending on the type of relationship you are trying to assess, you may want to add a trend line to the scatterplot. A trend line is a line that summarizes the overall trend in the relationship between the two numeric variables. To assess the extent to which the relationship is linear, a straight line is drawn which minimizes the distance between the line and the points.

In @fig-aa-belc-scatter-plot, we see a scatterplot and a scatterplot with a trend line for the relationship between `ttr` and `types` in the BELC dataset.

<!-- ttr + types scatter plot: points, points + trend line -->

```{r}
#| label: fig-aa-belc-scatter-plot
#| fig-cap: "Scatter plot for the relationship between `ttr` and `types` in the BELC dataset."
#| fig-subcap:
#| - "Points"
#| - "Points with a linear trend line"
#| fig-height: 3
#| layout-ncol: 2

belc_essay_tbl |>
  ggplot(aes(x = types, y = ttr)) +
  geom_point(color = "black", alpha = 0.5) +
  labs(x = "Number of types", y = "Type-Token Ratio score")

belc_essay_tbl |>
  ggplot(aes(x = types, y = ttr)) +
  geom_point(color = "black", alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Number of types", y = "Type-Token Ratio score")
```

We see that there is an apparent positive relationship between these two variables, which is consistent with the idea that as the number of types increases, the type-token ratio increases. In other words, as the number of unique words increases, so does the lexical diversity of the text. Since we are evaluating a linear relationship, we are assessing the extent to which there is a **correlation** between `ttr` and `types`. A correlation simply means that as the values of one variable change, the values of the other variable change in a consistent manner.

<!-- Assocation/ Correlation strength -->

Once a sense of the directionality of a relationship can be established, the next step is to gauge the relative strength of an association. There are a number of measures of association which aim to provide a statistic which summarizes the strength of an association, seen in @tbl-aa-association-measures.

|                                 | Categorical            | Ordinal                 | Numeric<br>*Non-parametric* | Numeric<br>*Parametric*   |
|---------------------------------|------------------------|-------------------------|-----------------------------|---------------------------|
| **Categorical**                 | Chi-square $\chi^2$, Cramr's $V$ | Goodman and Kruskal's $\gamma$ | Rank biserial Correlation   | Point-biseral Correlation |
| **Ordinal**                     | -                      | Kendall's $\tau$             | Kendall's $\tau$                  | Pearson's $r$               |
| **Numeric**<br>*Non-parametric* | -                      | -                       | Kendall's $\tau$                  | Pearson's $r$               |
| **Numeric**<br>*Parametric*     | -                      | -                       | -                           | Pearson's $r$               |

: Measures of association or correlation strength for different combinations of variable types. {#tbl-aa-association-measures tbl-colwidths="[20, 20, 20, 20, 20]" .sm .striped}

At the stage of descriptive analysis, however, we tend to focus on the visual and tabular summaries to gauge the strength of the association and let our analysis provide metrics for association strength.

Before moving on to the next section, it is important to remember than through the process of descriptive measures, we gain a thorough understanding of our data's characteristics and quality, preparing us for the analysis. The decisions we make at this stage, from estimating central tendency to understanding the possible association between our variables, can have significant implications on our approach and interpretation in the subsequent analysis. So, this initial step of data analysis deserves our careful attention and scrutiny.

## Analyze {#sec-aa-analyze}

The goal of analysis, generally, is to generate knowledge from information. The type of knowledge generated and the process by which it is generated, however, differ and can be broadly grouped into three analysis types: exploratory, predictive, and inferential.

In this section I will provide an overview of how each of these analysis types are tied to research aims and how the general purpose of each type affect: (1) how to *identify* the variables of interest, (2) how to *interrogate* these variables, and (3) how to *interpret* the results. I will structure the discussion of these analysis types moving from the least structured (inductive) to most structured (deductive) approach to deriving knowledge from information with the aim to provide enough information for you to identify these research approaches in the literature and to make appropriate decisions as to which approach your research should adopt.

### Explore {#sec-aa-explore}

In **Exploratory Data Analysis (EDA)**, we use a variety of methods to identify patterns, trends, and relations within and between variables. The goal of EDA is uncover insights in an inductive, data-driven manner. That is to say, that we do not enter into EDA with a fixed hypothesis in mind, but rather we explore intuition, probe anecdote, and follow hunches to identify patterns and relationships and to evaluate whether and why they are meaningful. We are admittedly treading new or unfamiliar terrain letting the data guide our analysis. This means that we can use and reuse the same data to explore different angles and approaches adjusting our methods and measures as we go. In this way, EDA is an iterative, meaning generating process.

<!-- Identification of variables -->

In line with the investigative nature of EDA, the identification of variables of interest is a discovery process. We most likely have a intuition about the variables we would like to explore, but we are able to adjust our variables as need be to suit our research aims. When the identification and selection of variables is open, the process is known as **feature engineering**. A process that is much an art as a science, feature engineering leverages a mixture of relevant domain knowledge, intuition, and trial and error to identify features that serve to best represent the data and to best serve the research aims. Furthermore, the roles of features in EDA are fluid --no variable has a special status, as seen in @fig-eda-variables. We will see that in other types of analysis, some or all the roles of the variables are fixed.

```{r}
#| label: fig-eda-variables
#| fig-cap: 'Roles of variables in EDA.'
#| out.width: '75%'

knitr::include_graphics("figures/aa-eda-variables.drawio.png")
```

For illustrative purposes let's consider the State of the Union Corpus (SOTU) [@R-quanteda.corpora]. The presidential addresses and a set of metadata variables are included in the corpus. I've subsetted this corpus to only include U.S. presidents since 1946. A tabular preview of the first 10 addresses (truncated for display) can be found in @tbl-eda-sotu-corpus.

<!-- [ ] Change to data dictionary view?  -->

```{r}
#| label: tbl-eda-sotu-corpus
#| tbl-cap: 'First ten addresses from the SOTU Corpus.'
#| echo: false
#| tbl-colwidths: [10, 15, 10, 15, 50]

library(quanteda) # for corpus()
sotu_corpus <-
  quanteda.corpora::data_corpus_sotu |> # load the corpus
  corpus_subset(Date > "1946-01-01") # subset to only include presidents since 1946

sotu_tbl <-
  sotu_corpus |> # select the corpus
  tidy() |> # convert to tibble
  select(president = President, date = Date, delivery, party, addresses = text) |> # select variables
  mutate(addresses = str_squish(addresses)) # remove line breaks

sotu_tbl |> # select the tibble
  slice_head(n = 10) |> # select first 10 addresses
  mutate(addresses = str_trunc(addresses, 50, "right")) |> # truncate addresses
  tt(width = 1)
```

A dataset such as this one could serve as a starting point to explore many different types of research questions. In order to maintain research coherence so our efforts to not careen into a free-for-all, we need to tether our feature engineering to a unit of analysis that is relevant to the research question. A **unit of analysis** is the entity that we are interested in studying. Not to be confused with the unit of observation, which is the entity that we are able to observe and measure [@Sedgwick2015].

To demonstrate the distinction, let's look consider different approaches to analyzing the SOTU dataset. For example, the unit of analysis could be the language of particular presidents, party ideology, or political rhetoric in general and the unit of observation could be individual words, phrases, sentences, etc. In some cases the unit of analysis and the unit of observation are the same. For example, if we were interested in potential changes use of the word "terrorist" over time in SOTU addresses, the unit of analysis and the unit of observation would be the same --individual addresses. So, depending on the perspective we are interested in investigating, the choice of how to approach engineering features to gain insight will vary.

<!--
- Methods
  - Categorical: Distributions (frequency, co-occurrence) and groupings (clustering)
  - Numeric: Dimensions (PCA, t-SNE, Topic modeling) and groupings (vector space models)
-->

By the same token, approaches for interrogating the dataset can differ significantly, between research projects and within the same project, but for instructive purposes, let's draw a distinction between descriptive methods and unsupervised learning methods, as seen in @tbl-eda-methods.

```{r}
#| label: tbl-eda-methods
#| tbl-cap: 'Some common EDA methods'

eda_approaches <-
  tribble(
    ~"Descriptive methods", ~"Unsupervised learning methods",
    "Frequency analysis", "Cluster analysis",
    "Keyness analysis", "Topic Modeling",
    "Co-occurence analysis", "Vector Space Models"
  )
eda_approaches |>
  tt(width = 1)
```

The first group, **descriptive methods** can be seen as a (more robust) extenstion of the descriptive statistics covered earlier in this chapter including statistic, tabular, and visual techniques. For example, a frequency analysis of the SOTU dataset could be used to identify the most common words used by U.S. political parties in their addresses, in @fig-aa-eda-sotu-descriptive-1, or a co-occurence analysis could be used to identify the most common words the appear after the term "free", in @fig-aa-eda-sotu-descriptive-2, in the dataset.

<!-- Descriptive methods graphics -->

```{r}
#| label: aa-eda-sotu-freq

library(quanteda.textstats) # for textstat_frequency()
library(quanteda.textplots) # for plot_frequency()

sotu_freq_p <- # Frequency plots
  sotu_corpus |>
  tokens(remove_punct = TRUE) |> # remove punctuation
  tokens_remove(pattern = stopwords("en")) |> # remove stopwords
  dfm() |> # convert to dfm
  textstat_frequency(n = 20, groups = party) |> # get top 20 terms for each party
  ggplot(aes(x = reorder(feature, frequency), y = frequency, color = group)) + # plot
  geom_point() + # point plot
  labs(
    x = "Terms", y = "Term frequency", color = "Party"
  ) +
  # place legend below plot
  theme(legend.position = "bottom", legend.box = "horizontal") +
  coord_flip() # flip axes
```

```{r }
#| label: aa-eda-sotu-cooccurence

# Co-occurence analysis
# network graph

library(igraph) # for graph_from_data_frame()
library(ggraph) # for ggraph()

bigrams <-
  sotu_tbl |>
  unnest_tokens(bigram, addresses, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(
    !word1 %in% stopwords::stopwords("en"),
    !word2 %in% stopwords::stopwords("en")
  ) |>
  count(word1, word2, sort = TRUE) |>
  filter(word1 == "free", n > 1)

# set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.05, "inches"))

sotu_free_p <-
  bigrams |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = log(n), label_size = log(n)), show.legend = FALSE, arrow = a) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, size = 3) +
  theme_void()
```

```{r}
#| label: aa-eda-sotu-keyness
#| eval: false

sotu_keyness_p <- # Barplot: keyness
  sotu_corpus |>
  tokens(remove_punct = TRUE) |>
  tokens_remove(pattern = stopwords("en")) |>
  dfm() |>
  textstat_keyness(target = docvars(sotu_corpus, "Date") > "2001-09-10") |>
  mutate(period = case_when(
    n_reference > n_target ~ "Pre 9-11",
    n_target > n_reference ~ "Post 9-11"
  )) |>
  filter(!is.na(period)) |>
  group_by(period) |>
  slice_head(n = 10) |>
  ggplot(aes(x = reorder(feature, chi2), y = chi2)) +
  geom_col(width = .5) +
  labs(
    x = "Terms", y = "Keyness statistic (Chi-squared)",
    title = "Term keyness"
  ) +
  facet_wrap(~ reorder(period, desc(period)), scales = "free_y") +
  coord_flip()
```

```{r}
#| label: fig-aa-eda-sotu-descriptive
#| fig-cap: "Example of descriptive methods applied to the SOTU dataset."
#| fig-subcap:
#| - "Frequency analysis of the 20 most frequent terms by party."
#| - "Co-occurence analysis of the terms that appear after the term 'free'."
#| fig-height: 3
#| layout-ncol: 2

sotu_freq_p # Frequency plots
sotu_free_p # Co-occurence analysis
```

The second group, **unsupervised learning**, is a subtype of machine learning in which an algorithm is used to find patterns within and between variables in the data without any guidance (supervision). In this way, the algorithm, or machine learner, is left to make connections and associations wherever they may appear in the input data. If we were interested in finding word-use continuities and discontinuities between presidents, we could use a clustering algorithm, seen in @fig-aa-eda-sotu-unsupervised-1. Or if we wanted to uncover semantic relatedness we could use a vector space model, as in @fig-aa-eda-sotu-unsupervised-2.

<!-- Add unsupervised learning graphics -->

```{r}
#| label: aa-eda-sotu-clustering
#| warning: false

# Clustering (hierarchical) analysis
# dendrogram

library(factoextra) # for fviz_dend()

sotu_hclust <-
  sotu_corpus |>
  tokens() |>
  dfm() |>
  dfm_remove(pattern = stopwords("en")) |>
  dfm_group(groups = President) |>
  textstat_dist() |>
  as.dist() |>
  hclust()

sotu_pres_p <-
  fviz_dend(sotu_hclust,
    cex = .5,
    k = 2,
    k_colors = c("black", "darkgrey")
  ) + labs(title = "")
```

```{r}
#| label: aa-eda-sotu-word-embeddings

# Word embeddings ----

library(word2vec) # for word2vec()
library(Rtsne) # for Rtsne()
library(ggrepel)

# Extract texts from corpus
sotu_texts <-
  sotu_corpus |>
  as.character() |>
  tolower()

# Train word2vec model
sotu_model <-
  word2vec(sotu_texts, stopwords = stopwords("en"))

# Extract word vectors and vocabulary
sotu_vectors <-
  sotu_model |>
  as.matrix()

sotu_vocab <-
  sotu_model |>
  summary(type = "vocabulary")

# Reduce dimensionality of word vectors w/ t-SNE
sotu_tsne <- Rtsne(sotu_vectors)

# Create data frame of word vectors and vocabulary
sotu_tsne_tbl <-
  sotu_tsne$Y |>
  as.data.frame() |>
  as_tibble() |>
  mutate(label = sotu_vocab)

# Cluster word vectors
set.seed(1234)

sotu_tsne_clusters <-
  sotu_tsne_tbl |>
  mutate(cluster = kmeans(sotu_tsne_tbl[, 1:2], centers = 5)$cluster) |>
  mutate(cluster = as.factor(cluster))

# Sample 20 words from each cluster
sotu_tsne_sampled <-
  sotu_tsne_clusters |>
  group_by(cluster) |>
  sample_n(20)

# Plot word vectors
sotu_tsne_p <-
  sotu_tsne_clusters |>
  ggplot(aes(x = V1, y = V2, color = cluster)) +
  geom_point(alpha = 0.75) +
  geom_text_repel(data = sotu_tsne_sampled, aes(label = label), color = "black", min.segment.length = 0, size = 3) +
  theme_minimal() +
  labs(x = "Component 1", y = "Component 2", color = "Cluster") +
  scale_color_brewer(palette = "Greys", direction = -1, type = "qual")
```


```{r}
#| label: fig-aa-eda-sotu-unsupervised
#| fig-cap: "Example of unsupervised learning methods applied to the SOTU dataset."
#| fig-subcap:
#| - "Hierarchical clustering of the SOTU corpus."
#| - "Word embedding space in the SOTU corpus."
#| fig-height: 3
#| layout-ncol: 2

sotu_pres_p # clustering analysis
sotu_tsne_p # word embeddings
```

<!--
- Results
  - Data-driven hypotheses
-->

Either through descriptive, unsupervised learning methods, or a combination of both, EDA employs quantitative methods to summarize, reduce, and sort complex datasets in order to provide the researcher novel perspective to be qualitatively assessed. Exploratory methods produce results that require associative thinking and pattern detection. Speculative as they are, the results from exploratory methods can be highly informative and lead to new insight and inspire further study in directions that may not have been expected.

### Predict {#sec-aa-predict}

<!--
- Purpose
  - Predicting future outcomes based on past data
  - Assessing the likelihood of a hypothesis
  - data-driven/ theory-driven (make educated guesses about the relationships between variables)
-->

**Predictive Data Analysis (PDA)** employs a variety of techniques to examine and evaluate the association strength between a variable or set of variables, with a specific focus on predicting a target variable. The aim of PDA is to construct models that can accurately forecast future outcomes, using either data-driven or theory-driven approaches. In this process, **supervised learning** methods, where the machine learning algorithm is guided (supervised) by a target outcome variable, are used. This means we don't begin PDA with a completely open-ended exploration, but rather with an objective - accurate predictions. However, the path to achieving this objective can be flexible, allowing us freedom to adjust our models and methods. Unlike EDA, where the entire dataset can be reused for different approaches, PDA requires a portion of the data to be reserved for evaluation, enhancing the validity of our predictive models. Thus, PDA is an iterative process that combines the flexibility of exploratory analysis with the rigor of confirmatory analysis.

<!-- [ ] 'confirmatory analysis', right language? -->

<!--
Previous:

PDA is a versatile method that often employed to derive intelligent action from data, but it can also be used for hypothesis generation and even hypothesis testing, under certain conditions. If a researcher's aim is to create model that can perform a language related task, explore association strength between a target variable and various types and combinations of features, or to perform emerging alternative approaches to hypothesis testing [^4], this is the analysis approach a researcher will take.

[^4]: see @Deshors2016 and @Baayen2011 for examples of hypothesis testing using PDA.

-->

<!-- Identification of variables -->

There are two types of variables in PDA: the outcome variable and the predictor variables, or features. The **outcome variable** is the variable that the researcher is trying to predict. It is the only variable that is necessarily fixed as part of the research question. The features are the variables that are used to predict the outcome variable. An overview of the roles of these variables in PDA is shown in @fig-pda-variables.

```{r}
#| label: fig-pda-variables
#| fig-cap: 'Roles of variables in PDA.'
#| out.width: '75%'

knitr::include_graphics("figures/aa-pda-variables.drawio.png")
```

Feature selection can be either data-driven or theory-driven. Data-driven features are those that are engineered to enhance predictive power, while theory-driven features are those that are selected based on theoretical relevance.

Let's consider the Europarl corpus of native, non-native and translated texts (ENNTT) [@Nisioi2016]. This is a monolingual English corpus of translated and non-translated texts from the European Parliament.

```{r}
#| label: tbl-enntt-dd
#| tbl-cap: 'Data dictionary of the ENNTT corpus.'

read_csv("data/aa-enntt_curated_dd.csv") |>
  tt(width = 1)
```

Now depending on our research question, we will have a different outcome variable. If we want to examine the potential linguistic differences between native and non-native speakers, we will select our outcome variable to be the  `type` (natives/ nonnatives). The features selected to use to predict `type` depend on our research question. If our research is guided by data, we will choose features that are specifically designed to boost the ability to predict. On the other hand, if our research is steered by theory, we will opt for features that are chosen due to their theoretical significance. In either case, the original dataset will likely need to be transformed.

The approach to interrogating the dataset includes three main steps: feature engineering, model selection, and model evaluation. We've discussed feature engineering, so what is model selection and model evaluation? And how do we go about performing these steps?

**Model selection** is the process of choosing a machine learning algorithm and set of features that produces the best prediction accuracy for the outcome variable. To refine our approach such that we arrive at the best combination of algorithm and features, we need to train our machine learner on a variety of combinations and evaluate the accuracy of each. We don't want to train and evaluate on the same data, as this would be cheating, and likely would not produce a model that generalizes well to new data. Instead, we split our data into two sets: a training set and a test set. The **training set** is used to train the machine learner, while the **test set** is used to evaluate the accuracy of the model[^dev-set]. The larger portion of the data, from 60% to 80%, is used for training, while the remaining portion is used for testing.

[^dev-set]: Depending on the application and the amount of available data, a third *development set* is sometimes created as a pseudo test set to facilitate the testing of multiple approaches on data outside the training set before the final evaluation on the test set is performed.

<!--
- Methods
  - Classification
    - Logistic regression, decision trees/ random forests, naive Bayes
  - Regression
    - Linear regression, support vector machines, neural networks
-->

The elephant in the room is, what type of machine learning algorithm do I use? Well, there are many different types of machine learning algorithms, each with their own strengths and weaknesses. The first rough cut is to decide what type of outcome variable we are predicting: categorical or numeric. If the outcome variable is categorical, we are performing a **classification** task, and if the outcome variable is numeric, we are performing a **regression** task. As we see in @tbl-pda-algorithms, there are various algorithms that can be used for each task.

| Classification         | Regression                | Learner type  |
|:-----------------------|:--------------------------|:--------------|
| Logistic Regression    | Linear Regression         | Interpretable |
| Decision Tree          | Regression Tree           | Interpretable |
| Support Vector Machine | Support Vector Regression | Black box     |
| Multilayer Perceptron  | Multilayer Perceptron     | Black box     |

: Some common supervised learning algorithms used in PDA. {#tbl-pda-algorithms tbl-colwidths="[33, 33, 33]" .sm .striped}

I've included a column in @tbl-pda-algorithms that charaterizes a second consideration which is whether we want an interpretable model or a black box model. When talking about whether a model is interpretable or not, we are not referring to the evaluation of the accuracy of the model. Rather, we are referring to the inner workings of the model itself that allow us to understand how the model is making its predictions. An **interpretable model** is one that can be understood and explored by humans, while a **black box model** is one whose inner workings are not trivially unraveled. The advantage of an interpretable model is that it researchers can go beyond evaluating prediction accuracy and probe feature-outcome associations. On the other hand, if the goal is to simply boost prediction accuracy, interpretability may not be a concern.


<!--

- [ ] Incorporate the previous text with the above text, modifying the table so that it is not so cluttered as it includes 'Interpretable' out of nowhere.

Previous:

Another consideration to take into account is the whether the researcher aims to go beyond simply using an algorithm to make accurate predictions, but also wants to understand how the algorithm made its predictions and what contribution features made in the process. There are algorithms that produce models that allow the researcher to peer into and understand its inner workings (e.g. logistic regression, nave bayes classifiers, inter alia) and those that do not (e.g. neural networks, support vector machines, inter alia). Those that do not are called **'black-box' algorithms**. Neither type assures the best prediction accuracy. Important trade-offs need to be considered, however, if the best prediction comes from a black-box method, but the goal of the research is to understand the contribution of the features to the model's predictions.

-->

Finally, there are a number of algorithm-specific strengths and weaknesses to be considered in the process of model selection. These hinge on characteristics of the data, such as the size of the dataset, the number of features, the type of features, and the expected type of relationships between features or on computing resources, such as the amount of time available to train the model or the amount of memory available to store the model.

<!--
- Results
  - Predictive accuracy
    - Accuracy, precision, recall, F1-score
    - MSE, RMSE, MAE, R2
-->

**Model evaluation** is the process of assessing the accuracy of the model on the test set, which is a proxy for how well the model will generalize to new data. Model evaluation is performed quantitatively by calculating the accuracy of the model on the training, to develop the model, and ultimately, the test set. The accuracy of a model is calculated by comparing the predicted values to the actual values. For the results of classification tasks, this results in a contingency table, known as a confusion matrix. A **confusion matrix** juxtaposes predicted and actual values allowing various metrics to be calculated, for example in @tbl-aa-confusion-matrix.

<!-- [ ] Add second 'metrics' table beside -->

|                        | Predicted: natives   | Predicted: nonnatives |
|:-----------------------|:---------------------|:----------------------|
| **Actual: natives**    | 26294 (90% of 29215) | 2921 (10% of 29215)   |
| **Actual: nonnatives** | 730 (10% of 7304)    | 6574 (90% of 7304)    |

: Confusion matrix for the utterance type classification task. {#tbl-aa-confusion-matrix}{.sm .striped}

Since regression tasks predict numeric values, the accuracy of the model is calculated by comparing the difference between the predicted and actual values.

It is important to note that whether the accuracy metrics are good is to some degree qualitative judgment. For example, classification accuracy overall may be relatively high, but the model may be performing poorly on one of the classes. In this case, the model may not be useful for the task at hand, despite the overall accuracy.

In the end, PDA offers a versitle path to discover data-driven insights, to probe theory-driven associations, or even simply to perform tasks that are too complex or time-consuming for humans to perform.

### Infer {#sec-aa-infer}

<!--
- Purpose
  - Generalizing from a sample to a population
  - theory-driven, hypothesis-testing
  - test relationships between variables based on theory
  - draw conclusions about the population (extrapolate - from the sample)
-->

The most commonly recognized of the three data analysis approaches, **Inferential data analysis (IDA)** is the bread-and-butter of science. IDA is a deductive, theory-driven approach in which all aspects of analysis stem from a pre-determined premise, or hypothesis, about the nature of a relationship in the world and then aims to test whether this relationship is statistically supported given the evidence. Since the goal is to infer conclusions about a certain relationship in the population based on a statistical evaluation of a (corpus) sample, the representativeness of the sample is of utmost importance. Furthermore, the use of the data is limited to the scope of the hypothesis --that is, the data cannot be used for exploratory purposes.


<!-- So, if a researcher's aim is to draw conclusions that generalize, then, this is the analysis approach a researcher will take. -->

<!-- Identify variables -->

The selection of variables and the roles they play in the analysis are determined by the hypothesis. In a nutshell, a **hypothesis** is a formal statement about the state of the world. This statement is theory-driven meaning that it is predicated on previous research. We are not exploring or examining relationships, rather we are testing a specific relationship. In practice, however, we are in fact proposing two mutally exclusive hypotheses. The first is the **Alternative Hypothesis**, or $H_1$. This is the hypothesis I just described --the statement grounded in the previous literature outlining a predicted relationship. The second is the **Null Hypothesis**, or $H_0$. This is the flip-side of the hypothesis testing coin and states that there is no difference or relationship. Together $H_1$ and $H_0$ cover all logical outcomes.

To connect hypotheses to variable selection and variable roles, let's consider a study in which a researcher is investigating the claim that men and women differ in terms of the number of questions they use in spontaneous conversations. The unit of analysis is individuals (i.e. men and women) and the unit of observation is (sponteanous) conversations.

A dataset based on the Switchboard Dialog Act Corpus (SWDA) [@SWDA2008], seen in @tbl-aa-ida-swda-dataset, aligns well with this investigation. It is a large collection of transcribed telephone conversations between strangers. The dataset includes gender information for each participant and dialog act annotation for each utterance, including a range of question types.

```{r}
#| label: tbl-aa-ida-swda-dataset
#| tbl-cap: 'Data dictionary of the SWDA dataset.'

swda_curated_tbl <-
  read_csv("data/swda_curated.csv") |>
  select(
    doc_id,
    speaker_id,
    sex,
    damsl_tag,
    utterance_text
  )

swda_curated_dd <-
  create_data_dictionary(
  data = swda_curated_tbl,
  file_path = "data/aa-swda_curated_dd.csv",
  model = "gpt-3.5-turbo"
)

# SWDA data dictionary ----
read_csv("data/aa-swda_curated_dd.csv") |>
  tt(width = 1)
```

The Alternative Hypothesis may be formulated in this way:

$H_1$: Men and women differ in the frequency of the use of questions in spontaneous conversations.

The Null Hypothesis, then, would be a statement describing the remaining logical outcomes. Specifically:

$H_0$: Men and women do *not* differ in the frequency of the use of questions in spontaneous conversations.

Now, in standard IDA one variable is the dependent variable and one or more variables are predictor variables. The **dependent variable**, sometimes referred to as the outcome or response variable, is the variable which contains the information which is hypothesized to depend on the information in the predictor variable(s). It is the variable whose variation a research study seeks to explain. A **predictor variable**, sometimes referred to as a independent or explanatory variable, is a variable whose variation is hypothesized to explain the variation in the dependent variable.

Returning to our hypothetical study and the hypotheses presented, we can identify the variables in our study and map them to their roles. The frequency of questions used by each speaker would be our dependent variable and the biological sex of the speakers our predictor variable. This is so because $H_1$ states the proposition that a speaker's sex will predict the frequency of questions used. The next step would be to operationalize what we mean by 'frequency of questions' and then transform the dataset to reflect this definition.

In our hypothetical study we've identified two variables, one dependent and one predictor. It is important keep in mind that there can be multiple predictor variables in cases where the dependent variable's variation is predicted to be related to multiple variables. This relationship would need to be explicitly part of the original hypothesis, however. Due to the increasing difficulty for interpretation, in practice, IDA studies rarely include more than two or three predictor variables in the same analysis.

Predictor variables add to the complexity of a study because they are part of our research focus, specifically our hypothesis. It is, however, common to include other variables which are not of central focus, but are commonly assumed to contribute to the explanation of the variation of the dependent variable. Let's assume that the background literature suggests that the age of speakers also plays a role in the number of questions that men and women use in spontaneous conversation. Let's also assume that the data we have collected includes information about the age of speakers. If we would like to factor out the potential influence of age on the use of questions and focus on the particular predictor variables we've defined in our hypothesis, we can include the age of speakers as a **control variable**. A control variable will be added to the statistical analysis and documented in our report but it will not be included in the hypothesis nor interpreted in our results.

We can now see in @fig-aa-ida-variables the variables roles assigned to variables in a hypothesis-driven study.

```{r}
#| label: fig-aa-ida-variables
#| fig-cap: 'Roles of variables in IDA.'
#| out.width: '75%'

knitr::include_graphics("figures/aa-ida-variables.drawio.png")
```

<!--
- Methods
  - Hypothesis testing
    - Univariate
      - Categorical: Chi-square test
      - Numeric: T-test, Mann-Whitney U test
    - Monofactorial:
      - Categorical: Chi-square test, logistic regression
      - Numeric: T-test, Kruskal-Wallis test, linear regression
    - Multifactorial:
      - Categorical: Logistic regression
      - Numeric: Linear regression
-->


At this point let's look at the main characteristics that need to be taken into account to statistically interrogate the variables we have chosen to test our hypothesis. The type of statistical test that one chooses is based on (1) the informational value of the dependent variable and (2) the number of predictor variables included in the analysis. Together these two characteristics go a long way in determining the appropriate class of statistical test, but other considerations about the distribution of particular variables (i.e. normality), relationships between variables (i.e. independence), and expected directionality of the predicted effect may condition the appropriate method to be applied.

As you can imagine, there are a host of combinations and statistical tests that apply in particular scenarios, too many to consider in given the scope of this coursebook (see @Gries2013a and @Paquot2020a for a more exhaustive description). Below I've summarized some common statistical scenarios and their associated tests which focus on the juxtaposition of informational values and the number of variables, leaving aside alternative tests which deal with non-normal distributions, ordinal variables, *etc.*


[ ] change these to simulation-based tests (diff in means, diff in proportions, etc. )

In @tbl-ida-methods-monofactorial we see **monofactorial tests**, tests with only one predictor variable.

```{r}
#| label: tbl-ida-methods-monofactorial
#| tbl-cap: 'Common monofactorial tests used in IDA.'

monofactorial <-
  tribble(
    ~"Dependent", ~"Predictor", ~"Test",
    "Categorical", "Categorical", "Pearson's Chi-squared test",
    "Numeric", "Categorical", "Student's t-Test",
    "Numeric", "Numeric", "Pearson's correlation test"
  )
monofactorial |>
  kable() |>
  kable_styling() |>
  add_header_above(c("Variable roles" = 2, " "))
```

@tbl-ida-methods-multifactorial includes a listing of **multifactorial tests**, tests with more than one predictor and/ or control variables.

```{r}
#| label: tbl-ida-methods-multifactorial
#| tbl-cap: 'Common multifactorial tests used in IDA.'

multifactorial <-
  tribble(
    ~"Dependent", ~"Predictor", ~"Control", ~"Test",
    "Categorical", "varied", "varied", "Logistic regression",
    "Numeric", "varied", "varied", "Linear regression"
  )
multifactorial |>
  kable() |>
  column_spec(2:3, italic = TRUE) |>
  add_header_above(c("Variable roles" = 3, " "))
```

<!--
- Results
  - Statistical significance
    - P-value
-->

IDA relies heavily on quantitative evaluation methods to draw conclusions that can be generalized to the target population. It is key to understand that our goal in hypothesis testing is not to find evidence in support of $H_1$, but rather to assess the likelihood that we can reliably reject $H_0$. The metric used to determine if there is sufficient evidence is based on the probability that given the nature of the relationship and the characteristics of the data, the likelihood of there being no difference or relationship is low. The threshold for likelihood has traditionally been summarized in the $p$-value statistic. In the Social Sciences, a $p$-value lower than .05 is considered *statistically significant* which when interpreted correctly means that there is more than a 95% chance that the observed relationship would not be predicted by $H_0$. Note that we are working in the realm of probability, not in absolutes, therefore an analysis that produces a significant result does not prove $H_1$ is correct or that $H_0$ is incorrect, for that matter. A margin of error is always present. For this reason, other metrics such as effect size and confidence intervals are also used to interpret the results of statistical tests.

## Communicate {#sec-aa-communicate}

<!-- Purpose -->

Conducting research should be enjoyable and personally rewarding but the effort you have invested and knowledge you have generated should be shared with others. Whether part of a blog, presentation, journal article, or for your own purposes it is important to document your analysis results and process in a way that is informative and interpretable. This enhances the value of your work, allowing others to learn from your experience and build on your findings.

### Report {#sec-aa-report}

<!-- Purpose -->

The most widely recognized form of communicating research is through a report. A report is a narrative of your analysis, including the research question, the data you used, the methods you applied, and the results you obtained. We are both reporting our findings and documenting our process to inform others of what we did and why we did it but also to invite readers to evaluate our findings for themselves. The scientific process is a collaborative one and evaluation by peers is a key component of the process.

<!-- Components -->

The audience for your report will determine the level of detail and the type of information you will need to include in your report but there are some common elements to reference in any report. First, the research question and/ or hypothesis should be clearly stated and the motivation for the question should be explained. This will help the reader understand the context of the analysis and the importance of the results. Second, diagnostic procedures to verifiy or describe the data should be explained. This may include anomaly correction, missing data, data transformation, etc. and/ or descriptive summaries of the data including assessments of individual variables (central tendency, dispersion, distribution) and/ or relationships between variables (association strength). Third, a blueprint of the methods used will describe the variable selection process, how the variables are operationalized, what analysis methods are employed, and how the variables are used in the statistical analysis. Fourth, the results from the analysis are reported. Reporting details will depend on the type of analysis and the particular method(s) employed. For inferential analyses this will include the test statistic(s) and some measure of confidence. In predictive analyses, accuracy results will be reported. For exploratory analyses, the reporting of results will vary and often include visualizations and metrics that require more human interpretation than the other analysis types. Finally, the results are interpreted in light of the research question and/ or hypothesis. This will include a discussion of the limitations of the analysis and a discussion of the implications of the results for future research.

### Document {#sec-aa-document}

<!-- Purpose -->

While a good report will include the most vital information to understand the procedures, results, and findings of an analysis, there is much more information generated in the course of an analysis which does not traditionally appear in prose. If a research project is conducted programmatically, however,  data, code, and documentation can be made available to others as part of the communication process. Increasingly, researchers are sharing their data and code as part of the publication process. This allows others to reproduce the analysis and verify the results contributing to the collaborative nature of the scientific process. {{< fa wrench >}} [CITATION]

<!-- Research compendium -->

Together, data, code, and documentation form a **research compendium**. As you can imagine the research process can quickly become complex and unwieldy as the number of files and folders grows. If not organized properly, it can be difficult to find the information you need. Furthermore, if not documented, decisions made in the course of the analysis can be difficult or impossible to trace. For this reason it is recommendable to follow a set of best practices for organizing and documenting your research compendium.

<!-- Components -->

We will have more to say about this in the next chapter but for now it will suffice to point to some key elements in a research compendium. First, the data used in the analysis should be saved as a separate file(s). As a given research project progresses to analysis, the data may be transformed and manipulated to best fit the needs of the analysis. Preserving the data at each stage adds to the complete picture of the data from collection to analysis. Second, since you are working programmatically, you can share your precise analysis step-by-step in code form. This allows others to reproduce your analysis and verify your results. Including code comments provides additional information to communicate the steps taken and your thought process. Finally, a codebook documents any additional information that helps understand the research better. This will often include guides for installing software and running the code to reproduce the analysis and an overview of the aims of the scripts and the contents of the data and datasets.

## Activities {.unnumbered}

In the following activies, we will build on our understanding of how to summarize data using statistics, tables, and plots. We will dive deeper into the use of the `skimr` package to summarize data and the `ggplot2` package to create plots. We also introduce producing Quarto tables and figures with appropriate code block options. We will reinforce our understanding of the `readr` package to read in data and the `dplyr` package to manipulate data.

::: {.callout}
**{{< fa regular file-code >}} Recipe**

**What**: [Descriptive assessment of datasets](https://qtalr.github.io/qtalrkit/articles/recipe-3.html)\
**How**: Read Recipe 3 and participate in the Hypothes.is online social annotation.\
**Why**: To explore appropriate methods for summarizing variables in datasets given the number and informational values of the variable(s).
:::

::: {.callout}
**{{< fa flask >}} Lab**

**What**: [Trace the datascape](https://github.com/qtalr/lab-03)\
**How**: Clone, fork, and complete the steps in Lab 3.\
**Why**: To identify and apply the appropriate descriptive methods for a vector's informational value and to assess both single variables and multiple variables with the appropriate statistical, tabular, and/ or graphical summaries.
:::


## Summary {.unnumbered}

In this chapter we have focused on description and analysis --the third component of DIKI Hierarchy. This process is visually summarized in @fig-approaching-analysis-vis-sum.

```{r}
#| label: fig-approaching-analysis-vis-sum
#| fig-cap: 'Approaching analysis: visual summary'
#| out.width: '75%'

knitr::include_graphics("figures/aa-diki.drawio.png")
```

The first key step in any analysis is to perform a diagnostic assessment of the individual variables and relationships between variables. To select the appropriate descriptive measures we covered the various informational values that a variable can take. In addition to providing key information for reporting purposes, descriptive measures are important to explore so the researcher can get a better feel for the dataset before conducting an analysis.

We outlined three data analysis types in this chapter: exploratory, predictive, and inferential. Each of these embodies distinct approaches to deriving knowledge from data. Ultimately the choice of analysis type is highly dependent on the goals of the research. Inferential analysis is centered around the goal of testing a hypothesis, and for this reason it is the most highly structured approach to analysis. This structure is aimed at providing the mechanisms to draw conclusions from the results that can be generalized to the target population. Predictive analysis has a less-ambitious but at times more relevant goal of examining the extent to which a given relationship can be established from the data to provide a model of language that can accurately predict an outcome using new data. This methodology is highly effective for applying different algorithmic approaches and examining relationships between an outcome variable and various configurations of variables. The ability to explore the data in multiple ways, is also a key strength of employing an exploratory analysis. The least structured and most variable of the analysis types, exploratory analyses are a powerful approach to generating knowledge from data in an area where clear predictions cannot be made.

I rounded out this chapter with a short description of the importance of communicating the analysis process and results. Reporting, in its traditional form, is documented in prose in an article. This reporting aims to provide the key information that a reader will need to understand what was done, how it was done, and why it was done. This information also provides the necessary information for reader's with a critical eye to understand the analysis in more detail. Yet even the most detailed reporting in a write-up still leaves many practical, but key, points of the analysis obscured. A programming approach provides the procedural steps taken that when shared provide the exact methods applied. Together with the write-up, a research compendium which provides the scripts to run the analysis and documentation on how to run the analysis forms an integral part of creating reproducible research.

<!-- Additional references:

- Roberts, S., & Winters, J. (2013). Linguistic Diversity and Traffic Accidents: Lessons from Statistical Studies of Cultural Traits. PLOS ONE, 8(8), e70902. https://doi.org/10.1371/journal.pone.0070902
  - This reference has a lot of good examples of spurious correlations. Nomothetic studies (large-scale statistical analysis of cross-cultural data) can often lead to spurious correlations that have little to no relationship. Point that correlation does not mean causation
- Lantz, B. (2013). Machine learning with R. Packt Publishing.
  - Good overview of machine learning and R
-->
