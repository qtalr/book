<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>8&nbsp; Explore – An Introduction to Quantitative Text Analysis for Linguistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_4/9_predict.html" rel="next">
<link href="../part_4/index.html" rel="prev">
<link href="../assets/images/logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><script data-goatcounter="https://italicize.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../assets/css/mini.css">
<meta property="og:title" content="8&nbsp; Explore – An Introduction to Quantitative Text Analysis for Linguistics">
<meta property="og:description" content="">
<meta property="og:site_name" content="An Introduction to Quantitative Text Analysis for Linguistics">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title"><span id="sec-explore-chapter" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Explore</span></span></h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">An Introduction to Quantitative Text Analysis for Linguistics</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/qtalr/book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../qtalr-manuscript.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_0/preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Orientation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/1_text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Text analysis</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/2_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/3_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/4_research.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Research</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preparation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/5_acquire.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Acquire</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/6_curate.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Curate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/7_transform.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transform</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/8_explore.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Explore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/9_predict.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Predict</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/10_infer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Infer</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/11_contribute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Contribute</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Contents</h2>
   
  <ul>
<li><a href="#sec-explore-orientation" id="toc-sec-explore-orientation" class="nav-link active" data-scroll-target="#sec-explore-orientation"><span class="header-section-number">8.1</span> Orientation</a></li>
  <li>
<a href="#sec-explore-analysis" id="toc-sec-explore-analysis" class="nav-link" data-scroll-target="#sec-explore-analysis"><span class="header-section-number">8.2</span> Analysis</a>
  <ul class="collapse">
<li><a href="#sec-explore-descriptive" id="toc-sec-explore-descriptive" class="nav-link" data-scroll-target="#sec-explore-descriptive">Descriptive analysis</a></li>
  <li><a href="#sec-explore-unsupervised" id="toc-sec-explore-unsupervised" class="nav-link" data-scroll-target="#sec-explore-unsupervised">Unsupervised learning</a></li>
  </ul>
</li>
  <li><a href="#activities" id="toc-activities" class="nav-link" data-scroll-target="#activities">Activities</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/qtalr/book/blob/main/part_4/8_explore.qmd" target="_blank" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/qtalr/book/edit/main/part_4/8_explore.qmd" target="_blank" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/qtalr/book/issues" target="_blank" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title d-none d-lg-block"><span id="sec-explore-chapter" class="quarto-section-identifier"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Explore</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-list-alt" aria-label="list-alt"></i> Outcomes</strong></p>
<ul>
<li>Determine the suitability of exploratory data analysis for a research project.</li>
<li>Understand descriptive analysis and unsupervised learning methods, their strengths in pattern recognition and data summarization.</li>
<li>Interpret insights from data summarization and pattern recognition, considering their potential to guide further research.</li>
</ul>
</div>
</div>
</div>
<p>In this chapter, we examine a wide range of strategies for exploratory data analysis. The chapter outlines two main branches of exploratory data analysis: descriptive analysis which statistically and/or visually summarizes a dataset and unsupervised learning which is a machine learning approach that does not assume any particular relationship between variables in a dataset. Either through descriptive or unsupervised learning methods, exploratory data analysis employs quantitative methods to summarize, reduce, and sort complex datasets and statistically and visually interrogate a dataset in order to provide the researcher novel perspective to be qualitatively assessed.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-terminal" aria-label="terminal"></i> Lessons</strong></p>
<p><strong>What</strong>: Advanced objects<br><strong>How</strong>: In an R console, load {swirl}, run <code>swirl()</code>, and follow prompts to select the lesson.<br><strong>Why</strong>: To learn about advanced objects in R, including lists and matrices, and create, inspect, access, and manipulate these objects.</p>
</div>
</div>
</div>
<section id="sec-explore-orientation" class="level2" data-number="8.1"><h2 data-number="8.1" class="anchored" data-anchor-id="sec-explore-orientation">
<span class="header-section-number">8.1</span> Orientation</h2>
<p>The goal of exploratory data analysis is to discover, describe, and posit new hypotheses. This analysis approach is best-suited for research questions where the literature is scarce, where the gap in knowledge is wide, or where new territories are being explored. The researcher may not know what to expect, but they are willing to let the data speak for itself. The researcher is open to new insights and new questions that may emerge from the analysis process.</p>
<p>While exploratory data analysis allows flexibility, it is essential to have a guiding research question that provides a focus for the analysis. This question will help to determine the variables of interest and the methods to be used. The research question will also help to determine the relevance of the results and the potential for the results to be used in further research.</p>
<p>The general workflow for exploratory data analysis is shown in <a href="#tbl-explore-workflow" class="quarto-xref">Table&nbsp;<span>8.1</span></a>.</p>
<div id="tbl-explore-workflow" class="quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[5, 15, 80]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.1: Workflow for exploratory data analysis
</figcaption><div aria-describedby="tbl-explore-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 5%">
<col style="width: 15%">
<col style="width: 80%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">Step</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: left;">Identify</td>
<td style="text-align: left;">Consider the research question and identify variables of potential interest to provide insight into our question.</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: left;">Inspect</td>
<td style="text-align: left;">Check for missing data, outliers, <em>etc</em>. and check data distributions and transform if necessary.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: left;">Interrogate</td>
<td style="text-align: left;">Submit the selected variables to descriptive (frequency, keyword, co-occurrence analysis, <em>etc.</em>) or unsupervised learning (clustering, dimensionality reduction, vector spacing modeling, <em>etc.</em>) methods to provide quantitative measures to evaluate.</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: left;">Interpret</td>
<td style="text-align: left;">Evaluate the results and determine if they are valid and meaningful to respond to the research question.</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: left;">Iterate (Optional)</td>
<td style="text-align: left;">Repeat steps 1-4 as new questions emerge from your interpretation.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-explore-analysis" class="level2" data-number="8.2"><h2 data-number="8.2" class="anchored" data-anchor-id="sec-explore-analysis">
<span class="header-section-number">8.2</span> Analysis</h2>
<p>To frame our demonstration and discussion of exploratory data analysis, let’s tackle a task. The task will be to identify relevant materials for an English- language learner (ELL) textbook. This will involve multiple research questions and allow us to illustrate some very fundamental concepts that emerge across text analysis research in both descriptive and unsupervised learning approaches.</p>
<p>Since our task is geared towards English language use, we will want a representative data sample. For this, we will use the Manually Annotated Sub-Corpus of American English (MASC) of the American National Corpus <span class="citation" data-cites="Ide2008">(<a href="../references.html#ref-Ide2008" role="doc-biblioref">Ide, Baker, Fellbaum, Fillmore, &amp; Passonneau, 2008</a>)</span>.</p>
<p>The data dictionary for the dataset we will use as our point of departure is shown in <a href="#tbl-explore-masc-dd-show" class="quarto-xref">Table&nbsp;<span>8.2</span></a>.</p>
<!-- Show data dictionary -->
<div class="cell" data-tbl-colwidths="[15,15,15,55]">
<div id="tbl-explore-masc-dd-show" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[15,15,15,55]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-dd-show-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.2: Data dictionary for the MASC dataset
</figcaption><div aria-describedby="tbl-explore-masc-dd-show-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 55%">
</colgroup>
<thead><tr class="header">
<th>variable</th>
<th>name</th>
<th>type</th>
<th>description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>doc_id</td>
<td>Document ID</td>
<td>numeric</td>
<td>Unique identifier for each document</td>
</tr>
<tr class="even">
<td>modality</td>
<td>Modality</td>
<td>categorical</td>
<td>The form in which the document is presented (written or spoken)</td>
</tr>
<tr class="odd">
<td>genre</td>
<td>Genre</td>
<td>categorical</td>
<td>The category or type of the document</td>
</tr>
<tr class="even">
<td>term_num</td>
<td>Term Number</td>
<td>numeric</td>
<td>Index number term per document</td>
</tr>
<tr class="odd">
<td>term</td>
<td>Term</td>
<td>categorical</td>
<td>Individual word forms in the document</td>
</tr>
<tr class="even">
<td>lemma</td>
<td>Lemma</td>
<td>categorical</td>
<td>Base or dictionary form of the term</td>
</tr>
<tr class="odd">
<td>pos</td>
<td>Part of Speech</td>
<td>categorical</td>
<td>Grammatical category of the term (modified PENN Treebank tagset)</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<!-- Load the MASC dataset/ preview -->
<p>First, I’ll read in and preview the dataset in <a href="#exm-explore-masc-read" class="quarto-xref">Example&nbsp;<span>8.1</span></a>.</p>
<div id="exm-explore-masc-read" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.1</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Read the dataset</span></span>
<span id="cb1-2"><a href="#cb1-2"></a>masc_tbl <span class="ot">&lt;-</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>  <span class="fu">read_csv</span>(<span class="st">"../data/masc/masc_transformed.csv"</span>)</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Preview the MASC dataset</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="fu">glimpse</span>(masc_tbl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
<!--Add space to push output to next page-->
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 591,036
Columns: 7
$ doc_id   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
$ modality &lt;chr&gt; "Written", "Written", "Written", "Written", "Written", "Writt…
$ genre    &lt;chr&gt; "Letters", "Letters", "Letters", "Letters", "Letters", "Lette…
$ term_num &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…
$ term     &lt;chr&gt; "December", "1998", "Your", "contribution", "to", "Goodwill",…
$ lemma    &lt;chr&gt; "december", "1998", "your", "contribution", "to", "goodwill",…
$ pos      &lt;chr&gt; "NNP", "CD", "PRP$", "NN", "TO", "NNP", "MD", "VB", "JJR", "I…</code></pre>
</div>
</div>
<p>From the output in <a href="#exm-explore-masc-read" class="quarto-xref">Example&nbsp;<span>8.1</span></a>, we get some sense of the structure of the dataset. However, we also need to perform diagnostic and descriptive procedures. This will include checking for missing data and anomalies and assessing central tendency, dispersion, and/or distributions of the variables. This may include using {skimr}, {dplyr}, {stringr}, {ggplot2}, <em>etc.</em> to identify the most relevant variables for our task and to identify any potential issues with the dataset.</p>
<p>After a descriptive and diagnostic assessment of the dataset, not included here, I identified and addressed missing data and anomalies (including many non-words). I also recoded the <code>doc_id</code> variable to a character variable. The dataset now has 486,368 observations, a reduction from the original 591,036 observations. There are 392 documents, 2 modalities, 18 genres, almost 38k unique terms (which are words), almost 26k lemmas, and 34 distinct POS tags.</p>
<section id="sec-explore-descriptive" class="level3"><h3 class="anchored" data-anchor-id="sec-explore-descriptive">Descriptive analysis</h3>
<p>Descriptive analysis includes common techniques such as frequency analysis to determine the most frequent words or phrases, dispersion analysis to see how terms are distributed throughout a document or corpus, keyword analysis to identify distinctive terms, and/or co-occurrence analysis to see what terms tend to appear together.</p>
<p>Using the MASC dataset, we will entertain questions such as:</p>
<ul>
<li>What are the most common terms a beginning ELL should learn?</li>
<li>Are there term differences between spoken and written discourses that should be emphasized?</li>
<li>What are some of the most common verb particle constructions?</li>
</ul>
<p>Along the way, we will discuss frequency, dispersion, and co-occurrence measures. In addition, we will apply various descriptive analysis techniques and visualizations to explore the dataset and identify new questions and new variables of interest.</p>
<section id="sec-explore-frequency" class="level4"><h4 class="anchored" data-anchor-id="sec-explore-frequency">Frequency analysis</h4>
<!-- 4 I's: identify, inspect, interrogate, interpret -->
<p>At its core, frequency analysis is a descriptive method that counts the number of times a linguistic unit occurs in a dataset. The results of frequency analysis can be used to describe the dataset and to identify terms that are linguistically distinctive or distinctive to a particular group or sub-group in the dataset.</p>
<!--- Raw frequency (counting) --->
<section id="sec-explore-frequency-raw" class="level5"><h5 class="anchored" data-anchor-id="sec-explore-frequency-raw">Raw frequency</h5>
<p>Let’s consider what the most common words in the MASC dataset are as a starting point to making inroads on our task by identifying relevant vocabulary for an ELL textbook.</p>
<p>In the <code>masc_tbl</code> data frame we have the linguistic unit <code>term</code> which corresponds to the word-level annotation of the MASC. The <code>lemma</code> corresponds to the base form of each term, for words with inflectional morphology, the lemma is the word sans the inflection (<em>e.g.</em> is/be, are/be). For other words, the <code>term</code> and the <code>lemma</code> will be the same (<em>e.g.</em> the/the, in/in). These two variables pose a choice point for us: do we consider words to be the actual forms or the base forms? There is an argument to be made for both. In this case, I will operationalize our linguistic unit as the <code>lemma</code> variable, as this will allow us to group words with distinct inflectional morphology together.</p>
<p>To perform a basic word frequency analysis, we can apply <code>summarize()</code> in combination with <code>n()</code> or the convenient <code>count()</code> function from {dplyr}. Our sorted lemma counts appear in <a href="#exm-explore-masc-count" class="quarto-xref">Example&nbsp;<span>8.2</span></a>.</p>
<div id="exm-explore-masc-count" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.2</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Lemma count, sorted in descending order</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>masc_tbl <span class="sc">|&gt;</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="fu">count</span>(lemma, <span class="at">sort =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 25,923 × 2
   lemma     n
   &lt;chr&gt; &lt;int&gt;
 1 the   26137
 2 be    19466
 3 to    13548
 4 and   12528
 5 of    12005
 6 a     10461
 7 in     8374
 8 i      7783
 9 that   7082
10 you    5276
# ℹ 25,913 more rows</code></pre>
</div>
</div>
<p> </p>
</div>
<p>The output of this raw frequency tabulation in <a href="#exm-explore-masc-count" class="quarto-xref">Example&nbsp;<span>8.2</span></a> is a data frame with two columns: <code>lemma</code> and <code>n</code>.</p>
<p>As we discussed in <a href="../part_2/3_analysis.html#sec-analysis-distributions" class="quarto-xref"><span>Section 3.1.3</span></a>, the frequency of linguistic units in a corpus tends to be highly right-skewed distribution, approximating the Zipf distribution. If we calculate the <strong>cumulative frequency</strong>, a rolling sum of the frequency term by term, of the lemmas in the <code>masc_tbl</code> data frame, we can see that the top 10 types account for around 25% of the lemmas used in the entire corpus —by 100 types that increases to near 50% and 1,000 around 75%, as seen in <a href="#fig-explore-masc-count-cumulative" class="quarto-xref">Figure&nbsp;<span>8.1</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-count-cumulative" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A line plot showing the cumulative frequency of lemmas in the MASC dataset. Vertical lines at 10, 100, and 1000 types show that the top 10 lemmas account for 25% of the corpus, 100 lemmas account for 50% of the corpus, and 1000 lemmas account for 75% of the corpus.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-count-cumulative-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-count-cumulative-1.png" class="img-fluid figure-img" alt="A line plot showing the cumulative frequency of lemmas in the MASC dataset. Vertical lines at 10, 100, and 1000 types show that the top 10 lemmas account for 25% of the corpus, 100 lemmas account for 50% of the corpus, and 1000 lemmas account for 75% of the corpus." width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-count-cumulative-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Cumulative frequency of lemmas
</figcaption></figure>
</div>
</div>
</div>
<p>If we look at the types that appear within the first 50 most frequent, you can likely also appreciate another thing about language use. Let’s list the top 50 types in <a href="#tbl-explore-masc-count-top-50" class="quarto-xref">Table&nbsp;<span>8.3</span></a>.</p>
<div class="cell" data-tbl-colwidths="[20,20,20,20,20]">
<div id="tbl-explore-masc-count-top-50" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[20,20,20,20,20]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-count-top-50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.3: Top 50 lemma types
</figcaption><div aria-describedby="tbl-explore-masc-count-top-50-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">the</td>
<td style="text-align: left;">have</td>
<td style="text-align: left;">at</td>
<td style="text-align: left;">your</td>
<td style="text-align: left;">all</td>
</tr>
<tr class="even">
<td style="text-align: left;">be</td>
<td style="text-align: left;">it</td>
<td style="text-align: left;">from</td>
<td style="text-align: left;">an</td>
<td style="text-align: left;">there</td>
</tr>
<tr class="odd">
<td style="text-align: left;">to</td>
<td style="text-align: left;">for</td>
<td style="text-align: left;">he</td>
<td style="text-align: left;">say</td>
<td style="text-align: left;">me</td>
</tr>
<tr class="even">
<td style="text-align: left;">and</td>
<td style="text-align: left;">on</td>
<td style="text-align: left;">but</td>
<td style="text-align: left;">what</td>
<td style="text-align: left;">would</td>
</tr>
<tr class="odd">
<td style="text-align: left;">of</td>
<td style="text-align: left;">do</td>
<td style="text-align: left;">by</td>
<td style="text-align: left;">so</td>
<td style="text-align: left;">about</td>
</tr>
<tr class="even">
<td style="text-align: left;">a</td>
<td style="text-align: left;">with</td>
<td style="text-align: left;">will</td>
<td style="text-align: left;">his</td>
<td style="text-align: left;">know</td>
</tr>
<tr class="odd">
<td style="text-align: left;">in</td>
<td style="text-align: left;">we</td>
<td style="text-align: left;">my</td>
<td style="text-align: left;">if</td>
<td style="text-align: left;">get</td>
</tr>
<tr class="even">
<td style="text-align: left;">i</td>
<td style="text-align: left;">as</td>
<td style="text-align: left;">or</td>
<td style="text-align: left;">’s</td>
<td style="text-align: left;">make</td>
</tr>
<tr class="odd">
<td style="text-align: left;">that</td>
<td style="text-align: left;">this</td>
<td style="text-align: left;">n’t</td>
<td style="text-align: left;">can</td>
<td style="text-align: left;">out</td>
</tr>
<tr class="even">
<td style="text-align: left;">you</td>
<td style="text-align: left;">not</td>
<td style="text-align: left;">they</td>
<td style="text-align: left;">go</td>
<td style="text-align: left;">up</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>For the most part, the most frequent words are function words. <strong>Function words</strong> are a closed class of relatively few words that are used to express grammatical relationships between content words (<em>e.g.</em> determiners, prepositions, pronouns, and auxiliary verbs). Given the importance of these words, it then is no surprise that they comprise many of the most frequent words in a corpus.</p>
<p>Another key observation is that for those the content words (<em>e.g.</em> nouns, verbs, adjectives, adverbs) that do figure in the most frequent words, we find that they are quite generic semantically speaking. That is, they are words that are used in a wide range of contexts and take a wide range of meanings. Take for example the adjective ‘good’. It can be used to describe a wide range of nouns, such as ‘good food’, ‘good people’, ‘good times’, <em>etc</em>. A sometimes near-synonym of ‘good’, for example ‘good student’, is the word ‘studious’. Yet, ‘studious’ is not as frequent as ‘good’ as it is used to describe a narrower range of nouns, such as ‘studious student’, ‘studious scholar’, ‘studious researcher’, <em>etc</em>. In this way, ‘studious’ is more semantically specific than ‘good’.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>Based on what you now know about the expected distribution of words in a corpus, what if your were asked to predict what the most frequent English word used is in each U.S. State? What would you predict? How confident would you be in your prediction? What if you were asked to predict what the most frequent word used is in the language of a given country? What would you want to know before making your prediction?</p>
</div>
</div>
</div>
<p>So common across corpus samples, in some analyses these function words (and sometimes generic content words) are considered irrelevant and are filtered out. In our ELL materials task, however, we might exclude them for the simple fact that it will be a given that we will teach these words given their overall frequency. Let’s aim to focus solely on the content words in the dataset.</p>
<p>One approach to filtering out these words is to use a list of words to exclude, known as a <strong>stopwords</strong> lexicon. {tidytext} includes a data frame <code>stop_words</code> which includes stopword lexicons for English. We can select a lexicon from <code>stop_words</code> and use <code>anti_join()</code> to filter out the words that appear in the <code>word</code> variable from the <code>lemma</code> variable in the <code>masc_tbl</code> data frame.</p>
<p>Eliminating words in this fashion, however, may not always be the best approach. Available lists of stopwords vary in their contents and are determined by other researchers for other potential uses. We may instead opt to create our own stopword list that is tailored to the task, or we may opt to use a statistical approach based on their distribution in the dataset using frequency and/or dispersion measures.</p>
<p>For our case, however, we have another available strategy. Since our task is to identify relevant vocabulary, beyond the fundamental function words in English, we can use the POS tags to reduce our dataset to just the content words, that is nouns, verbs, adjectives, and adverbs. Using the Penn tagset as reference, we can create a vector with the POS tags we want to retain and then use the <code><a href="https://rdrr.io/r/stats/filter.html">filter()</a></code> function on the datasets. I will assign this new data frame to <code>masc_content_tbl</code> to keep it separate from our main data frame <code>masc_tbl</code>, seen in <a href="#exm-explore-masc-filter-pos" class="quarto-xref">Example&nbsp;<span>8.3</span></a>.</p>
<div id="exm-explore-masc-filter-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.3</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Penn Tagset for content words</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co"># Nouns: NN, NNS,</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># Verbs: VB, VBD, VBG, VBN, VBP, VBZ</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co"># Adjectives: JJ, JJR, JJS</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Adverbs: RB, RBR, RBS</span></span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a>content_pos <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"NN"</span>, <span class="st">"NNS"</span>, <span class="st">"VB"</span>, <span class="st">"VBD"</span>, <span class="st">"VBG"</span>, <span class="st">"VBN"</span>, <span class="st">"VBP"</span>, <span class="st">"VBZ"</span>, <span class="st">"JJ"</span>, <span class="st">"JJR"</span>, <span class="st">"JJS"</span>, <span class="st">"RB"</span>, <span class="st">"RBR"</span>, <span class="st">"RBS"</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co"># Select content words</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>masc_content_tbl <span class="ot">&lt;-</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>  masc_tbl <span class="sc">|&gt;</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>  <span class="fu">filter</span>(pos <span class="sc">%in%</span> content_pos)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</div>
<p>Let’s now preview the top 50 lemmas in the <code>masc_content_tbl</code> data frame to see how the most frequent words have changed in <a href="#tbl-explore-masc-filter-pos" class="quarto-xref">Table&nbsp;<span>8.4</span></a>.</p>
<div class="cell" data-tbl-colwidths="[20,20,20,20,20]">
<div id="tbl-explore-masc-filter-pos" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[20,20,20,20,20]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-filter-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.4: Frequency of tokens after filtering out lemmas with POS tags that are not content words
</figcaption><div aria-describedby="tbl-explore-masc-filter-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">be</td>
<td style="text-align: left;">think</td>
<td style="text-align: left;">work</td>
<td style="text-align: left;">also</td>
<td style="text-align: left;">t</td>
</tr>
<tr class="even">
<td style="text-align: left;">have</td>
<td style="text-align: left;">more</td>
<td style="text-align: left;">year</td>
<td style="text-align: left;">need</td>
<td style="text-align: left;">first</td>
</tr>
<tr class="odd">
<td style="text-align: left;">do</td>
<td style="text-align: left;">just</td>
<td style="text-align: left;">come</td>
<td style="text-align: left;">way</td>
<td style="text-align: left;">help</td>
</tr>
<tr class="even">
<td style="text-align: left;">not</td>
<td style="text-align: left;">time</td>
<td style="text-align: left;">use</td>
<td style="text-align: left;">back</td>
<td style="text-align: left;">day</td>
</tr>
<tr class="odd">
<td style="text-align: left;">n’t</td>
<td style="text-align: left;">so</td>
<td style="text-align: left;">well</td>
<td style="text-align: left;">here</td>
<td style="text-align: left;">many</td>
</tr>
<tr class="even">
<td style="text-align: left;">say</td>
<td style="text-align: left;">other</td>
<td style="text-align: left;">look</td>
<td style="text-align: left;">new</td>
<td style="text-align: left;">man</td>
</tr>
<tr class="odd">
<td style="text-align: left;">go</td>
<td style="text-align: left;">see</td>
<td style="text-align: left;">then</td>
<td style="text-align: left;">find</td>
<td style="text-align: left;">ask</td>
</tr>
<tr class="even">
<td style="text-align: left;">know</td>
<td style="text-align: left;">people</td>
<td style="text-align: left;">right</td>
<td style="text-align: left;">give</td>
<td style="text-align: left;">very</td>
</tr>
<tr class="odd">
<td style="text-align: left;">get</td>
<td style="text-align: left;">take</td>
<td style="text-align: left;">only</td>
<td style="text-align: left;">thing</td>
<td style="text-align: left;">much</td>
</tr>
<tr class="even">
<td style="text-align: left;">make</td>
<td style="text-align: left;">now</td>
<td style="text-align: left;">want</td>
<td style="text-align: left;">tell</td>
<td style="text-align: left;">even</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>The resulting list in <a href="#tbl-explore-masc-filter-pos" class="quarto-xref">Table&nbsp;<span>8.4</span></a> paints a different picture of the most frequent words in the dataset. The most frequent words are now content words, and included in most frequent words are more semantically specific words. We now have reduced the number of observations by 50% focusing on the content words. We are getting closer to identifying the vocabulary that we want to include in our ELL materials, but we will need some more tools to help us identify the most relevant vocabulary.</p>
<div style="page-break-after: always;"></div>
</section><section id="sec-explore-frequency-dispersion" class="level5"><h5 class="anchored" data-anchor-id="sec-explore-frequency-dispersion">Dispersion</h5>
<p><strong>Dispersion</strong> is a measure of how evenly distributed a linguistic unit is across a dataset. This is a key concept in text analysis, as important as frequency. It is important to recognize that frequency and dispersion are measures of different characteristics. We can have two words that occur with the same frequency, but one word may be more evenly distributed across a dataset than the other. Depending on the researcher’s aims, this may be an important distinction to make. For our task, it is likely the case that we want to capture words that are well-dispersed across the dataset, as words that have a high frequency and a low dispersion tend to be connected to a particular context, whether that be a particular genre, a particular speaker, a particular topic, <em>etc</em>. In other research, the aim may be the reverse; to identify words that are highly frequent and highly concentrated in a particular context to identify words that are distinctive to that context.</p>
<p>There are a variety of measures that can be used to estimate the distribution of types across a corpus. Let’s focus on three measures: document frequency (<span class="math inline">\(df\)</span>), inverse document frequency (<span class="math inline">\(idf\)</span>), and Gries’ Deviation of Proportions (<span class="math inline">\(dp\)</span>).</p>
<p>The most basic measure is <strong>document frequency</strong> (<span class="math inline">\(df\)</span>). This is the number of documents in which a type appears at least once. For example, if a type appears in 10 documents, then the document frequency is 10. This is a very basic measure, but it is a decent starting point.</p>
<p>A nuanced version of document frequency is <strong>inverse document frequency</strong> (<span class="math inline">\(idf\)</span>). This measure takes the total number of documents and divides it by the document frequency. This results in a measure that is inversely proportional to the document frequency. That is, the higher the document frequency, the lower the inverse document frequency. This measure is often log-transformed to spread out the values.</p>
<p>One thing to consider about <span class="math inline">\(df\)</span> and <span class="math inline">\(idf\)</span> is that neither takes into account the length of the documents in which the type appears nor the spread of each type within each document. To take these factors into account, we can use Gries’ <strong>deviation of proportions</strong> (<span class="math inline">\(dp\)</span>) measure <span class="citation" data-cites="Gries2023">(<a href="../references.html#ref-Gries2023" role="doc-biblioref">Gries, 2023, pp. 87–88</a>)</span>. The <span class="math inline">\(dp\)</span> measure considers the proportion of a type’s frequency in each document relative to its total frequency. This produces a measure that is more sensitive to the distribution of types within and across documents in a corpus.</p>
<div style="page-break-after: always;"></div>
<p>Let’s consider how these measures differ with three scenarios:</p>
<ol type="1">
<li>Scenario A: A type with a token frequency of 100 appears in each of the 10 documents in a corpus. Each document is 100 words long, and the type appears 10 times in each document.</li>
<li>Scenario B: The same type with a token frequency of 100 appears in each of the 10 documents, each 100 words long. However, in this scenario, the type appears once in 9 documents and 91 times in 1 document.</li>
<li>Scenario C: Nine of the documents constitute 99% of the corpus. The type appears once in each of these 9 documents and 91 times in the 10th document.</li>
</ol>
<p>In these scenarios, Scenario A is the most dispersed, Scenario B is less dispersed, and Scenario C is the least dispersed. Despite these differences, the type’s document frequency (<span class="math inline">\(df\)</span>) and inverse document frequency (<span class="math inline">\(idf\)</span>) scores remain the same across all three scenarios. However, the dispersion (<span class="math inline">\(dp\)</span>) score will accurately reflect the increasing concentration of the type’s dispersion from Scenario A to Scenario B to Scenario C.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-medal" aria-label="medal"></i> Dive deeper</strong></p>
<p>You may wonder why we would want to use <span class="math inline">\(df\)</span> or <span class="math inline">\(idf\)</span> at all. The answer is some combination of the fact that they are computationally less expensive to calculate, they are widely used (especially <span class="math inline">\(idf\)</span>), and/or in many practical situations they often highly correlated with <span class="math inline">\(dp\)</span>.</p>
</div>
</div>
</div>
<p>So for our task we will use <span class="math inline">\(dp\)</span> as our measure of dispersion. {qtkit} includes the <code>calc_type_metrics()</code> function which calculates, among other metrics, the dispersion metrics <span class="math inline">\(df\)</span>, <span class="math inline">\(idf\)</span>, and/or <span class="math inline">\(dp\)</span>. Let’s select <code>dp</code> and assign the result to <code>masc_lemma_disp</code>, as seen in <a href="#exm-explore-masc-dp" class="quarto-xref">Example&nbsp;<span>8.4</span></a>.</p>
<div id="exm-explore-masc-dp" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.4</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Load package</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="fu">library</span>(qtkit)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Calculate deviance of proportions (DP)</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>masc_lemma_disp <span class="ot">&lt;-</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>  masc_content_tbl <span class="sc">|&gt;</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>  <span class="fu">calc_type_metrics</span>(</span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="at">type =</span> lemma,</span>
<span id="cb6-9"><a href="#cb6-9"></a>    <span class="at">documents =</span> doc_id,</span>
<span id="cb6-10"><a href="#cb6-10"></a>    <span class="at">dispersion =</span> <span class="st">"dp"</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>  <span class="fu">arrange</span>(dp)</span>
<span id="cb6-13"><a href="#cb6-13"></a></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="co"># Preview</span></span>
<span id="cb6-15"><a href="#cb6-15"></a>masc_lemma_disp <span class="sc">|&gt;</span></span>
<span id="cb6-16"><a href="#cb6-16"></a>  <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 3
   type      n    dp
   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
 1 be    19231 0.123
 2 have   5136 0.189
 3 not    2279 0.240
 4 make   1149 0.266
 5 other   882 0.269
 6 more   1005 0.276
 7 take    769 0.286
 8 only    627 0.286
 9 time    931 0.314
10 see     865 0.327</code></pre>
</div>
</div>
<p> </p>
</div>
<p>We would like to identify lemmas that are frequent and well-dispersed. But an important question arises, what is the threshold for frequency and dispersion that we should use to identify the lemmas that we want to include in our ELL materials?</p>
<p>There are statistical approaches to identifying natural breakpoints but a visual inspection is often good enough for practical purposes. Let’s create a density plot to see if there is a natural break in the distribution of our dispersion measure, as seen in <a href="#fig-explore-masc-dp-density" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>.</p>
<div id="exm-explore-masc-dp-density" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.5</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="co"># Density plot of dp</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>masc_lemma_disp <span class="sc">|&gt;</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> dp)) <span class="sc">+</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, .<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Deviation of Proportions"</span>, <span class="at">y =</span> <span class="st">"Density"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-dp-density" class="quarto-float quarto-figure quarto-figure-center anchored" alt="The plot shows a bend in the distribution between 0.85 and 0.97.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-dp-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-dp-density-1.png" class="img-fluid figure-img" alt="The plot shows a bend in the distribution between 0.85 and 0.97." width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-dp-density-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.2: Density plot of lemma dispersion
</figcaption></figure>
</div>
</div>
</div>
<p> </p>
<p>What we are looking for is a distinctive bend in the distribution of dispersion measures. In <a href="#fig-explore-masc-dp-density" class="quarto-xref">Figure&nbsp;<span>8.2</span></a>, we can see one roughly between <span class="math inline">\(0.87\)</span> and <span class="math inline">\(0.97\)</span>. The inflection point appears to be near <span class="math inline">\(0.95\)</span>. This bend is called an elbow, and using this bend to make informed decisions about thresholds is called the <strong>elbow method</strong>.</p>
<p>In <a href="#exm-explore-masc-dp-filter" class="quarto-xref">Example&nbsp;<span>8.6</span></a>, I filter out lemmas that have a dispersion measure less than <span class="math inline">\(0.95\)</span>.</p>
<div style="page-break-after: always;"></div>
<div id="exm-explore-masc-dp-filter" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.6</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Filter for lemmas with dp &lt;= 0.95</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>masc_lemma_disp_thr <span class="ot">&lt;-</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>  masc_lemma_disp <span class="sc">|&gt;</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>  <span class="fu">filter</span>(dp <span class="sc">&lt;=</span> <span class="fl">0.95</span>) <span class="sc">|&gt;</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(n))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</div>
<p>Then in <a href="#tbl-explore-masc-dp-filter-preview-top" class="quarto-xref">Tables&nbsp;<span>8.5</span></a> and <a href="#tbl-explore-masc-dp-filter-preview-bottom" class="quarto-xref"><span>8.6</span></a>, I preview the top and bottom 25 lemmas in the dataset.</p>
<div class="cell" data-tbl-colwidths="[20,20,20,20,20]">
<div id="tbl-explore-masc-dp-filter-preview-top" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[20,20,20,20,20]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-dp-filter-preview-top-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.5: Top 25 lemmas after our dispersion threshold
</figcaption><div aria-describedby="tbl-explore-masc-dp-filter-preview-top-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">be</td>
<td style="text-align: left;">say</td>
<td style="text-align: left;">think</td>
<td style="text-align: left;">other</td>
<td style="text-align: left;">work</td>
</tr>
<tr class="even">
<td style="text-align: left;">have</td>
<td style="text-align: left;">go</td>
<td style="text-align: left;">more</td>
<td style="text-align: left;">see</td>
<td style="text-align: left;">year</td>
</tr>
<tr class="odd">
<td style="text-align: left;">do</td>
<td style="text-align: left;">know</td>
<td style="text-align: left;">just</td>
<td style="text-align: left;">people</td>
<td style="text-align: left;">come</td>
</tr>
<tr class="even">
<td style="text-align: left;">not</td>
<td style="text-align: left;">get</td>
<td style="text-align: left;">time</td>
<td style="text-align: left;">take</td>
<td style="text-align: left;">use</td>
</tr>
<tr class="odd">
<td style="text-align: left;">n’t</td>
<td style="text-align: left;">make</td>
<td style="text-align: left;">so</td>
<td style="text-align: left;">now</td>
<td style="text-align: left;">well</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<div class="cell" data-tbl-colwidths="[20,20,20,20,20]">
<div id="tbl-explore-masc-dp-filter-preview-bottom" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[20,20,20,20,20]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-dp-filter-preview-bottom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.6: Bottom 25 lemmas after our dispersion threshold
</figcaption><div aria-describedby="tbl-explore-masc-dp-filter-preview-bottom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">ramification</td>
<td style="text-align: left;">contradiction</td>
<td style="text-align: left;">deckhand</td>
<td style="text-align: left;">injustice</td>
<td style="text-align: left;">imaginative</td>
</tr>
<tr class="even">
<td style="text-align: left;">trickled</td>
<td style="text-align: left;">flatly</td>
<td style="text-align: left;">graveyard</td>
<td style="text-align: left;">intimately</td>
<td style="text-align: left;">pastime</td>
</tr>
<tr class="odd">
<td style="text-align: left;">conceivably</td>
<td style="text-align: left;">mindset</td>
<td style="text-align: left;">rooftop</td>
<td style="text-align: left;">preoccupation</td>
<td style="text-align: left;">rickety</td>
</tr>
<tr class="even">
<td style="text-align: left;">charade</td>
<td style="text-align: left;">mischaracterized</td>
<td style="text-align: left;">wharf</td>
<td style="text-align: left;">specifics</td>
<td style="text-align: left;">scroll</td>
</tr>
<tr class="odd">
<td style="text-align: left;">traipse</td>
<td style="text-align: left;">shameful</td>
<td style="text-align: left;">commend</td>
<td style="text-align: left;">checkered</td>
<td style="text-align: left;">uphill</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p>We now have a solid candidate list of common vocabulary that is spread well across the corpus.</p>
<div style="page-break-after: always;"></div>
</section><section id="sec-explore-frequency-relative" class="level5"><h5 class="anchored" data-anchor-id="sec-explore-frequency-relative">Relative frequency</h5>
<p>Gauging frequency and dispersion across the entire corpus sets the foundation for any frequency analysis, but it is often the case that we want to compare the frequency and/or dispersion of linguistic units across corpora or sub-corpora.</p>
<p>In the case of the MASC dataset, for example, we may want to compare metrics across the two modalities or the various genres. Simply comparing raw frequency counts across these sub-corpora is not a good approach, and can be misleading, as the sub-corpora will likely vary in size. For example, if one sub-corpus is twice as large as another sub-corpus, then, all else being equal, the frequency counts will be twice as large in the larger sub-corpus. This is why we use relative frequency measures, which are normalized by the size of the sub-corpus.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>A variable in the MASC dataset that has yet to be used is the <code>pos</code> variable. How could we use this POS variable to refine our frequency and dispersion analysis of lemma types?</p>
<p>Hint: consider lemma forms that may be tagged with different parts of speech.</p>
</div>
</div>
</div>
<p>To normalize the frequency of linguistic units across sub-corpora, we can use the <strong>relative frequency</strong> (<span class="math inline">\(rf\)</span>) measure. This is the frequency of a linguistic unit divided by the total number of linguistic units in the sub-corpus. This bakes in the size of the sub-corpus into the measure. The notion of relative frequency is key to all research working with text, as it is the basis for the statistical approach to text analysis where comparisons are made.</p>
<p>There are some field-specific terms that are used to refer to relative frequency measures. For example, in NLP literature, the relative frequency measure is often referred to as the <strong>term frequency</strong> (<span class="math inline">\(tf\)</span>). In corpus linguistics, the relative frequency measure is often modified slightly to include a constant (<em>e.g.</em> <span class="math inline">\(rf * 100\)</span>) which is known as the <strong>observed relative frequency</strong> (<span class="math inline">\(orf\)</span>). Although the observed relative frequency per number of tokens is not strictly necessary, it is often used to make the values more interpretable as we can now talk about an observed relative frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100 linguistic units.</p>
<p>Let’s consider how we might compare the frequency and dispersion of lemmas across the two modalities in the MASC dataset, spoken and written. To make this a bit more interesting and more relevant, let’s add the <code>pos</code> variable to our analysis. The intent, then, will be to identify lemmas tagged with particular parts of speech that are particularly indicative of each modality.</p>
<p>We can do this by collapsing the <code>lemma</code> and <code>pos</code> variables into a single variable, <code>lemma_pos</code>, with the <code>str_c()</code> function, as seen in <a href="#exm-explore-masc-type" class="quarto-xref">Example&nbsp;<span>8.7</span></a>.</p>
<div id="exm-explore-masc-type" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.7</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Collapse lemma and pos into type</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>masc_content_tbl <span class="ot">&lt;-</span></span>
<span id="cb10-3"><a href="#cb10-3"></a>  masc_content_tbl <span class="sc">|&gt;</span></span>
<span id="cb10-4"><a href="#cb10-4"></a>  <span class="fu">mutate</span>(<span class="at">lemma_pos =</span> <span class="fu">str_c</span>(lemma, pos, <span class="at">sep =</span> <span class="st">"_"</span>))</span>
<span id="cb10-5"><a href="#cb10-5"></a></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co"># Preview</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>masc_content_tbl <span class="sc">|&gt;</span></span>
<span id="cb10-8"><a href="#cb10-8"></a>  <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  doc_id modality genre   term_num term         lemma        pos   lemma_pos    
  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;        
1 1      Written  Letters        3 contribution contribution NN    contribution…
2 1      Written  Letters        7 mean         mean         VB    mean_VB      
3 1      Written  Letters        8 more         more         JJR   more_JJR     
4 1      Written  Letters       12 know         know         VB    know_VB      
5 1      Written  Letters       15 help         help         VB    help_VB      </code></pre>
</div>
</div>
<p> </p>
</div>
<p>Now this will increase the number of lemma types in the dataset as we are now considering lemmas where the same lemma form is tagged with different parts of speech.</p>
<p>Getting back to calculating the frequency and dispersion of lemmas in each modality, we can use the <code>calc_type_metrics()</code> function with <code>lemma_pos</code> as our type argument. We will, however, need to apply this function to each sub-corpus independently and then concatenate the two data frames. This function returns a (raw) frequency (<span class="math inline">\(n\)</span>) measure by default, but we can specify the <code>frequency</code> argument to <code>rf</code> to calculate the relative frequency of the linguistic units as in <a href="#exm-explore-masc-metrics-modality" class="quarto-xref">Example&nbsp;<span>8.8</span></a>.</p>
<div id="exm-explore-masc-metrics-modality" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.8</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Calculate relative frequency</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="co"># Spoken</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>masc_spoken_metrics <span class="ot">&lt;-</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>  masc_content_tbl <span class="sc">|&gt;</span></span>
<span id="cb12-5"><a href="#cb12-5"></a>  <span class="fu">filter</span>(modality <span class="sc">==</span> <span class="st">"Spoken"</span>) <span class="sc">|&gt;</span></span>
<span id="cb12-6"><a href="#cb12-6"></a>  <span class="fu">calc_type_metrics</span>(</span>
<span id="cb12-7"><a href="#cb12-7"></a>    <span class="at">type =</span> lemma_pos,</span>
<span id="cb12-8"><a href="#cb12-8"></a>    <span class="at">documents =</span> doc_id,</span>
<span id="cb12-9"><a href="#cb12-9"></a>    <span class="at">frequency =</span> <span class="st">"rf"</span>,</span>
<span id="cb12-10"><a href="#cb12-10"></a>    <span class="at">dispersion =</span> <span class="st">"dp"</span></span>
<span id="cb12-11"><a href="#cb12-11"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>  <span class="fu">mutate</span>(<span class="at">modality =</span> <span class="st">"Spoken"</span>) <span class="sc">|&gt;</span></span>
<span id="cb12-13"><a href="#cb12-13"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(n))</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="co"># Written</span></span>
<span id="cb12-16"><a href="#cb12-16"></a>masc_written_metrics <span class="ot">&lt;-</span> </span>
<span id="cb12-17"><a href="#cb12-17"></a>  masc_content_tbl <span class="sc">|&gt;</span></span>
<span id="cb12-18"><a href="#cb12-18"></a>  <span class="fu">filter</span>(modality <span class="sc">==</span> <span class="st">"Written"</span>) <span class="sc">|&gt;</span></span>
<span id="cb12-19"><a href="#cb12-19"></a>  <span class="fu">calc_type_metrics</span>(</span>
<span id="cb12-20"><a href="#cb12-20"></a>    <span class="at">type =</span> lemma_pos,</span>
<span id="cb12-21"><a href="#cb12-21"></a>    <span class="at">documents =</span> doc_id,</span>
<span id="cb12-22"><a href="#cb12-22"></a>    <span class="at">frequency =</span> <span class="st">"rf"</span>,</span>
<span id="cb12-23"><a href="#cb12-23"></a>    <span class="at">dispersion =</span> <span class="st">"dp"</span></span>
<span id="cb12-24"><a href="#cb12-24"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb12-25"><a href="#cb12-25"></a>  <span class="fu">mutate</span>(<span class="at">modality =</span> <span class="st">"Written"</span>) <span class="sc">|&gt;</span></span>
<span id="cb12-26"><a href="#cb12-26"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(n))</span>
<span id="cb12-27"><a href="#cb12-27"></a></span>
<span id="cb12-28"><a href="#cb12-28"></a><span class="co"># Concatenate metrics</span></span>
<span id="cb12-29"><a href="#cb12-29"></a>masc_metrics <span class="ot">&lt;-</span></span>
<span id="cb12-30"><a href="#cb12-30"></a>  <span class="fu">bind_rows</span>(masc_spoken_metrics, masc_written_metrics)</span>
<span id="cb12-31"><a href="#cb12-31"></a></span>
<span id="cb12-32"><a href="#cb12-32"></a><span class="co"># Preview</span></span>
<span id="cb12-33"><a href="#cb12-33"></a>masc_metrics <span class="sc">|&gt;</span></span>
<span id="cb12-34"><a href="#cb12-34"></a>  <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 5
  type         n     rf     dp modality
  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   
1 be_VBZ    2612 0.0489 0.0843 Spoken  
2 be_VBP    1282 0.0240 0.111  Spoken  
3 be_VBD    1020 0.0191 0.300  Spoken  
4 n't_RB     829 0.0155 0.139  Spoken  
5 have_VBP   766 0.0143 0.152  Spoken  </code></pre>
</div>
</div>
<p> </p>
</div>
<p>With the <code>rf</code> measure, we are now in a position to compare ‘apples to apples’, as you might say. We can now compare the relative frequency of lemmas across the two modalities. Let’s preview the top 5 lemmas in each modality, as seen in <a href="#exm-explore-masc-relative-frequency-top" class="quarto-xref">Example&nbsp;<span>8.9</span></a>.</p>
<div id="exm-explore-masc-relative-frequency-top" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.9</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># Preview top 10 lemmas in each modality</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>masc_metrics <span class="sc">|&gt;</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>  <span class="fu">group_by</span>(modality) <span class="sc">|&gt;</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>  <span class="fu">slice_max</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">order_by =</span> rf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 20 × 5
# Groups:   modality [2]
   type         n      rf     dp modality
   &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   
 1 be_VBZ    2612 0.0489  0.0843 Spoken  
 2 be_VBP    1282 0.0240  0.111  Spoken  
 3 be_VBD    1020 0.0191  0.300  Spoken  
 4 n't_RB     829 0.0155  0.139  Spoken  
 5 have_VBP   766 0.0143  0.152  Spoken  
 6 do_VBP     728 0.0136  0.180  Spoken  
 7 be_VB      655 0.0123  0.147  Spoken  
 8 not_RB     638 0.0119  0.137  Spoken  
 9 just_RB    404 0.00757 0.267  Spoken  
10 so_RB      387 0.00725 0.357  Spoken  
11 be_VBZ    4745 0.0249  0.230  Written 
12 be_VBD    3317 0.0174  0.366  Written 
13 be_VBP    2617 0.0137  0.237  Written 
14 be_VB     1863 0.00976 0.218  Written 
15 not_RB    1640 0.00859 0.259  Written 
16 have_VBP  1227 0.00643 0.291  Written 
17 n't_RB     905 0.00474 0.540  Written 
18 have_VBD   859 0.00450 0.446  Written 
19 have_VBZ   777 0.00407 0.335  Written 
20 say_VBD    710 0.00372 0.609  Written </code></pre>
</div>
</div>
<p> </p>
</div>
<p>We can appreciate, now, that there are similarities and a few differences between the most frequent lemmas for each modality. First, there are similar lemmas in written and spoken modalities, such as ‘be’, ‘have’, and ‘not’. Second, the top 10 include verbs and adverbs. Now we are looking at the most frequent types, so it is not surprising that we see more in common than not. However, looking close we can see that contracted forms are more frequent in the spoken modality, such as ‘isn’t’, ‘don’t’, and ‘can’t’ and that ordering of the verb tenses differs to some degree. Whether these are important distinctions for our task is something we will need to consider.</p>
<p>We can further cull our results by filtering out lemmas that are not well-dispersed across the sub-corpora. Although it may be tempting to use the threshold we used earlier, we should consider that the sizes of the sub-corpora are different and the distribution of the dispersion measure may be different. With this in mind, we need to visualize the distribution of the dispersion measure for each modality and apply the elbow method to identify a threshold for each modality.</p>
<p>After assessing the density plots for the dispersion of each modality via the elbow method, we update our thresholds. We maintain the <span class="math inline">\(0.95\)</span> threshold for the written sub-corpus and use a <span class="math inline">\(0.79\)</span> threshold for the spoken sub-corpus. I apply these filters as seen in <a href="#exm-explore-masc-subcorpora-filtered" class="quarto-xref">Example&nbsp;<span>8.10</span></a>.</p>
<div style="page-break-after: always;"></div>
<div id="exm-explore-masc-subcorpora-filtered" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.10</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="co"># Filter for lemmas with</span></span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="co"># dp &lt;= 0.95 for written and</span></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="co"># dp &lt;= .79 for spoken</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>masc_metrics_thr <span class="ot">&lt;-</span></span>
<span id="cb16-5"><a href="#cb16-5"></a>  masc_metrics <span class="sc">|&gt;</span></span>
<span id="cb16-6"><a href="#cb16-6"></a>  <span class="fu">filter</span>(</span>
<span id="cb16-7"><a href="#cb16-7"></a>    (modality <span class="sc">==</span> <span class="st">"Written"</span> <span class="sc">&amp;</span> dp <span class="sc">&lt;=</span> <span class="fl">0.95</span>) <span class="sc">|</span></span>
<span id="cb16-8"><a href="#cb16-8"></a>    (modality <span class="sc">==</span> <span class="st">"Spoken"</span> <span class="sc">&amp;</span> dp <span class="sc">&lt;=</span> .<span class="dv">79</span>)</span>
<span id="cb16-9"><a href="#cb16-9"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(rf))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</div>
<p>Filtering the less-dispersed types reduces the dataset from 33,428 to 7,459 observations. This will provide us with a more succinct list of common and well-dispersed lemmas that are used in each modality.</p>
<p>As much as the frequency and dispersion measures can provide us with a starting point, it does not provide an understanding of what types are more indicative of a particular sub-corpus, modality sub-corpora in our case. We can do this by calculating the log odds ratio of each lemma in each modality.</p>
<p>The <strong>log odds ratio</strong> is a measure that quantifies the difference between the frequencies of a type in two corpora or sub-corpora. In spirit and in name, it compares the odds of a type occurring in one corpus versus the other. The values range from negative to positive infinity, with negative values indicating that the type is more frequent in the first corpus and positive values indicating that the lemma is more frequent in the second corpus. The magnitude of the value indicates the strength of the association.</p>
<p>{tidylo} provides a convenient function <code>bind_log_odds()</code> to calculate the log odds ratio, and a weighed variant, for each type in each sub-corpus. The weighted log odds ratio measure provides a more robust and interpretable measure for comparing term frequencies across corpora, especially when term frequencies are low or when corpora are of different sizes. The weighting (or standardization) also makes it easier to identify terms that are particularly distinctive or characteristic of one corpus over another.</p>
<p>Let’s calculate the weighted log odds ratio for each lemma in each modality and preview the top 10 lemmas in each modality, as seen in <a href="#exm-explore-masc-log-odds-weighted" class="quarto-xref">Example&nbsp;<span>8.11</span></a>.</p>
<div style="page-break-after: always;"></div>
<div id="exm-explore-masc-log-odds-weighted" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.11</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Load package</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="fu">library</span>(tidylo)</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="co"># Calculate log odds ratio</span></span>
<span id="cb17-5"><a href="#cb17-5"></a>masc_metrics_thr <span class="ot">&lt;-</span></span>
<span id="cb17-6"><a href="#cb17-6"></a>  masc_metrics_thr <span class="sc">|&gt;</span></span>
<span id="cb17-7"><a href="#cb17-7"></a>  <span class="fu">bind_log_odds</span>(</span>
<span id="cb17-8"><a href="#cb17-8"></a>    <span class="at">set =</span> modality,</span>
<span id="cb17-9"><a href="#cb17-9"></a>    <span class="at">feature =</span> type,</span>
<span id="cb17-10"><a href="#cb17-10"></a>    <span class="at">n =</span> n</span>
<span id="cb17-11"><a href="#cb17-11"></a>  )</span>
<span id="cb17-12"><a href="#cb17-12"></a></span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="co"># Preview top 10 lemmas in each modality</span></span>
<span id="cb17-14"><a href="#cb17-14"></a>masc_metrics_thr <span class="sc">|&gt;</span></span>
<span id="cb17-15"><a href="#cb17-15"></a>  <span class="fu">group_by</span>(modality) <span class="sc">|&gt;</span></span>
<span id="cb17-16"><a href="#cb17-16"></a>  <span class="fu">slice_max</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">order_by =</span> log_odds_weighted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 20 × 6
# Groups:   modality [2]
   type             n       rf     dp modality log_odds_weighted
   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;
 1 be_VBZ        2612 0.0489   0.0843 Spoken               20.7 
 2 n't_RB         829 0.0155   0.139  Spoken               13.6 
 3 be_VBP        1282 0.0240   0.111  Spoken               13.4 
 4 do_VBP         728 0.0136   0.180  Spoken               13.2 
 5 have_VBP       766 0.0143   0.152  Spoken               11.4 
 6 think_VBP      350 0.00655  0.259  Spoken               10.2 
 7 be_VBD        1020 0.0191   0.300  Spoken                9.18
 8 well_RB        334 0.00626  0.283  Spoken                8.90
 9 know_VBP       282 0.00528  0.260  Spoken                8.78
10 just_RB        404 0.00757  0.267  Spoken                8.53
11 t_NN           475 0.00249  0.778  Written               9.62
12 figure_NN      140 0.000733 0.868  Written               5.21
13 financial_JJ   138 0.000723 0.880  Written               5.18
14 city_NN        137 0.000718 0.766  Written               5.16
15 email_NN       133 0.000697 0.866  Written               5.08
16 eye_NNS        129 0.000676 0.731  Written               5.00
17 style_NN       108 0.000566 0.829  Written               4.58
18 mail_NN        106 0.000555 0.876  Written               4.54
19 channel_NN     103 0.000540 0.919  Written               4.47
20 text_NN        103 0.000540 0.845  Written               4.47</code></pre>
</div>
</div>
<p> </p>
</div>
<p>Let’s imagine we would like to extract the most indicative verbs for each modality using the weighted log odds as our measure. We can do this with a little regex magic. Let’s use the <code>str_subset()</code> function to filter for lemmas that contain <code>_V</code> and then use <code>slice_max()</code> to extract the top 10 most indicative verb lemmas, as seen in <a href="#exm-explore-masc-log-odds-weighted-verbs" class="quarto-xref">Example&nbsp;<span>8.12</span></a>.</p>
<div id="exm-explore-masc-log-odds-weighted-verbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.12</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="co"># Preview (ordered by log_odds_weighted)</span></span>
<span id="cb19-2"><a href="#cb19-2"></a>masc_metrics_thr <span class="sc">|&gt;</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>  <span class="fu">group_by</span>(modality) <span class="sc">|&gt;</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(type, <span class="st">"_V"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>  <span class="fu">slice_max</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">order_by =</span> log_odds_weighted) <span class="sc">|&gt;</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>  <span class="fu">select</span>(<span class="sc">-</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 20 × 5
# Groups:   modality [2]
   type                rf     dp modality log_odds_weighted
   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;
 1 be_VBZ        0.0489   0.0843 Spoken               20.7 
 2 be_VBP        0.0240   0.111  Spoken               13.4 
 3 do_VBP        0.0136   0.180  Spoken               13.2 
 4 have_VBP      0.0143   0.152  Spoken               11.4 
 5 think_VBP     0.00655  0.259  Spoken               10.2 
 6 be_VBD        0.0191   0.300  Spoken                9.18
 7 know_VBP      0.00528  0.260  Spoken                8.78
 8 go_VBG        0.00534  0.207  Spoken                8.29
 9 do_VBD        0.00603  0.321  Spoken                8.03
10 be_VB         0.0123   0.147  Spoken                7.92
11 post_VBN      0.000372 0.928  Written               3.71
12 don_VB        0.000361 0.839  Written               3.66
13 doe_VBZ       0.000351 0.870  Written               3.61
14 walk_VBD      0.000320 0.790  Written               3.44
15 associate_VBN 0.000304 0.777  Written               3.35
16 reply_VBD     0.000293 0.837  Written               3.30
17 develop_VBG   0.000288 0.812  Written               3.27
18 require_VBN   0.000272 0.793  Written               3.18
19 fall_VBD      0.000267 0.757  Written               3.15
20 meet_VB       0.000241 0.729  Written               2.99</code></pre>
</div>
</div>
<p> </p>
</div>
<p>Note that the log odds are larger for the spoken modality than the written modality. This indicates that theses types are more strongly indicative of the spoken modality than the types in the written modality are indicative of the written modality. This is not surprising, as the written modality is typically more diverse in terms of lexical usage than the spoken modality, where the terms tend to be repeated more often, including verbs.</p>
</section></section><section id="sec-explore-co-occurrence" class="level4"><h4 class="anchored" data-anchor-id="sec-explore-co-occurrence">Co-occurrence analysis</h4>
<p>Moving forward on our task, we have a general idea of the vocabulary that we want to include in our ELL materials and can identify lemma types that are particularly indicative of each modality. Another useful approach to complement our analysis is to identify words that co-occur with our target lemmas (verbs). In English, it is common for verbs to appear with a preposition or adverb, such as ‘give up’, ‘look after’. These ‘phrasal verbs’ form a semantic unit that is distinct from the verb alone.</p>
<p>In a case such as this, we are aiming to do a co-occurrence analysis. Co-occurrence analysis is a set of methods that are used to identify words that appear in close proximity to a target type.</p>
<!-- Concordances -->
<p>An exploratory, primarily qualitative, approach is to display the co-occurrence of words in a Keyword in Context (KWIC) search. <strong>KWIC</strong> produces a table that displays the target word in the center of the table and the words that appear before and after the target word within some defined window context. This is a useful approach for spot identifying co-occurring patterns which include the target word or phrase. However, it can be a time-consuming process to manually inspect these results and is likely not a feasible approach for large datasets.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-hand-point-up" aria-label="hand-point-up"></i> Tip</strong></p>
<p>KWIC tables are a common tool in corpus linguistics and can be used either before or after a quantitative analysis. If you are interested, {quanteda} includes a function <code>kwic()</code> that can be used to create a KWIC table.</p>
</div>
</div>
</div>
<!-- N-grams -->
<p>A straightforward quantitative way to explore co-occurrence is to set the unit of observation to an ngram of word terms. Then, the frequency and dispersion metrics can be calculated for each ngram. Yet, there is an issue with this approach for our purposes. The frequency and dispersion of ngrams does not necessarily relate to whether the two words form a semantic unit. For example, in any given corpus there will be highly frequent pairings of function words, such as ‘of the’, ‘in the’, ‘to the’, <em>etc</em>. These combinations our bound to occur frequently in large part because the high frequency of each individual word. However, these combinations do not have the same semantic cohesion as other, likely lower-frequency, ngrams such as ‘look after’, ‘give up’, <em>etc</em>.</p>
<!-- Collocation -->
<p>To better address our question, we can use a statistical measure to estimate collocational strength between two words. A <strong>collocation</strong> is a sequence of words that co-occur more often than would be expected by chance. A common measure of collocation is the <strong>pointwise mutual information</strong> (PMI) measure. PMI scores reflect the likelihood of two words occurring together given their individual frequencies and compares this to the actual co-occurrence frequency. A high PMI indicates a strong semantic association between the words.</p>
<p>One consideration that we need to take into account for our goal to identify verb particle constructions, is how we ultimately want to group our <code>lemma_pos</code> values. This is particularly important given the fact that our <code>pos</code> tags for verbs include information about the verb’s tense and person attributes. This means that a verb in a verb particle bigram, such as ‘look after’, will be represented by multiple <code>lemma_pos</code> values, such as <code>look_VB</code>, <code>look_VBP</code>, <code>look_VBD</code>, and <code>look_VBG</code>. We want to group the verb particle bigrams by a single verb value, so we need to reclassify the <code>pos</code> values for verbs. We can do this with the <code>case_when()</code> function from {dplyr}.</p>
<p>In <a href="#exm-explore-masc-lemma-pos" class="quarto-xref">Example&nbsp;<span>8.13</span></a>, I recode the <code>pos</code> values for verbs to <code>V</code> and then join the <code>lemma</code> and <code>pos</code> columns into a single string.</p>
<div id="exm-explore-masc-lemma-pos" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.13</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>masc_lemma_pos_tbl <span class="ot">&lt;-</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>  masc_tbl <span class="sc">|&gt;</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="fu">mutate</span>(<span class="at">pos =</span> <span class="fu">case_when</span>(</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="fu">str_detect</span>(pos, <span class="st">"^V"</span>) <span class="sc">~</span> <span class="st">"V"</span>,</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="cn">TRUE</span> <span class="sc">~</span> pos</span>
<span id="cb21-6"><a href="#cb21-6"></a>  )) <span class="sc">|&gt;</span></span>
<span id="cb21-7"><a href="#cb21-7"></a>  <span class="fu">group_by</span>(doc_id) <span class="sc">|&gt;</span></span>
<span id="cb21-8"><a href="#cb21-8"></a>  <span class="fu">mutate</span>(<span class="at">lemma_pos =</span> <span class="fu">str_c</span>(lemma, pos, <span class="at">sep =</span> <span class="st">"_"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb21-9"><a href="#cb21-9"></a>  <span class="fu">ungroup</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p> </p>
</div>
<p>Let’s calculate the PMI for all the bigrams in the MASC dataset. We can use the <code>calc_assoc_metrics()</code> function from {qtkit}. We need to specify the <code>association</code> argument to <code>pmi</code> and the <code>type</code> argument to <code>bigrams</code>, as seen in <a href="#exm-explore-masc-bigrams-pmi" class="quarto-xref">Example&nbsp;<span>8.14</span></a>.</p>
<div id="exm-explore-masc-bigrams-pmi" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.14</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>masc_lemma_pos_assoc <span class="ot">&lt;-</span></span>
<span id="cb22-2"><a href="#cb22-2"></a>  masc_lemma_pos_tbl <span class="sc">|&gt;</span></span>
<span id="cb22-3"><a href="#cb22-3"></a>  <span class="fu">calc_assoc_metrics</span>(</span>
<span id="cb22-4"><a href="#cb22-4"></a>    <span class="at">doc_index =</span> doc_id,</span>
<span id="cb22-5"><a href="#cb22-5"></a>    <span class="at">token_index =</span> term_num,</span>
<span id="cb22-6"><a href="#cb22-6"></a>    <span class="at">type =</span> lemma_pos,</span>
<span id="cb22-7"><a href="#cb22-7"></a>    <span class="at">association =</span> <span class="st">"pmi"</span></span>
<span id="cb22-8"><a href="#cb22-8"></a>  )</span>
<span id="cb22-9"><a href="#cb22-9"></a></span>
<span id="cb22-10"><a href="#cb22-10"></a><span class="co"># Preview</span></span>
<span id="cb22-11"><a href="#cb22-11"></a>masc_lemma_pos_assoc <span class="sc">|&gt;</span></span>
<span id="cb22-12"><a href="#cb22-12"></a>  <span class="fu">arrange</span>(<span class="fu">desc</span>(pmi)) <span class="sc">|&gt;</span></span>
<span id="cb22-13"><a href="#cb22-13"></a>  <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 4
   x               y                   n   pmi
   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;
 1 #Christian_NN   bigot_NN            1  12.4
 2 #FAIL_NN        phenomenally_RB     1  12.4
 3 #NASCAR_NN      #indycar_NN         1  12.4
 4 #PALM_NN        merchan_NN          1  12.4
 5 #Twitter_NN     #growth_NN          1  12.4
 6 #college_NN     #jobs_NN            1  12.4
 7 #education_NN   #teaching_NN        1  12.4
 8 #faculty_NN     #cites_NN           1  12.4
 9 #fb_NN          siebel_NNP          1  12.4
10 #glitchmyass_NN reps_NNP            1  12.4</code></pre>
</div>
</div>
<p> </p>
</div>
<p>One caveat to using the PMI measure is that it is sensitive to the frequency of the words. If the words in a bigram pair are infrequent, and especially if they only occur once, then the PMI measure will be unduly inflated. To mitigate this issue, we can apply a frequency threshold to the bigrams before calculating the PMI measure. Let’s filter out bigrams that occur less than 10 times and have a positive PMI, and while we are at it, let’s also filter <code>x</code> and <code>y</code> for the appropriate forms we are targeting, either <code>_V</code> and <code>_IN</code>, as seen <a href="#exm-explore-masc-bigrams-pmi-filtered" class="quarto-xref">Example&nbsp;<span>8.15</span></a>.</p>
<div id="exm-explore-masc-bigrams-pmi-filtered" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.15</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a><span class="co"># Filter for target bigrams</span></span>
<span id="cb24-2"><a href="#cb24-2"></a>masc_verb_part_assoc <span class="ot">&lt;-</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>  masc_lemma_pos_assoc <span class="sc">|&gt;</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>  <span class="fu">filter</span>(n <span class="sc">&gt;=</span> <span class="dv">10</span> <span class="sc">&amp;</span> pmi <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">|&gt;</span></span>
<span id="cb24-5"><a href="#cb24-5"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(x, <span class="st">"_V"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb24-6"><a href="#cb24-6"></a>  <span class="fu">filter</span>(<span class="fu">str_detect</span>(y, <span class="st">"_IN"</span>))</span>
<span id="cb24-7"><a href="#cb24-7"></a></span>
<span id="cb24-8"><a href="#cb24-8"></a><span class="co"># Preview</span></span>
<span id="cb24-9"><a href="#cb24-9"></a>masc_verb_part_assoc <span class="sc">|&gt;</span></span>
<span id="cb24-10"><a href="#cb24-10"></a>  <span class="fu">slice_max</span>(<span class="at">order_by =</span> pmi, <span class="at">n =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 10 × 4
   x           y            n   pmi
   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;
 1 figure_V    out_IN      17  4.93
 2 worry_V     about_IN    27  4.78
 3 walk_V      up_IN       10  4.62
 4 talk_V      about_IN   114  4.57
 5 sound_V     like_IN     15  4.42
 6 post_V      by_IN       57  4.29
 7 derive_V    from_IN     17  4.27
 8 stem_V      from_IN     10  4.13
 9 deal_V      with_IN     53  4.12
10 associate_V with_IN     48  4.05</code></pre>
</div>
</div>
<p> </p>
</div>
<p>We have a working method for identify verb particle constructions. We can clean up the results a bit by removing the POS tags from the <code>x</code> and <code>y</code> variables, up our minimum PMI value, and create a network plot to visualize the results. A <strong>network plot</strong> is a type of graph that shows relationships between entities. In this case, the entities are verbs and particles, and the relationships are the PMI values between them. The connections between are represented by edges, and the thickness of the edges is proportional to the PMI value.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-verb-part-network" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A network plot showing the association between verbs and prepositions in the MASC dataset. The plot shows a network of verbs and prepositions connected by edges with varying thicknesses.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-verb-part-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-verb-part-network-1.png" class="img-fluid figure-img" alt="A network plot showing the association between verbs and prepositions in the MASC dataset. The plot shows a network of verbs and prepositions connected by edges with varying thicknesses." width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-verb-part-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.3: Network plot of verb particle constructions
</figcaption></figure>
</div>
</div>
</div>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-medal" aria-label="medal"></i> Dive deeper</strong></p>
<p>{ggplot2} cannot create network plots directly, so we use {ggraph} <span class="citation" data-cites="R-ggraph">(<a href="../references.html#ref-R-ggraph" role="doc-biblioref">Pedersen, 2024</a>)</span> and {igraph} <span class="citation" data-cites="R-igraph">(<a href="../references.html#ref-R-igraph" role="doc-biblioref">Csárdi et al., 2024</a>)</span> to create the network plot. For more information on creating network plots, see the {ggraph} documentation.</p>
</div>
</div>
</div>
<p>From <a href="#fig-explore-masc-verb-part-network" class="quarto-xref">Figure&nbsp;<span>8.3</span></a>, and from the underlying data, we can explore verb particle constructions. We could go further and apply our co-occurrence methods to each modality separately, if we wanted to identify verb particle constructions that are distinctive to each modality. We could also apply our co-occurrence methods to other parts of speech, such as adjectives and nouns, to identify collocations of these parts of speech. There is much more to explore with co-occurrence analysis, but this should give you a good idea of the types of questions that can be addressed.</p>
</section></section><section id="sec-explore-unsupervised" class="level3"><h3 class="anchored" data-anchor-id="sec-explore-unsupervised">Unsupervised learning</h3>
<p>Aligned in purpose with descriptive approaches, unsupervised learning approaches to exploratory data analysis are used to identify patterns in the data from an algorithmic perspective. Common methods in text analysis include principal component analysis, clustering, and vector space modeling.</p>
<p>We will continue to use the MASC dataset as we develop materials for our ELL textbook to illustrate unsupervised learning methods. In the process, we will explore the following questions:</p>
<ul>
<li>Can we identify and group documents based on linguistic features or co-occurrence patterns of the data itself?</li>
<li>Do the groups of documents relate to categories in the dataset?</li>
<li>Can we estimate the semantics of words based on their co-occurrence patterns?</li>
</ul>
<p>Through these questions we will build on our knowledge of frequency, dispersion, and co-occurrence analysis and introduce concepts and methods associated with machine learning.</p>
<section id="sec-explore-clustering" class="level4"><h4 class="anchored" data-anchor-id="sec-explore-clustering">Clustering</h4>
<p><strong>Clustering</strong> is an unsupervised learning technique that can be used to group similar items in the text data, helping to organize the data into distinct categories and discover relationships between different elements in the text. The main steps in the procedure includes identifying the relevant linguistic features to use for clustering, representing the features in a way that can be used for clustering, applying a clustering algorithm to the data, and then interpreting the results.</p>
<p>In our ELL textbook task, we may very well want to explore the similarities and/or differences between the documents based on the distribution of linguistic features. This provides us a view to evaluate to what extent the variables in the dataset, say genre for this demonstration, map to the distribution of linguistic features. Based on this evaluation, we may want to consider re-categorizing the documents, collapsing categories, or even adding new categories.</p>
<p>Instead of relying entirely on the variables’ values in the MASC dataset, we can let the data itself say something about how documents may or may not be related. Yet, a pivotal question is what linguistic features we will use, otherwise known as <strong>feature selection</strong>. We could use terms or lemmas, but we may want to consider other features, such as parts of speech or some co-occurrence pattern. We are not locked into using one criterion, and we can perform clustering multiple times with different features, but we should consider the implications of our feature selection for our interpretation of the results.</p>
<p>Imagine that among the various features that we are interested in associating documents, we consider lemma use and POS use. However, we need to operationalize what we mean by ‘use’. In machine learning, this process is known as <strong>feature engineering</strong>. We likely want to use some measure of frequency. Since we are comparing documents, a relative frequency measure will be most useful. Another consideration it means to use lemmas or POS tags as our features. Each represents a different linguistic of the documents. Lemmas represent the lexical diversity of the documents while POS tags approximate the grammatical diversity of the documents <span class="citation" data-cites="Petrenz2011">(<a href="../references.html#ref-Petrenz2011" role="doc-biblioref">Petrenz &amp; Webber, 2011</a>)</span>.</p>
<p>Let’s assume that our interest is to gauge the grammatical diversity of the documents, so we will go with POS tags. With this approach, we aim to distinguish between documents in a way that may allow us to consider whether genre-document categories are meaningful, along grammatical lines.</p>
<p>The next question to address in any analysis is how to represent the features. In machine learning, the most common way to represent relationships is in a matrix. In our case, we want to create a matrix with the documents in the rows and the features in the columns. The values in the matrix will be the operationalization of grammatical diversity in each document. This configuration is known as a <strong>document-term matrix</strong> (DTM).</p>
<p>To recast a data frame into a DTM, we can use the <code>cast_dtm()</code> function from {tidytext}. This function takes a data frame with a document identifier, a feature identifier, and a value for each observation and casts it into a matrix. Operations such as normalization are easily and efficiently performed in R on matrices, so initially we can cast a frequency table of POS tags into a matrix and then normalize the matrix by documents.</p>
<p>Let’s see how this works with the MASC dataset in <a href="#exm-explore-masc-dtms" class="quarto-xref">Example&nbsp;<span>8.16</span></a>.</p>
<div id="exm-explore-masc-dtms" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.16</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Load package</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="fu">library</span>(tidytext)</span>
<span id="cb26-3"><a href="#cb26-3"></a></span>
<span id="cb26-4"><a href="#cb26-4"></a><span class="co"># Create a document-term matrix of POS tags</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>masc_pos_dtm <span class="ot">&lt;-</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>  masc_tbl <span class="sc">|&gt;</span></span>
<span id="cb26-7"><a href="#cb26-7"></a>  <span class="fu">count</span>(doc_id, pos) <span class="sc">|&gt;</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>  <span class="fu">cast_dtm</span>(doc_id, pos, n) <span class="sc">|&gt;</span></span>
<span id="cb26-9"><a href="#cb26-9"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb26-10"><a href="#cb26-10"></a></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="co"># Inspect</span></span>
<span id="cb26-12"><a href="#cb26-12"></a><span class="fu">dim</span>(masc_pos_dtm)</span>
<span id="cb26-13"><a href="#cb26-13"></a></span>
<span id="cb26-14"><a href="#cb26-14"></a><span class="co"># Preview</span></span>
<span id="cb26-15"><a href="#cb26-15"></a>masc_pos_dtm[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 392  32
     Terms
Docs  CC DT EX IN JJ
  1   14 35  1 44 27
  10  11 38  0 39 18
  100  0  2  0  2  3
  101  3 16  0 23  7
  102 20 29  0 34 20</code></pre>
</div>
</div>
<p> </p>
</div>
<p>The matrix <code>masc_pos_dtm</code> has 392 documents and 32 POS tags. The values in the matrix are the frequency of each POS tag in each document. Note to preview a subset of the contents of a matrix, such as in <a href="#exm-explore-masc-dtms" class="quarto-xref">Example&nbsp;<span>8.16</span></a>, we use bracket syntax <code>[]</code> instead of the <code><a href="https://rdrr.io/r/utils/head.html">head()</a></code> function.</p>
<p>We can now normalize the matrix by documents. We can do this by dividing each feature count by the total count in each document. This is a row-wise transformation, so we can use the <code><a href="https://rdrr.io/r/base/colSums.html">rowSums()</a></code> function from base R to calculate the total count in each document. Then each count divided by its row’s total count, as seen in <a href="#exm-explore-masc-dtms-normalized" class="quarto-xref">Example&nbsp;<span>8.17</span></a>.</p>
<div id="exm-explore-masc-dtms-normalized" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.17</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># Normalize pos matrix by documents</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>masc_pos_dtm <span class="ot">&lt;-</span></span>
<span id="cb28-3"><a href="#cb28-3"></a>  masc_pos_dtm <span class="sc">/</span> <span class="fu">rowSums</span>(masc_pos_dtm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>There are two concerns to address before we can proceed with clustering. First, clustering algorithm performance tends to degrade with the number of features. Second, clustering algorithms perform better with more informative features. That is to say, features that are more distinct across the documents provide better information for deriving useful clusters.</p>
<p>We can address both of these concerns by reducing the number of features and increasing the informativeness of the features. To accomplish this is to use dimensionality reduction. <strong>Dimensionality reduction</strong> is a set of methods that are used to reduce the number of features in a dataset while retaining as much information as possible. The most common method for dimensionality reduction is <strong>principal component analysis</strong> (PCA). PCA is a method that transforms a set of correlated variables into a set of uncorrelated variables, known as principal components. The principal components are ordered by the amount of variance that they explain in the data. The first principal component explains the most variance, the second principal component explains the second most variance, and so on.</p>
<p>We can apply PCA to the matrix and assess how well it accounts for the variation in the data and how the variation is distributed across components. The <code><a href="https://rdrr.io/r/stats/prcomp.html">prcomp()</a></code> function from base R can be used to perform PCA.</p>
<p>Let’s apply PCA to the matrix, as seen in <a href="#exm-explore-masc-dtms-pca" class="quarto-xref">Example&nbsp;<span>8.18</span></a>.</p>
<div id="exm-explore-masc-dtms-pca" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.18</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># for reproducibility</span></span>
<span id="cb29-2"><a href="#cb29-2"></a></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="co"># Apply PCA to matrix</span></span>
<span id="cb29-4"><a href="#cb29-4"></a>masc_pos_pca <span class="ot">&lt;-</span></span>
<span id="cb29-5"><a href="#cb29-5"></a>  masc_pos_dtm <span class="sc">|&gt;</span></span>
<span id="cb29-6"><a href="#cb29-6"></a>  <span class="fu">prcomp</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>We can visualize the amount of variance explained by each principal component with a scree plot. A <strong>scree plot</strong> is a bar plot ordered by the amount of variance explained by each principal component. The <code>fviz_eig()</code> function from {factoextra} implements a scree plot on a PCA object. We can set the number of components to visualize with <code>ncp =</code>, as seen in <a href="#exm-explore-masc-dtms-pca-scree" class="quarto-xref">Example&nbsp;<span>8.19</span></a>.</p>
<div id="exm-explore-masc-dtms-pca-scree" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.19</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># Load package</span></span>
<span id="cb30-2"><a href="#cb30-2"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb30-3"><a href="#cb30-3"></a></span>
<span id="cb30-4"><a href="#cb30-4"></a><span class="co"># Scree plot: POS relative frequency</span></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="fu">fviz_eig</span>(masc_pos_pca, <span class="at">ncp =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> </p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-dtms-pca-scree" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A scree plot which is a bar plot ordered by the amount of variance explained by each principal component. The plot shows the first 10 principal components and the amount of variance explained by each component. The first two components explain almost 50% of the variance.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-dtms-pca-scree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-dtms-pca-scree-1.png" class="img-fluid figure-img" alt="A scree plot which is a bar plot ordered by the amount of variance explained by each principal component. The plot shows the first 10 principal components and the amount of variance explained by each component. The first two components explain almost 50% of the variance." width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-dtms-pca-scree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.4: Scree plot of the principal components of the POS relative frequency
</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-medal" aria-label="medal"></i> Dive deeper</strong></p>
<p>As with many modeling techniques we will encounter, it is possible to extract the importance of features that contribute to the model. In the case of PCA, we can extract the feature values from the principal components using the <code>get_pca_var()</code> function from {factoextra}. Feature importance provides more detailed insight into the inner workings of the algorithms we employ in our research and therefore can serve to inform our interpretation of the results.</p>
</div>
</div>
</div>
<p>From the scree plot for the matrix in <a href="#fig-explore-masc-dtms-pca-scree" class="quarto-xref">Figure&nbsp;<span>8.4</span></a>, we can see that the first component shows the most variance explained, around just over 30%, and then drops for subsequent drops as the number of dimensions increase. Visually we will apply the elbow method to identify the number of dimensions to use for clustering. It appears the variance explained decreases after 4 dimensions. This is a good indication that we should use 4 dimensions for our clustering algorithm.</p>
<p>Let’s go ahead and create a matrix of the first four principal components for the POS data, as seen in <a href="#exm-explore-masc-pos-pca-pc" class="quarto-xref">Example&nbsp;<span>8.20</span></a>.</p>
<div id="exm-explore-masc-pos-pca-pc" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.20</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Create a matrix of the first four principal components</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>masc_pos_pca_pc <span class="ot">&lt;-</span></span>
<span id="cb31-3"><a href="#cb31-3"></a>  masc_pos_pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>Now that we have identified the features that we want to use for clustering and we have represented the features in a way that can be used for clustering, we can apply a clustering algorithm to the data.</p>
<p>Some algorithms are better suited for certain types of data and certain types of tasks. For example, <strong>hierarchical clustering</strong> is best when we are not sure how many clusters we want to identify, as it does not require us to specify the number of clusters from the outset. However, it is not ideal when we have a large dataset, as it can be computationally expensive compared to some other algorithms. <strong>k-means clustering</strong>, on the other hand, is a good choice when we want to identify a pre-defined number of clusters, and the aim is to gauge how well the data fit the clusters. These two clustering techniques, therefore, complement each other, with hierarchical clustering being favored for initial exploration and k-means clustering being better suited for targeted evaluation.</p>
<div style="page-break-after: always;"></div>
<p>Since we are exploring the usefulness of the 18 genre labels used in the MASC dataset we have an idea of how many clusters we want to start with. This is a good case to employ the k-means clustering algorithm.</p>
<p>In k-means clustering, we specify the number of clusters that we want to identify. For each cluster number, a random center is generated. Then each observation is assigned to the cluster with the nearest center. The center of each cluster is then recalculated based on the distribution of the observations in the cluster. This process iterates either a pre-defined number of times, or until the centers converge (<em>i.e.</em> observations stop switching clusters).</p>
<p>The <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function from base R takes the matrix of features as its first argument and the number of clusters as its second argument. We can specify the number of clusters with the <code>centers</code> argument. Other arguments <code>nstart</code> and <code>iter.max</code> can be used to specify the number of random starts and the maximum number of iterations, respectively. Since the starting point for centers is random, it is recommendable to run the algorithm multiple times with different starting points. Furthermore, we will limit the iterations to avoid the algorithm running indefinitely.</p>
<p>Our goal, then, will be to assess how well this number of clusters fits the data. After finding the optimal number of clusters, we can then compare the results with the genre variable to see how well the clusters map to the values of this variable.</p>
<p>One way to assess the fit of the clustering algorithm is to visualize the results, interpret, and adjust the number of clusters, if necessary, any number of times. Another, more efficient, approach is to algorithmically assess the variability of the clusters based on differing number of clusters and then select the number of clusters that best fits the data.</p>
<p>We will take the later approach and plot the <strong>within-cluster sum of squares</strong> (WSS) for a range of values for <span class="math inline">\(k\)</span>. The WSS is the sum of the squared distance between each observation and its cluster center. With a plot of the WSS for a range of values for <span class="math inline">\(k\)</span>, we can identify the value for <span class="math inline">\(k\)</span> where the WSS begins to level off, using the elbow method. It is not always clear where the elbow is, yet it is a good starting point for identifying the optimal number of clusters.</p>
<p>The <code>fviz_nbclust()</code> function can be used to plot the WSS for a range of values for <span class="math inline">\(k\)</span>. The <code>fviz_nbclust()</code> function takes the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function as its first argument and the matrix of features as its second argument. The <code>fviz_nbclust()</code> function also takes arguments <code>method = "wss"</code> to specify the WSS method and <code>k.max = 20</code> to specify the maximum number of clusters to plot. Let’s plot the WSS for a range of values for <span class="math inline">\(k\)</span>, as seen in <a href="#fig-explore-masc-pos-kmeans-elbow" class="quarto-xref">Figure&nbsp;<span>8.5</span></a>, using the code in <a href="#exm-explore-masc-pos-kmeans-elbow" class="quarto-xref">Example&nbsp;<span>8.21</span></a>.</p>
<div style="page-break-after: always;"></div>
<div id="exm-explore-masc-pos-kmeans-elbow" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.21</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a>masc_pos_pca_pc <span class="sc">|&gt;</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>  <span class="fu">fviz_nbclust</span>(</span>
<span id="cb32-3"><a href="#cb32-3"></a>    <span class="at">FUNcluster =</span> kmeans,</span>
<span id="cb32-4"><a href="#cb32-4"></a>    <span class="at">method =</span> <span class="st">"wss"</span>, <span class="co"># method</span></span>
<span id="cb32-5"><a href="#cb32-5"></a>    <span class="at">k.max =</span> <span class="dv">20</span>,</span>
<span id="cb32-6"><a href="#cb32-6"></a>    <span class="at">nstart =</span> <span class="dv">25</span>,</span>
<span id="cb32-7"><a href="#cb32-7"></a>    <span class="at">iter.max =</span> <span class="dv">20</span></span>
<span id="cb32-8"><a href="#cb32-8"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-pos-kmeans-elbow" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A line plot showing the within-cluster sum of squares for a range of values for $k$ in the MASC dataset. The plot shows the WSS for $k$ ranging from 1 to 20. The plot shows that the WSS decreases rapidly from 1 to 4 clusters and then levels off after 5-7 clusters.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-pos-kmeans-elbow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-pos-kmeans-elbow-1.png" class="img-fluid figure-img" alt="A line plot showing the within-cluster sum of squares for a range of values for $k$ in the MASC dataset. The plot shows the WSS for $k$ ranging from 1 to 20. The plot shows that the WSS decreases rapidly from 1 to 4 clusters and then levels off after 5-7 clusters." width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-pos-kmeans-elbow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.5: Elbow method for k-means clustering
</figcaption></figure>
</div>
</div>
</div>
<p>It is clear that there are significant gains in cluster fit from 1 to 4 clusters, but the gains begin to level off after 5 clusters.</p>
<p>Now we have an informed selection for <span class="math inline">\(k\)</span>. Let’s use 4 clusters in the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function and collect the results, as seen in <a href="#exm-explore-masc-pos-kmeans-fit" class="quarto-xref">Example&nbsp;<span>8.22</span></a>.</p>
<div id="exm-explore-masc-pos-kmeans-fit" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.22</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># for reproducibility</span></span>
<span id="cb33-2"><a href="#cb33-2"></a></span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="co"># k-means: for 4 clusters</span></span>
<span id="cb33-4"><a href="#cb33-4"></a>masc_pos_kmeans_fit <span class="ot">&lt;-</span></span>
<span id="cb33-5"><a href="#cb33-5"></a>  masc_pos_pca_pc <span class="sc">|&gt;</span></span>
<span id="cb33-6"><a href="#cb33-6"></a>  <span class="fu">kmeans</span>(</span>
<span id="cb33-7"><a href="#cb33-7"></a>    <span class="at">centers =</span> <span class="dv">4</span>,</span>
<span id="cb33-8"><a href="#cb33-8"></a>    <span class="at">nstart =</span> <span class="dv">25</span>,</span>
<span id="cb33-9"><a href="#cb33-9"></a>    <span class="at">iter.max =</span> <span class="dv">20</span></span>
<span id="cb33-10"><a href="#cb33-10"></a>  )</span>
<span id="cb33-11"><a href="#cb33-11"></a></span>
<span id="cb33-12"><a href="#cb33-12"></a><span class="co"># Preview</span></span>
<span id="cb33-13"><a href="#cb33-13"></a>masc_pos_kmeans_fit<span class="sc">$</span>cluster[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  1  10 100 101 102 103 104 105 106 107 
  1   1   2   3   3   2   2   2   4   3 </code></pre>
</div>
</div>
<p> </p>
</div>
<p>The preview from <a href="#exm-explore-masc-pos-kmeans-fit" class="quarto-xref">Example&nbsp;<span>8.22</span></a> shows the cluster assignments for the first 10 documents (<code>doc_id</code>) in the dataset.</p>
<p>From this point we can join document-cluster pairings produced by the k-means algorithm with the original dataset. We can then explore the clusters in terms of the original features. We can also explore the clusters in terms of the original labels.</p>
<p>Let’s join the cluster assignments to the original dataset, as seen in <a href="#exm-explore-masc-pos-kmeans-join" class="quarto-xref">Example&nbsp;<span>8.23</span></a>.</p>
<div id="exm-explore-masc-pos-kmeans-join" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.23</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># Organize k-means clusters into a tibble</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>masc_pos_cluster_tbl <span class="ot">&lt;-</span></span>
<span id="cb35-3"><a href="#cb35-3"></a>  <span class="fu">tibble</span>(</span>
<span id="cb35-4"><a href="#cb35-4"></a>    <span class="at">doc_id =</span> <span class="fu">names</span>(masc_pos_kmeans_fit<span class="sc">$</span>cluster),</span>
<span id="cb35-5"><a href="#cb35-5"></a>    <span class="at">cluster =</span> masc_pos_kmeans_fit<span class="sc">$</span>cluster</span>
<span id="cb35-6"><a href="#cb35-6"></a>  )</span>
<span id="cb35-7"><a href="#cb35-7"></a></span>
<span id="cb35-8"><a href="#cb35-8"></a><span class="co"># Join cluster assignments to original dataset</span></span>
<span id="cb35-9"><a href="#cb35-9"></a>masc_cluster_tbl <span class="ot">&lt;-</span></span>
<span id="cb35-10"><a href="#cb35-10"></a>  masc_tbl<span class="sc">|&gt;</span></span>
<span id="cb35-11"><a href="#cb35-11"></a>  <span class="fu">left_join</span>(</span>
<span id="cb35-12"><a href="#cb35-12"></a>    masc_pos_cluster_tbl,</span>
<span id="cb35-13"><a href="#cb35-13"></a>    <span class="at">by =</span> <span class="st">"doc_id"</span></span>
<span id="cb35-14"><a href="#cb35-14"></a>  )</span>
<span id="cb35-15"><a href="#cb35-15"></a></span>
<span id="cb35-16"><a href="#cb35-16"></a><span class="co"># Preview</span></span>
<span id="cb35-17"><a href="#cb35-17"></a>masc_cluster_tbl <span class="sc">|&gt;</span></span>
<span id="cb35-18"><a href="#cb35-18"></a>  <span class="fu">slice_head</span>(<span class="at">n =</span> <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 8
  doc_id modality genre   term_num term         lemma        pos   cluster
  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;
1 1      Written  Letters        2 Your         your         PRP$        1
2 1      Written  Letters        3 contribution contribution NN          1
3 1      Written  Letters        4 to           to           TO          1
4 1      Written  Letters        6 will         will         MD          1
5 1      Written  Letters        7 mean         mean         VB          1</code></pre>
</div>
</div>
<p> </p>
</div>
<p>We now see that the cluster assignments from the k-means algorithm have been joined to the original dataset. We can now explore the clusters in terms of the original features. For example, let’s look at the distribution of the clusters across genre, as seen in <a href="#exm-explore-masc-pos-kmeans-genre" class="quarto-xref">Example&nbsp;<span>8.24</span></a>. To do this, we first need to reduce our dataset to the distinct combinations of genre and cluster. Then, we can use {janitor}’s <code>tabyl()</code> function to provided formatted percentages.</p>
<div id="exm-explore-masc-pos-kmeans-genre" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.24</strong></span> &nbsp;</p>
<div class="cell" data-tbl-colwidths="[20,20,20,20,20]">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># Load package</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="fu">library</span>(janitor)</span>
<span id="cb37-3"><a href="#cb37-3"></a></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="co"># Reduce to distinct combinations of genre and cluster</span></span>
<span id="cb37-5"><a href="#cb37-5"></a>masc_meta_tbl <span class="ot">&lt;-</span></span>
<span id="cb37-6"><a href="#cb37-6"></a>  masc_cluster_tbl <span class="sc">|&gt;</span></span>
<span id="cb37-7"><a href="#cb37-7"></a>  <span class="fu">distinct</span>(genre, cluster)</span>
<span id="cb37-8"><a href="#cb37-8"></a></span>
<span id="cb37-9"><a href="#cb37-9"></a><span class="co"># Tabulate: cluster by genre</span></span>
<span id="cb37-10"><a href="#cb37-10"></a>masc_meta_tbl <span class="sc">|&gt;</span></span>
<span id="cb37-11"><a href="#cb37-11"></a>  <span class="fu">tabyl</span>(genre, cluster) <span class="sc">|&gt;</span></span>
<span id="cb37-12"><a href="#cb37-12"></a>  <span class="fu">adorn_percentages</span>(<span class="st">"col"</span>) <span class="sc">|&gt;</span></span>
<span id="cb37-13"><a href="#cb37-13"></a>  <span class="fu">adorn_pct_formatting</span>(<span class="at">digits =</span> <span class="dv">1</span>) <span class="sc">|&gt;</span></span>
<span id="cb37-14"><a href="#cb37-14"></a>  <span class="fu">as_tibble</span>() <span class="sc">|&gt;</span></span>
<span id="cb37-15"><a href="#cb37-15"></a>  <span class="fu">tt</span>(<span class="at">width =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="tbl-explore-masc-pos-kmeans-genre" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-tbl-colwidths="[20,20,20,20,20]">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-explore-masc-pos-kmeans-genre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8.7: Distribution of clusters by genre
</figcaption><div aria-describedby="tbl-explore-masc-pos-kmeans-genre-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<table class="cell caption-top table table-sm table-striped small">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead><tr class="header">
<th>genre</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Blog</td>
<td>7.7%</td>
<td>20.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Email</td>
<td>7.7%</td>
<td>20.0%</td>
<td>8.3%</td>
<td>20.0%</td>
</tr>
<tr class="odd">
<td>Essay</td>
<td>7.7%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>20.0%</td>
</tr>
<tr class="even">
<td>Face-to-face</td>
<td>7.7%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>Fiction</td>
<td>7.7%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Fictlets</td>
<td>7.7%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>Government</td>
<td>0.0%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Jokes</td>
<td>7.7%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>Journal</td>
<td>7.7%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Letters</td>
<td>7.7%</td>
<td>20.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>Movie Script</td>
<td>7.7%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Newspaper</td>
<td>7.7%</td>
<td>20.0%</td>
<td>8.3%</td>
<td>20.0%</td>
</tr>
<tr class="odd">
<td>Non-fiction</td>
<td>0.0%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>20.0%</td>
</tr>
<tr class="even">
<td>Technical</td>
<td>0.0%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>20.0%</td>
</tr>
<tr class="odd">
<td>Telephone</td>
<td>7.7%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Transcript</td>
<td>7.7%</td>
<td>0.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
<tr class="odd">
<td>Travel Guide</td>
<td>0.0%</td>
<td>0.0%</td>
<td>8.3%</td>
<td>0.0%</td>
</tr>
<tr class="even">
<td>Twitter</td>
<td>0.0%</td>
<td>20.0%</td>
<td>0.0%</td>
<td>0.0%</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<p> </p>
</div>
<p>From <a href="#exm-explore-masc-pos-kmeans-genre" class="quarto-xref">Example&nbsp;<span>8.24</span></a>, we can see that the clusters are not evenly distributed across the genres. In particular, cluster 2 tends to be more associated with ‘blog’, ‘email’, ‘letters’, ‘twitter’, and ‘newspaper’. Another interesting cluster is cluster 4, which is more associated with ‘non-fiction’, and interestingly, ‘email’ and ‘newspaper’. This suggest that the clusters are capturing some of the variation in the across the genres and potential within some of the genres.</p>
<p>We could continue to explore genre, but we could also entertain the possibility that the clusters may capture differences between modality —even some interaction between modality and genre! This highlights how exploratory data analysis through clustering can be used to identify new questions and new variables of interest.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-lightbulb" aria-label="lightbulb"></i> Consider this</strong></p>
<p>Given the cluster assignments derived using the distribution of POS tags, what other relationships between the clusters and the original features could one explore? What are the limitations of this approach? What are the implications of this approach for the interpretation of the results?</p>
</div>
</div>
</div>
</section><section id="sec-explore-vector-space-models" class="level4"><h4 class="anchored" data-anchor-id="sec-explore-vector-space-models">Vector space models</h4>
<p>In our discussion of clustering, we targeted associations between documents based on the distribution of linguistic features. We now turn to targeting associations between linguistic features based on their distribution across documents. The technique we will introduce is known as <strong>vector space modeling</strong>. Vector space modeling aims to represent linguistic features as numerical vectors which reflect the various linguistic contexts in which the features appear. Together these vectors form a feature-context space in which features with similar contextual distributions are closer together.</p>
<p>An interesting property of vector space models is that they are able to capture semantic and/or syntactic relationships between features based on their distribution. In this way, vector space modeling can be seen as an implementation of the <strong>distributional hypothesis</strong> —that is, terms that appear in similar linguistic contexts tend to have similar meanings <span class="citation" data-cites="Harris1954">(<a href="../references.html#ref-Harris1954" role="doc-biblioref">Harris, 1954</a>)</span>. As <span class="citation" data-cites="Firth1957">Firth (<a href="../references.html#ref-Firth1957" role="doc-biblioref">1957</a>)</span> states, “you shall know a word by the company it keeps”.</p>
<p>Let’s assume in our textbook project we are interested in gathering information about English’s expression of the semantic concepts of manner and motion. For learners of English, this can be an area of difficulty as languages differ in how these semantic properties are expressed. English is an example of a “satellite-framed” language, that is that manner and motion are often encoded in the same verb with a particle encoding the motion path (“rush out”, “climb up”). Other languages such as Spanish, Turkish, and Japanese are “verb-framed” languages, that is that motion but not manner is encoded in the verb (“salir corriendo”, “koşarak çıkmak”, “走り出す”).</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-file-alt" aria-label="file-alt"></i> Case study</strong></p>
<p><span class="citation" data-cites="Garg2018">Garg, Schiebinger, Jurafsky, &amp; Zou (<a href="../references.html#ref-Garg2018" role="doc-biblioref">2018</a>)</span> quantify and compare gender and ethnic stereotypes over time using word embeddings. The authors explore the temporal dynamics of stereotypes using word embeddings as a quantitative measure of bias. The data used includes word embeddings from the Google News dataset for contemporary analysis, as well as embeddings from the COHA and Google Books datasets for historical analysis. Additional validation is done using embeddings from the New York Times Annotated Corpus. Several word lists representing gender, ethnicity, and neutral words are collated for analysis. The main finding is that language reflects and perpetuates cultural stereotypes, and the analysis shows consistency in the relationships between embedding bias and external metrics across datasets over time. The results also highlight the impact of historical events, such as the women’s movement of the 1960s, on the encoding of stereotypes.</p>
</div>
</div>
</div>
<p>We can use vector space modeling to attempt to represent the distribution of verbs in the MASC dataset and then target the concepts of manner and motion to then explore how English encodes these concepts. The question will be what our features will be. They could be terms, lemmas, POS tags, <em>etc</em>. Or they could be some combination. Considering the task at hand, which we will ultimately want to know something about verbs, it makes sense to include the POS information in combination with either the term or the lemma.</p>
<p>If we include term and POS then we have a feature for every morphological variant of the term (<em>e.g.</em> house_VB, housed_VBD, housing_VBG). This can make the model larger than it needs to be. If we include lemma and POS then we have a feature for every lemma with a distinct grammatical category (<em>e.g.</em> house_NN, house_VB). Note that as the POS tags are from the Penn tagset, many morphological variants appear in the tag itself (<em>e.g.</em> house_VB, houses_VBZ, housing_VBG). This demonstrates how the choice of features can impact the size of the model. In our case, it is not clear that we need to include the morphological variants of the verbs, so I will use lemmas and recode the POS variables as a simplified tagset.</p>
<p>After simplifying the features, we can then apply the vector space model (VSM) to the MASC dataset. When VSM is applied to words, it is known as <strong>word embedding</strong>. To calculate word embeddings there are various algorithms that can be used (BERT, word2vec, GloVe, <em>etc.</em>) We will use the <strong>word2vec</strong> <span class="citation" data-cites="Mikolov2013b">(<a href="../references.html#ref-Mikolov2013b" role="doc-biblioref">Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013</a>)</span> algorithm. Word2vec is a neural network-based algorithm that learns word embeddings from a large corpus of text. In the word2vec algorithm, the researcher can choose to learn embeddings from a <strong>Continuous Bag of Words</strong> (CBOW) or a <strong>Skip-gram model</strong>. The CBOW model predicts a target word based on the context words. The Skip-gram model predicts the context words based on the target word. The CBOW model is faster to train and is better for frequent words. The Skip-gram model is slower to train and is better for infrequent words.</p>
<div class="halfsize callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-medal" aria-label="medal"></i> Dive deeper</strong></p>
<p>Choosing window size and dimensions for word2vec models is another important consideration. The window size is the number of words that the model will consider as context for the target word. Smaller window sizes tend to capture more syntactic information, while larger window sizes tend to capture more semantic information.</p>
<p>The number of dimensions is the number of features that the model will learn. More dimensions can capture more information, but can also lead to <strong>overfitting</strong> —picking up on nuances that are particular to the dataset and that do not generalize well. Fewer dimensions can capture less information, but can also lead to <strong>underfitting</strong> —<em>not</em> picking up on nuances that are particular to the dataset and that do generalize well. The number of dimensions is a hyperparameter that can be tuned to optimize the model for the task at hand.</p>
</div>
</div>
</div>
<p>Another consideration to take into account is the size of the corpus used to train the model. VSM provide more reliable results when trained on larger corpora. The MASC dataset is relatively small. We’ve simplified our features in order to have a smaller vocabulary in hopes to offset this limitation to a degree. But the choice of either CBOW or Skip-gram can also help to offset this limitation. CBOW can be better for smaller corpora as it aggregates context information. Skip-gram can be better for larger corpora as it can capture more nuanced relationships between words.</p>
<p>To implement the word2vec algorithm on our lemma + POS features, we will use {word2vec}. The <code>word2vec()</code> function takes a text file and uses it to train the vector representations. To prepare the MASC dataset for training, we will need to write the lemma + POS features to a text file as a single character string. We can do this by first collapsing the <code>lemma_pos</code> variable into a single string for the entire corpus using the <code>str_c()</code> function. Then we can use the <code>write_lines()</code> function to write the string to a text file, as in <a href="#exm-explore-masc-vsm-write-txt" class="quarto-xref">Example&nbsp;<span>8.25</span></a>.</p>
<div id="exm-explore-masc-vsm-write-txt" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.25</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="co"># Write lemma + POS to text file</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>masc_tbl <span class="sc">|&gt;</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>  <span class="fu">summarize</span>(<span class="at">text =</span> <span class="fu">str_c</span>(lemma_pos, <span class="at">collapse =</span> <span class="st">" "</span>)) <span class="sc">|&gt;</span></span>
<span id="cb38-4"><a href="#cb38-4"></a>  <span class="fu">pull</span>(text) <span class="sc">|&gt;</span></span>
<span id="cb38-5"><a href="#cb38-5"></a>  <span class="fu">write_lines</span>(</span>
<span id="cb38-6"><a href="#cb38-6"></a>    <span class="at">file =</span> <span class="st">"../data/analysis/masc_lemma_pos.txt"</span></span>
<span id="cb38-7"><a href="#cb38-7"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> </p>
</div>
<p>With the single line text file on disk, we will read it in, apply the word2vec algorithm using {word2vec} <span class="citation" data-cites="R-word2vec">(<a href="../references.html#ref-R-word2vec" role="doc-biblioref">Wijffels &amp; Watanabe, 2023</a>)</span>, and write the model to disk. By default, the <code>word2vec()</code> function applies the CBOW model, with 50 dimensions, a window size of 5, and a minimum word count of 5. We can change these parameters as needed, but let’s apply the default algorithm to the text file splitting features by sentence punctuation, as seen in <a href="#exm-explore-masc-vsm-word2vec-train" class="quarto-xref">Example&nbsp;<span>8.26</span></a>.</p>
<div id="exm-explore-masc-vsm-word2vec-train" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.26</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># Load package</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="fu">library</span>(word2vec)</span>
<span id="cb39-3"><a href="#cb39-3"></a></span>
<span id="cb39-4"><a href="#cb39-4"></a><span class="co"># Traing word2vec model</span></span>
<span id="cb39-5"><a href="#cb39-5"></a>masc_model <span class="ot">&lt;-</span></span>
<span id="cb39-6"><a href="#cb39-6"></a>  <span class="fu">word2vec</span>(</span>
<span id="cb39-7"><a href="#cb39-7"></a>    <span class="at">x =</span> <span class="st">"../data/analysis/masc_lemma_pos.txt"</span>,</span>
<span id="cb39-8"><a href="#cb39-8"></a>    <span class="at">type =</span> <span class="st">"cbow"</span>, <span class="co"># or "skip-gram"</span></span>
<span id="cb39-9"><a href="#cb39-9"></a>    <span class="at">dim =</span> <span class="dv">100</span>,</span>
<span id="cb39-10"><a href="#cb39-10"></a>    <span class="at">split =</span> <span class="fu">c</span>(<span class="st">" "</span>),</span>
<span id="cb39-11"><a href="#cb39-11"></a>    <span class="at">threads =</span> <span class="dv">8</span>L</span>
<span id="cb39-12"><a href="#cb39-12"></a>  )</span>
<span id="cb39-13"><a href="#cb39-13"></a></span>
<span id="cb39-14"><a href="#cb39-14"></a><span class="co"># Write model to disk</span></span>
<span id="cb39-15"><a href="#cb39-15"></a><span class="fu">write.word2vec</span>(</span>
<span id="cb39-16"><a href="#cb39-16"></a>  masc_model,</span>
<span id="cb39-17"><a href="#cb39-17"></a>  <span class="at">file =</span> <span class="st">"../data/analysis/masc_lemma_pos.bin"</span></span>
<span id="cb39-18"><a href="#cb39-18"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> </p>
</div>
<p>Writing the model to disk is important as it allows us to read the model in without having to retrain it. In cases where the corpus is large, this can save a lot of computational time.</p>
<p>Now that we have a trained model, we can read it in with the <code>read.vectors()</code> function from {wordVectors} as in <a href="#exm-explore-masc-vsm-word2vec-read" class="quarto-xref">Example&nbsp;<span>8.27</span></a>.</p>
<div id="exm-explore-masc-vsm-word2vec-read" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.27</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Load package</span></span>
<span id="cb40-2"><a href="#cb40-2"></a><span class="fu">library</span>(wordVectors)</span>
<span id="cb40-3"><a href="#cb40-3"></a></span>
<span id="cb40-4"><a href="#cb40-4"></a><span class="co"># Read word2vec model</span></span>
<span id="cb40-5"><a href="#cb40-5"></a>masc_model <span class="ot">&lt;-</span></span>
<span id="cb40-6"><a href="#cb40-6"></a>  <span class="fu">read.vectors</span>(</span>
<span id="cb40-7"><a href="#cb40-7"></a>    <span class="at">filename =</span> <span class="st">"../data/analysis/masc_lemma_pos.bin"</span></span>
<span id="cb40-8"><a href="#cb40-8"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>read.vectors()</code> function returns a matrix where each row is a term in the model and each column is a dimension in the vector space, as seen in <a href="#exm-explore-masc-vsm-word2vec-vector-object" class="quarto-xref">Example&nbsp;<span>8.28</span></a>.</p>
<div id="exm-explore-masc-vsm-word2vec-vector-object" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.28</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Inspect</span></span>
<span id="cb41-2"><a href="#cb41-2"></a><span class="fu">dim</span>(masc_model)</span>
<span id="cb41-3"><a href="#cb41-3"></a></span>
<span id="cb41-4"><a href="#cb41-4"></a><span class="co"># Preview</span></span>
<span id="cb41-5"><a href="#cb41-5"></a>masc_model[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5808  100
A VectorSpaceModel object of  5  words and  5  vectors
                   [,1]   [,2]    [,3]   [,4]    [,5]
abbreviated_ADJ   1.068  1.103 -0.3439  0.386 -1.0062
absent_ADJ       -1.839 -1.753  0.0658  0.119  0.9376
absorb_VERB      -1.772 -1.528 -0.0554  0.664 -1.2453
accidentally_ADV -1.264 -0.742 -0.6870  0.613 -1.0750
aesthetic_ADJ     0.567  0.524  1.0638 -0.332 -0.0424
attr(,".cache")
&lt;environment: 0x139c496d0&gt;</code></pre>
</div>
</div>
</div>
<p>The row-wise vector in the model is the vector representation of each feature. The notion is that these values can now be compared with other features to explore distributional relatedness. We can extract specific features from the matrix using the <code>[]</code> operator.</p>
<p>As an example, let’s compare the vectors for noun-verb pairs for the lemmas ‘run’ and ‘walk’. To do this we extract these features from the model. To appreciate the relatedness of these features it is best to visualize them. We can do this by first reducing the dimensionality of the vectors using principal components analysis. We can then plot the first two principal components with the code in <a href="#exm-explore-masc-vsm-word2vec-similarity" class="quarto-xref">Example&nbsp;<span>8.29</span></a> which produces <a href="#fig-explore-masc-vsm-word2vec-similarity" class="quarto-xref">Figure&nbsp;<span>8.6</span></a>.</p>
<div id="exm-explore-masc-vsm-word2vec-similarity" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.29</strong></span> &nbsp;</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="co"># Extract vectors</span></span>
<span id="cb43-2"><a href="#cb43-2"></a>word_vectors <span class="ot">&lt;-</span></span>
<span id="cb43-3"><a href="#cb43-3"></a>  masc_model[<span class="fu">c</span>(<span class="st">"run_VERB"</span>, <span class="st">"walk_VERB"</span>, <span class="st">"run_NOUN"</span>, <span class="st">"walk_NOUN"</span>), ] <span class="sc">|&gt;</span></span>
<span id="cb43-4"><a href="#cb43-4"></a>  <span class="fu">as.matrix</span>()</span>
<span id="cb43-5"><a href="#cb43-5"></a></span>
<span id="cb43-6"><a href="#cb43-6"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># for reproducibility</span></span>
<span id="cb43-7"><a href="#cb43-7"></a></span>
<span id="cb43-8"><a href="#cb43-8"></a>pca <span class="ot">&lt;-</span></span>
<span id="cb43-9"><a href="#cb43-9"></a>  word_vectors <span class="sc">|&gt;</span></span>
<span id="cb43-10"><a href="#cb43-10"></a>  <span class="fu">scale</span>() <span class="sc">|&gt;</span></span>
<span id="cb43-11"><a href="#cb43-11"></a>  <span class="fu">prcomp</span>()</span>
<span id="cb43-12"><a href="#cb43-12"></a></span>
<span id="cb43-13"><a href="#cb43-13"></a>pca_tbl <span class="ot">&lt;-</span></span>
<span id="cb43-14"><a href="#cb43-14"></a>  <span class="fu">as_tibble</span>(pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]) <span class="sc">|&gt;</span></span>
<span id="cb43-15"><a href="#cb43-15"></a>  <span class="fu">mutate</span>(<span class="at">word =</span> <span class="fu">rownames</span>(word_vectors))</span>
<span id="cb43-16"><a href="#cb43-16"></a></span>
<span id="cb43-17"><a href="#cb43-17"></a>pca_tbl <span class="sc">|&gt;</span></span>
<span id="cb43-18"><a href="#cb43-18"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2, <span class="at">label =</span> word)) <span class="sc">+</span></span>
<span id="cb43-19"><a href="#cb43-19"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb43-20"><a href="#cb43-20"></a>  ggrepel<span class="sc">::</span><span class="fu">geom_text_repel</span>(<span class="at">size =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-vsm-word2vec-similarity" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A scatterplot showing the similarity between 'run' and 'walk' in the MASC dataset. The plot shows the first two principal components of the vectors for 'run' and 'walk', as nouns and as verbs.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-vsm-word2vec-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-vsm-word2vec-similarity-1.png" class="img-fluid figure-img" alt="A scatterplot showing the similarity between 'run' and 'walk' in the MASC dataset. The plot shows the first two principal components of the vectors for 'run' and 'walk', as nouns and as verbs." width="264">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-vsm-word2vec-similarity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.6: Similarity between ‘run’ and ‘walk’
</figcaption></figure>
</div>
</div>
</div>
<p> </p>
</div>
<p>From <a href="#fig-explore-masc-vsm-word2vec-similarity" class="quarto-xref">Figure&nbsp;<span>8.6</span></a>, we can see that each of these features occupies a distinct position in the reduced vector space. But on closer inspection, we can see that there is a relationship between the lemma pairs. Remember that PCA reduces the dimensionality of the data by identifying the dimensions that capture the greatest amount of variance in the data. This means that of the 50 dimensions in the model, the PC1 and PC2 correspond to orthogonal dimensions that capture the greatest amount of variance in the data. If we look along PC1, we can see that there is a distinction between POS. Looking along PC2, we see some parity between lemma meanings. Given these features, we can see that meaning and grammatical category can be approximated in the vector space.</p>
<p>An interesting property of vector space models is that we can build up a dimension of meaning by adding vectors that we expect to approximate that meaning. For example, we can add the vectors for typical motion verbs to create a vector for motion-similarity and one for manner-similarity. We can then compare the feature vectors for all verbs and assess their motion-similarity and manner-similarity.</p>
<p>To do this let’s first subset the model to only include verbs, as in <a href="#exm-explore-masc-vsm-word2vec-verbs" class="quarto-xref">Example&nbsp;<span>8.30</span></a>. We will also remove the POS tags from the row names of the matrix as they are no longer needed.</p>
<div id="exm-explore-masc-vsm-word2vec-verbs" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.30</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Filter to verbs</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>verbs <span class="ot">&lt;-</span> <span class="fu">str_subset</span>(<span class="fu">rownames</span>(masc_model), <span class="st">".*_VERB"</span>)</span>
<span id="cb44-3"><a href="#cb44-3"></a>verb_vectors <span class="ot">&lt;-</span> masc_model[verbs, ]</span>
<span id="cb44-4"><a href="#cb44-4"></a></span>
<span id="cb44-5"><a href="#cb44-5"></a><span class="co"># Remove POS tags</span></span>
<span id="cb44-6"><a href="#cb44-6"></a><span class="fu">rownames</span>(verb_vectors) <span class="ot">&lt;-</span></span>
<span id="cb44-7"><a href="#cb44-7"></a>  verb_vectors <span class="sc">|&gt;</span></span>
<span id="cb44-8"><a href="#cb44-8"></a>  <span class="fu">rownames</span>() <span class="sc">|&gt;</span></span>
<span id="cb44-9"><a href="#cb44-9"></a>  <span class="fu">str_replace_all</span>(<span class="st">"_VERB"</span>, <span class="st">""</span>)</span>
<span id="cb44-10"><a href="#cb44-10"></a></span>
<span id="cb44-11"><a href="#cb44-11"></a><span class="co"># Inspect</span></span>
<span id="cb44-12"><a href="#cb44-12"></a><span class="fu">dim</span>(verb_vectors)</span>
<span id="cb44-13"><a href="#cb44-13"></a></span>
<span id="cb44-14"><a href="#cb44-14"></a><span class="co"># Preview</span></span>
<span id="cb44-15"><a href="#cb44-15"></a>verb_vectors[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1115  100
A VectorSpaceModel object of  5  words and  5  vectors
          [,1]   [,2]    [,3]    [,4]   [,5]
absorb  -1.772 -1.528 -0.0554  0.6642 -1.245
auction -2.083 -0.977 -0.2505 -0.0204 -0.874
bid      0.217 -0.490 -0.4588  0.1373  0.247
brief    1.215 -0.674 -0.7121  0.5072 -0.445
cap     -0.135  0.884  0.2278 -0.2563 -0.207
attr(,".cache")
&lt;environment: 0x13a0f0748&gt;</code></pre>
</div>
</div>
<p> </p>
</div>
<p>We now have <code>verb_vectors</code> which includes the vector representations for all verbs 1,115 in the MASC dataset. Next, let’s seed the vectors for motion-similarity and manner-similarity and calculate the vector ‘closeness’ to the motion and manner seed vectors with the <code>closest_to()</code> function from {wordVectors} package, in <a href="#exm-explore-masc-vsm-word2vec-manner-motion" class="quarto-xref">Example&nbsp;<span>8.32</span></a>.</p>
<div style="page-break-after: always;"></div>
<div id="exm-explore-masc-vsm-word2vec-manner-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.31</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a><span class="co"># Add vectors for motion-similarity and manner-similarity</span></span>
<span id="cb46-2"><a href="#cb46-2"></a>motion <span class="ot">&lt;-</span></span>
<span id="cb46-3"><a href="#cb46-3"></a>  <span class="fu">c</span>(<span class="st">"go"</span>, <span class="st">"come"</span>, <span class="st">"leave"</span>, <span class="st">"arrive"</span>, <span class="st">"enter"</span>, <span class="st">"exit"</span>, <span class="st">"depart"</span>, <span class="st">"return"</span>)</span>
<span id="cb46-4"><a href="#cb46-4"></a></span>
<span id="cb46-5"><a href="#cb46-5"></a>motion_similarity <span class="ot">&lt;-</span></span>
<span id="cb46-6"><a href="#cb46-6"></a>  verb_vectors <span class="sc">|&gt;</span> <span class="fu">closest_to</span>(motion, <span class="at">n =</span> <span class="cn">Inf</span>)</span>
<span id="cb46-7"><a href="#cb46-7"></a></span>
<span id="cb46-8"><a href="#cb46-8"></a><span class="co"># Preview</span></span>
<span id="cb46-9"><a href="#cb46-9"></a><span class="fu">glimpse</span>(motion_similarity)</span>
<span id="cb46-10"><a href="#cb46-10"></a></span>
<span id="cb46-11"><a href="#cb46-11"></a>manner <span class="ot">&lt;-</span></span>
<span id="cb46-12"><a href="#cb46-12"></a>  <span class="fu">c</span>(<span class="st">"run"</span>, <span class="st">"walk"</span>, <span class="st">"jump"</span>, <span class="st">"crawl"</span>, <span class="st">"swim"</span>, <span class="st">"fly"</span>, <span class="st">"drive"</span>, <span class="st">"ride"</span>)</span>
<span id="cb46-13"><a href="#cb46-13"></a></span>
<span id="cb46-14"><a href="#cb46-14"></a>manner_similarity <span class="ot">&lt;-</span></span>
<span id="cb46-15"><a href="#cb46-15"></a>  verb_vectors <span class="sc">|&gt;</span> <span class="fu">closest_to</span>(manner, <span class="at">n =</span> <span class="cn">Inf</span>)</span>
<span id="cb46-16"><a href="#cb46-16"></a></span>
<span id="cb46-17"><a href="#cb46-17"></a><span class="co"># Preview</span></span>
<span id="cb46-18"><a href="#cb46-18"></a><span class="fu">glimpse</span>(manner_similarity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1,115
Columns: 2
$ word                   &lt;chr&gt; "walk", "step", "return", "enter", "leave", "le…
$ `similarity to motion` &lt;dbl&gt; 0.742, 0.741, 0.732, 0.727, 0.682, 0.669, 0.664…
Rows: 1,115
Columns: 2
$ word                   &lt;chr&gt; "walk", "drop", "step", "hang", "rub", "shut", …
$ `similarity to manner` &lt;dbl&gt; 0.865, 0.841, 0.831, 0.826, 0.826, 0.824, 0.820…</code></pre>
</div>
</div>
<p> </p>
</div>
<p>The <code>motion_similarity</code> and <code>manner_similarity</code> data frames each contain all the verbs with a corresponding closeness measure. We can join these two data frames by feature to create a single data frame with the motion-similarity and manner-similarity measures, as seen in <a href="#exm-explore-masc-vsm-word2vec-manner-motion" class="quarto-xref">Example&nbsp;<span>8.32</span></a>.</p>
<div id="exm-explore-masc-vsm-word2vec-manner-motion" class="theorem example">
<p><span class="theorem-title"><strong>Example 8.32</strong></span> &nbsp;</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a><span class="co"># Join motion-similarity and manner-similarity</span></span>
<span id="cb48-2"><a href="#cb48-2"></a>manner_motion_similarity <span class="ot">&lt;-</span></span>
<span id="cb48-3"><a href="#cb48-3"></a>  manner_similarity <span class="sc">|&gt;</span></span>
<span id="cb48-4"><a href="#cb48-4"></a>  <span class="fu">inner_join</span>(motion_similarity)</span>
<span id="cb48-5"><a href="#cb48-5"></a></span>
<span id="cb48-6"><a href="#cb48-6"></a><span class="co"># Preview</span></span>
<span id="cb48-7"><a href="#cb48-7"></a><span class="fu">glimpse</span>(manner_motion_similarity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 1,115
Columns: 3
$ word                   &lt;chr&gt; "walk", "drop", "step", "hang", "rub", "shut", …
$ `similarity to manner` &lt;dbl&gt; 0.865, 0.841, 0.831, 0.826, 0.826, 0.824, 0.820…
$ `similarity to motion` &lt;dbl&gt; 0.742, 0.642, 0.741, 0.635, 0.624, 0.589, 0.561…</code></pre>
</div>
</div>
<p> </p>
</div>
<p>The result of <a href="#exm-explore-masc-vsm-word2vec-manner-motion" class="quarto-xref">Example&nbsp;<span>8.32</span></a> is a data frame with the motion-similarity and manner-similarity measures for all verbs in the MASC dataset. We can now visualize the distribution of motion-similarity and manner-similarity measures, as seen in <a href="#fig-explore-masc-vsm-word2vec-manner-motion-compare" class="quarto-xref">Figure&nbsp;<span>8.7</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-explore-masc-vsm-word2vec-manner-motion-compare" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A scatterplot showing the motion-similarity and manner-similarity of verbs in the MASC dataset. The plot shows the similarity to motion on the x-axis and the similarity to manner on the y-axis. The plot shows the motion-similarity and manner-similarity of 50 randomly sampled verbs from the dataset, as well as the motion and manner seed vectors.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-explore-masc-vsm-word2vec-manner-motion-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="8_explore_files/figure-html/fig-explore-masc-vsm-word2vec-manner-motion-compare-1.png" class="img-fluid figure-img" alt="A scatterplot showing the motion-similarity and manner-similarity of verbs in the MASC dataset. The plot shows the similarity to motion on the x-axis and the similarity to manner on the y-axis. The plot shows the motion-similarity and manner-similarity of 50 randomly sampled verbs from the dataset, as well as the motion and manner seed vectors." width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-explore-masc-vsm-word2vec-manner-motion-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.7: Motion-similarity and manner-similarity of verbs
</figcaption></figure>
</div>
</div>
</div>
<p>From <a href="#fig-explore-masc-vsm-word2vec-manner-motion-compare" class="quarto-xref">Figure&nbsp;<span>8.7</span></a>, we see that manner-similarity is plotted on the x-axis and motion-similarity on the y-axis. I’ve added lines to divide the scatterplot into quadrants: the top-right shows high manner- and motion-similarity, while the bottom-left shows low manner- and motion-similarity. Verbs in the top-left quadrant have high motion-similarity but low manner-similarity, and verbs in the bottom-right quadrant have high manner-similarity but low motion-similarity.</p>
<p>I’ve randomly sampled 50 verbs from the dataset and plotted them with text labels, along with the motion and manner seed vectors as triangle and box points, respectively. Motion- and manner-similarity seed verbs appear together in the top-right quadrant, indicating their semantic relationship. Verbs in other quadrants exhibit lower similarity in either manner or motion, or both. Qualitatively, many verbs align with intuition, though some do not, which is expected given the model’s training on a relatively small corpus. This example demonstrates how vector space modeling can explore semantic relationships between linguistic features.</p>
</section></section></section><section id="activities" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="activities">Activities</h2>
<p>Exploratory analysis is a wide-ranging term that encompasses many different methods. In these activities, we will focus on the methods that are most commonly used in the analysis of textual data. These include frequency and distributional analysis, clustering, and word embedding models. We will model how to explore iteratively using the output of one method to inform the next and ultimately to address a research question.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-regular fa-file-code" aria-label="file-code"></i> Recipe</strong></p>
<p><strong>What</strong>: Exploratory analysis methods<br><strong>How</strong>: Read Recipe 8, complete comprehension check, and prepare for Lab 8.<br><strong>Why</strong>: To illustrate how to prepare a dataset for descriptive and unsupervised machine learning methods and evaluate the results for exploratory data analysis.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong><i class="fa-solid fa-flask" aria-label="flask"></i> Lab</strong></p>
<p><strong>What</strong>: Pattern discovery<br><strong>How</strong>: Clone, fork, and complete the steps in Lab 8.<br><strong>Why</strong>: To gain experience working with coding strategies to prepare, feature engineer, explore, and evaluate results from exploratory data analyses, practice transforming datasets into new object formats and visualizing relationships, and implement organizational strategies for organizing and reporting results in a reproducible fashion.</p>
</div>
</div>
</div>
</section><section id="summary" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="summary">Summary</h2>
<p>In this chapter, we surveyed a range of methods for uncovering insights from data, particularly when we do not have a predetermined hypothesis. We broke the chapter discussion along the two central branches of exploratory data analysis: descriptive analysis and unsupervised learning. Descriptive analysis offers statistical or visual summaries of datasets through frequency, dispersion, and co-occurrence measures, while unsupervised learning utilizes machine learning techniques to uncover patterns without pre-defining variable relationships. Here we covered a few unsupervised learning methods including clustering, dimensionality reduction, and vector space modeling. Through either descriptive or unsupervised learning methodologies, we probe questions in a data-driven fashion and apply methods to summarize, reduce, and sort complex datasets. This in turn facilitates novel, quantitative perspectives that can subsequently be evaluated qualitatively, offering us a robust approach to exploring and generating research questions.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Ackoff1989" class="csl-entry" role="listitem">
Ackoff, R. L. (1989). From data to wisdom. <em>Journal of Applied Systems Analysis</em>, <em>16</em>(1), 3–9.
</div>
<div id="ref-R-igraph" class="csl-entry" role="listitem">
Csárdi, G., Nepusz, T., Traag, V., Horvát, S., Zanini, F., Noom, D., &amp; Müller, K. (2024). <em><span class="nocase">igraph</span>: Network analysis and visualization</em>. Retrieved from <a href="https://r.igraph.org/">https://r.igraph.org/</a>
</div>
<div id="ref-Firth1957" class="csl-entry" role="listitem">
Firth, J. R. (1957). <em>Papers in linguistics</em>. Oxford University Press.
</div>
<div id="ref-Garg2018" class="csl-entry" role="listitem">
Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. <em>Proceedings of the National Academy of Sciences</em>, <em>115</em>(16), E3635–E3644. doi:<a href="https://doi.org/10.1073/pnas.1720347115">10.1073/pnas.1720347115</a>
</div>
<div id="ref-Gries2023" class="csl-entry" role="listitem">
Gries, S. Th. (2023). New technologies and advances in statistical analysis in recent decades. In M. Díaz-Campos &amp; S. Balasch (Eds.), <em>The <span>Handbook</span> of <span>Usage-Based Linguistics</span></em> (first edition.). John Wiley &amp; Sons Inc.
</div>
<div id="ref-Harris1954" class="csl-entry" role="listitem">
Harris, Z. S. (1954). Distributional structure. <em>Word</em>, <em>10</em>(2-3), 146–162. doi:<a href="https://doi.org/10.1080/00437956.1954.11659520">10.1080/00437956.1954.11659520</a>
</div>
<div id="ref-Ide2008" class="csl-entry" role="listitem">
Ide, N., Baker, C., Fellbaum, C., Fillmore, C., &amp; Passonneau, R. (2008). <span>MASC</span>: <span>The Manually Annotated Sub-Corpus</span> of <span>American English</span>. In <em>Sixth <span>International Conference</span> on <span>Language Resources</span> and <span>Evaluation</span>, <span>LREC</span> 2008</em> (pp. 2455–2460). European Language Resources Association (ELRA).
</div>
<div id="ref-Mikolov2013b" class="csl-entry" role="listitem">
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In <em>Advances in neural information processing systems</em> (pp. 3111–3119).
</div>
<div id="ref-R-ggraph" class="csl-entry" role="listitem">
Pedersen, T. L. (2024). <em><span class="nocase">ggraph</span>: An implementation of grammar of graphics for graphs and networks</em>. Retrieved from <a href="https://ggraph.data-imaginist.com">https://ggraph.data-imaginist.com</a>
</div>
<div id="ref-Petrenz2011" class="csl-entry" role="listitem">
Petrenz, P., &amp; Webber, B. (2011). Stable classification of text genres. <em>Computational Linguistics</em>, <em>37</em>(2), 385–393. doi:<a href="https://doi.org/10.1162/COLI_a_00052">10.1162/COLI_a_00052</a>
</div>
<div id="ref-Rowley2007" class="csl-entry" role="listitem">
Rowley, J. (2007). The wisdom hierarchy: <span>Representations</span> of the <span>DIKW</span> hierarchy. <em>Journal of Information Science</em>, <em>33</em>(2), 163–180. doi:<a href="https://doi.org/10.1177/0165551506070706">10.1177/0165551506070706</a>
</div>
<div id="ref-R-word2vec" class="csl-entry" role="listitem">
Wijffels, J., &amp; Watanabe, K. (2023). <em><span class="nocase">word2vec</span>: Distributed representations of words</em>. Retrieved from <a href="https://github.com/bnosac/word2vec">https://github.com/bnosac/word2vec</a>
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../part_4/index.html" class="pagination-link" aria-label="Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Analysis</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_4/9_predict.html" class="pagination-link" aria-label="Predict">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Predict</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/qtalr/book/blob/main/part_4/8_explore.qmd" target="_blank" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/qtalr/book/edit/main/part_4/8_explore.qmd" target="_blank" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/qtalr/book/issues" target="_blank" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>