% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{latex/krantz}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage{makeidx}
\makeindex
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Block}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Quantitative Text Analysis for Linguistics},
  pdfauthor={Jerid Francom},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Quantitative Text Analysis for Linguistics}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Reproducible Research using R}
\author{Jerid Francom}
\date{December 7, 2022}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[colback={shadecolor}, enhanced, breakable, frame hidden, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}

This textbook is an introduction to the fundamental concepts and
practical programming skills from Data Science that are increasingly
employed in a variety of language-centered fields and sub-fields applied
to the task of quantitative text analysis. It is geared towards advanced
undergraduates, graduate students, and researchers looking to expand
their methodological toolbox.

\textbf{Author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at
Wake Forest University. His research focuses on the use of large-scale
language archives (corpora) from a variety of sources (news, social
media, and other internet sources) to better understand the linguistic
and cultural similarities and differences between language varieties for
both scholarly and pedagogical projects. He has published on topics
including the development, annotation, and evaluation of linguistic
corpora and analyzed corpora through corpus, psycholinguistic, and
computational methodologies. He also has experience working with and
teaching statistical programming with R.

\hypertarget{license}{%
\section*{License}\label{license}}
\addcontentsline{toc}{section}{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is
licensed under a Creative Commons Attribution-NonCommercial-ShareAlike
3.0 United States License.

\hypertarget{credits}{%
\section*{Credits}\label{credits}}
\addcontentsline{toc}{section}{Credits}

Icons made from Icon Fonts are licensed by CC BY 3.0

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

TAD has been reviewed by and suggestions and changes incorporated based
on the feedback through
\href{https://hypothes.is/groups/Q3o92MJg/tad}{the TAD Hypothes.is
group} by the following people: Andrea Bowling, Caroline Brady, Declan
Golsen, Asya Little, Claudia Valdez, \ldots{}

\hypertarget{build-information}{%
\section*{Build information}\label{build-information}}
\addcontentsline{toc}{section}{Build information}

This version of the textbook was built with R version 4.1.2 (2021-11-01)
on macOS Big Sur 10.16 with the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule()
package & version & source \\
\midrule()
\endhead
dplyr & 1.0.10 & CRAN (R 4.1.2) \\
forcats & 0.5.2 & CRAN (R 4.1.2) \\
ggplot2 & 3.3.6 & CRAN (R 4.1.2) \\
here & 1.0.1 & CRAN (R 4.1.0) \\
knitr & 1.40 & CRAN (R 4.1.2) \\
purrr & 0.3.5 & CRAN (R 4.1.2) \\
readr & 2.1.3 & CRAN (R 4.1.2) \\
rmarkdown & 2.17 & CRAN (R 4.1.2) \\
stringr & 1.4.1 & CRAN (R 4.1.2) \\
tadr & 0.1.1 & local \\
tibble & 3.1.8 & CRAN (R 4.1.2) \\
tidyr & 1.2.1 & CRAN (R 4.1.2) \\
tidyverse & 1.3.2 & CRAN (R 4.1.2) \\
webshot & 0.5.4 & CRAN (R 4.1.2) \\
\bottomrule()
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{sec-preface}{%
\chapter*{Preface}\label{sec-preface}}
\addcontentsline{toc}{chapter}{Preface}

\begin{quote}
The journey of a thousand miles begins with one step.

--- \href{https://en.wikipedia.org/wiki/Laozi}{Lao Tzu}
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What is the rationale for this textbook?
\item
  What are the aims and the approach taken in this textbook?
\item
  How is the textbook designed to support attaining these aims?
\item
  What is needed to get started?
\end{itemize}

\end{tcolorbox}

This chapter aims to provide a brief summary of current research trends
that form the context for the rationale for this textbook. It also
provides instructors and readers an overview of the aims and approach of
the textbook, a description of the main components of each section and
chapter, a guide to conventions used in the book, and a summary of the
supporting resources available. Additionally information on setting up
your computing environment and where to seek support is included.

\hypertarget{rationale}{%
\section*{Rationale}\label{rationale}}
\addcontentsline{toc}{section}{Rationale}

In recent years there has been a growing buzz around the term `Data
Science' and related terms; data analytics, data mining, \emph{etc}. In
a nutshell data science is the process by which an investigator
leverages statistical methods and computational power to uncover insight
from machine-readable data sources. Driven in large part by the increase
in computing power available to the average individual and the
increasing amount of electronic data that is now available through the
internet, interest in data science has expanded to virtually all fields
in academia and areas in the public sector.

This textbook is an introduction to the fundamental concepts and
practical programming skills from data science applied to the task of
quantitative text analysis. The intended readership for this textbook is
primarily undergraduate students but may also be applicable to graduate
students and researchers looking to expand their methodological toolbox.
This textbook aims to meet the growing interest in quantitative data
analysis methods taking hold across linguistics subfields and
language-informed disciplines (digital humanities, political science,
economics, \emph{inter alia}). To ensure that this resource is
accessible to a wide variety of students and researchers it does not
assume a background in linguistics. Additionally, some readers
interested in the aforementioned disciplines may lack experience with
and/ or feel hesitant towards statistical methods and/ or programming.
To make this textbook attractive to novices in quantitative text
analysis methods, I will make no assumptions about the reader's
experience with quantitative data analysis or programming, in general,
or programming with the statistical programming language R, in
particular.

\hypertarget{aims}{%
\section*{Aims}\label{aims}}
\addcontentsline{toc}{section}{Aims}

This textbook aims to develop the reader's proficiency in three main
areas:

\textbf{Data literacy}: the ability to interpret, assess, and
contextualize findings based on data. Throughout this textbook we will
explore topics which will help you understand how data analysis methods
derive insight from data. In this process you will be encouraged to
critically evaluate connections across linguistic and language-related
disciplines using data analysis knowledge and skills. Data literacy is
an invaluable skillset for academics and professionals but also is an
indispensable aptitude for in the 21st century citizens to navigate and
actively participate in the `Information Age' in which we live (Carmi et
al. 2020).

\textbf{Research skills}: the ability to conduct original research,
communicate findings, and make meaningful connections with findings in
the literature of the field. This target area does not differ
significantly, in spirit, from common learning outcomes in a research
methods course: identify an area of investigation, develop a viable
research question or hypothesis, collect relevant data, analyze data
with relevant statistical methods, and interpret and communicate
findings. However, working with text will incur a series of key steps in
the selection, collection, and preparation of the data that are unique
to text analysis projects. In addition, I will stress the importance of
research documentation and creating reproducible research as an integral
part of modern scientific inquiry (Buckheit and Donoho 1995).

\textbf{Programming skills}: the ability to implement research skills
programmatically and produce research that is replicable and
collaborative. Modern data analysis, and by extension, text analysis is
conducted using programming. There are various key reasons for this: (1)
programming affords researchers unlimited research freedom --if you can
envision it, you can program it. The same cannot be said for
off-the-shelf software which is either proprietary or unmaintained --or
both. (2) Programming underlies well-documented and reproducible
research --documenting button clicks and menu option selections leads to
research which is not readily reproduced, either by some other
researcher or by your future self! (3) Programming invites researchers
to engage more intimately with the data and the methods for analysis.
The more familiar you are with the data and the methods the more likely
you are to produce higher quality work.

\hypertarget{approach}{%
\section*{Approach}\label{approach}}
\addcontentsline{toc}{section}{Approach}

Goals

\begin{itemize}
\item
  Include the \textbf{importance of goal-driven research} (real research
  problems used in the text)
\item
  \textbf{Emphasize statistical thinking} as part of the goal formation
  process through data organization and visualization
\item
  \textbf{Ensure communication is the end goal} with literate
  programming from the outset.
\end{itemize}

Implementation

\begin{itemize}
\item
  Programming approach

  \begin{itemize}
  \item
    Flexibility
  \item
    Transparency
  \item
    Reproducibility
  \end{itemize}
\item
  Why R?

  \begin{itemize}
  \item
    One stop shopping
  \item
    You are not alone
  \item
    Robust IDE (RStudio)
  \end{itemize}
\item
  Why the Tidyverse?\\
  The `tidyverse' is a set of packages which are designed to work around
  the principle of tidy data. The `tidy' approach to to data has caught
  on in the R programming community and the set of packages which
  implement this approach to R programming is rapidly growing. The
  `tidyverse' approach to R programming is attractive for a number of
  reasons:

  \begin{itemize}
  \item
    primary data format is assummed to be a tidy data frame, or tibble
    (more on this later)
  \item
    operations are often chained together through a convention known as
    `piping' which provides the ability to process data in logical
    chunks, only assigning output at meaningful stages.
  \item
    consistent functions and expected output- workflow coverage for the
    main components of data science: import, tidy, transform/ visualize/
    model, and communicate
  \item
    ...
  \end{itemize}
\end{itemize}

\hypertarget{resources}{%
\section*{Resources}\label{resources}}
\addcontentsline{toc}{section}{Resources}

This textbook includes three resources to support learning in the areas
of Data literacy, Research skills, and Programming skills in a
systematic chapter-related fashion: 1) the textbook itself which
includes prose discussion, figures/tables, R code, and thought and
practical exercises, 2) a companion website
``\href{https://lin380.github.io/tadr/}{Text as Data Resources}'' which
includes programming tutorials and demonstrations to develop and augment
the reader's recognition of how programming strategies are implemented,
and 3) a \href{https://github.com/lin380}{GitHub repository} which
contains both a set of interactive R programming lessons
(\href{https://github.com/lin380/swirl}{Swirl}) and
\href{https://github.com/stars/francojc/lists/labs}{lab exercises} which
guide the reader through practical hands-on programming applications.

\hypertarget{conventions}{%
\section*{Conventions}\label{conventions}}
\addcontentsline{toc}{section}{Conventions}

This textbook is about the concepts for understanding and the techniques
for doing quantitative text analysis with R. Therefore there will be an
intermingling of prose and code presented. As such, an attempt to
establish consistent conventions throughout the text has been made to
signal the reader's attention as appropriate.

In terms of prose, key concepts will be signaled using \textbf{bold},
package names will appear in title case (Tidyverse), function names will
appear as verbatim text with parentheses (\texttt{read\_csv()}) and
object names as verbatim text without parentheses
(\texttt{variable\_1}).

As we explore concepts, R code itself will be incorporated into the
text. For example, the code block in Block~\ref{lst-code-block} shows
actual R code and the results that are generated when running this code.
Note that the hashtag \texttt{\#} to the right of \texttt{1\ +\ 1}
signals the beginning of a \textbf{code comment}. Everything right of
the \texttt{\#} is not run as code. In this textbook you will see code
comments above code on a separate line and sometimes to the right of
code on the same line. The code follows within the same code block and a
subsequent code blocks display the output of the code.

\begin{codelisting}

\caption{Example code block}

\hypertarget{lst-code-block}{%
\label{lst-code-block}}%
\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1} \CommentTok{\# Add 1 plus 1}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{verbatim}
#> [1] 2
\end{verbatim}

Code blocks which make use of functions will be hyperlinked to the
function's online documentation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste}\NormalTok{(}\StringTok{"Hello world!"}\NormalTok{) }\CommentTok{\# simple \textquotesingle{}hello world\textquotesingle{} message}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Hello world!"
\end{verbatim}

Inline code will be used when code blocks are short and the results are
not needed for display. For example, the same code as above will
sometimes appear as \texttt{1\ +\ 1}.

At times meta-description of code or code chunk options will appear.
This is particularly relevant for descriptions on authoring Quarto
documents.

\begin{Shaded}
\begin{Highlighting}[]
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: test{-}code }
\CommentTok{\#| include: false}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

There is a series of text blocks that will be used to signal the
reader's attention.

Key points summarize the main points to be covered in a chapter or a
subsection of the text.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What is the rationale for this textbook?
\item
  What are the aims and the approach taken in this textbook?
\item
  How is the textbook designed to support attaining these aims?
\item
  What is needed to get started?
\end{itemize}

\end{tcolorbox}

From time to time there will be points for you to consider and questions
to explore.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-important-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm]
Consider the objectives in this course: what ways can the knowledge and
skills you will learn benefit you in your academic studies and/ or
professional and personal life?
\end{tcolorbox}

Case studies are provided in-line which highlight key concepts and/ or
methodological approaches relevant to the current topic or section.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Eisenstein et al. (2012) track the geographic spread of neologisms from
city to city in the United States using Twitter data collected between
6/2009 and 5/2011. They only used tweets with geolocation data and then
associated each tweet with a zipcode using the US Census. The most
populous metropolitan areas were used. They also used the demographics
from these areas to make associations between lexical innovations and
demographic attributes. From this analysis they are able to reconstruct
a network of linguistic influence. One of the main findings is that
demographically-similar cities are more likely to share linguistic
influence. At the individual level, there is a strong, potentially
stronger role of demographics than geographical location.
\end{tcolorbox}

Swirl interactive R programming lessons appear at the beginning of each
chapter. The lessons provide a guided environment to experience running
code in the R console. The instructions to install the \texttt{swirl}
package and the textbook lessons can be found on the
``\href{https://lin380.github.io/tadr}{Text as Data Resources}'' site or
directly from \href{https://github.com/lin380/swirl}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Intro to Swirl}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons.
\end{tcolorbox}

At the end of each chapter, a text block will provide readers a cue to
explore the applied programming demonstrations called
``\href{https://lin380.github.io/tadr/articles/}{Recipes}'' on the
``\href{https://lin380.github.io/tadr}{Text as Data Resources}'' site.
Readers may add online annotations using the built-in social annotation
tool
\href{https://web.hypothes.is/education/annotated/research/}{hypothes.is}.
\emph{Note: Instructors may opt to
\href{https://web.hypothes.is/creating-groups/}{create their own private
Hypothes.is annotation group}.}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_1.html}{Literate
programming I}\\
\textbf{How}: Read Recipe 1 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To introduce the concept of Literate Programming using R,
RStudio, and R Markdown.
\end{tcolorbox}

Hands-on lab activities to implement and extend programming strategies
round out each chapter. These labs are found on
\href{https://github.com/lin380?q=lab\&type=all\&language=\&sort=name}{GitHub}
and can be downloaded and/ or cloned to any RStudio instance (either
your computer or on the web
\href{https://www.rstudio.com/products/cloud/}{RStudio Cloud}).

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_1}{Literate
programming I}\\
\textbf{\emph{How}}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 1
into practice. Specifically, you will create and edit an R Markdown
document and render a report in PDF format.
\end{tcolorbox}

Tips are used to signal helpful tips and warnings that might otherwise
be overlooked.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
During a the course of an exploratory work session, many R objects are
often created to test ideas. At some point inspecting the workspace
becomes difficult due to the number of objects displayed using
\texttt{ls()}.

To remove all objects from the workspace, use
\texttt{rm(list\ =\ ls())}.
\end{tcolorbox}

Although this is not intended to be a in-depth introduction to
statistical techniques, mathematical formulas will at times be included
in the text. These formulas will appear either inline \(1 + 1 = 2\) or
as block equations as in Equation~\ref{eq-example-formula}.

\begin{equation}\protect\hypertarget{eq-example-formula}{}{
\hat{c} = \underset{c \in C} {\mathrm{argmax}} ~\hat{P}(c) \prod_i \hat{P}(w_i|c)
}\label{eq-example-formula}\end{equation}

Data analysis leans heavily on graphical representations. Figures will
appear numbered, as in Figure~\ref{fig-test-fig}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./preface_files/figure-pdf/fig-test-fig-1.pdf}

}

\caption{\label{fig-test-fig}Test plot from mtcars dataset}

\end{figure}

Tables, such as Table~\ref{tbl-test-table} will be numbered separately
from figures.

\hypertarget{tbl-test-table}{}
\begin{table}
\caption{\label{tbl-test-table}Here is a nice table! }\tabularnewline

\centering
\begin{tabular}{r|r|r|r|l}
\hline
Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\
\hline
5.1 & 3.5 & 1.4 & 0.2 & setosa\\
\hline
4.9 & 3.0 & 1.4 & 0.2 & setosa\\
\hline
4.7 & 3.2 & 1.3 & 0.2 & setosa\\
\hline
4.6 & 3.1 & 1.5 & 0.2 & setosa\\
\hline
5.0 & 3.6 & 1.4 & 0.2 & setosa\\
\hline
5.4 & 3.9 & 1.7 & 0.4 & setosa\\
\hline
4.6 & 3.4 & 1.4 & 0.3 & setosa\\
\hline
5.0 & 3.4 & 1.5 & 0.2 & setosa\\
\hline
4.4 & 2.9 & 1.4 & 0.2 & setosa\\
\hline
4.9 & 3.1 & 1.5 & 0.1 & setosa\\
\hline
\end{tabular}
\end{table}

\hypertarget{getting-started}{%
\section*{Getting started}\label{getting-started}}
\addcontentsline{toc}{section}{Getting started}

Before jumping in to this and subsequent chapter's textbook activities,
it is important to prepare your computing environment and understand how
to take advantage of the resources available, both those directly and
indirectly associated with the textbook.

\hypertarget{r-and-rstudio}{%
\subsection*{R and RStudio}\label{r-and-rstudio}}
\addcontentsline{toc}{subsection}{R and RStudio}

Programming is the backbone for modern quantitative research. R is a
popular programming language with statisticians and was adopted by many
other fields in natural and social sciences. It is freely downloadable
from \href{https://www.r-project.org/}{The R Project for Statistical
Programming} website and is available for
\href{https://cloud.r-project.org/}{macOS, Linux, and Windows} operating
systems.

While R code can be written and executed in many different environments,
\href{https://www.rstudio.com/products/rstudio/}{RStudio} provides a
very powerful interface that has been widely adopted by R programmers.
RStudio is an IDE (Integrated Development Environment) and serves as a
dashboard for working with R --therefore you must download and install R
before installing RStudio. You may choose to run RStudio on your own
computer (\href{https://www.rstudio.com/products/rstudio/}{RStudio
Desktop}) or use RStudio on the web
(\href{https://www.rstudio.com/products/cloud/}{RStudio Cloud}). There
are advantages to both approaches. Either approach will be compatible
with this textbook but if you plan to continue to work with R/RStudio in
the future at some point you will most likely want to install the
desktop version and maintain your own R and RStudio environment.

For more details to install R and RStudio consult the
\href{https://education.rstudio.com/learn/beginner/}{RStudio Education}
page.

\hypertarget{r-packages}{%
\subsection*{R packages}\label{r-packages}}
\addcontentsline{toc}{subsection}{R packages}

Throughout your R programming journey you will take advantage of code
created by other R users in the form of packages. A package is a
downloadable set of functions and/ or datasets which aim to accomplish a
given cohesive set of related tasks. There are official R package
repositories such as \href{https://cran.r-project.org/}{CRAN}
(Comprehensive R Archive Network) and other packages are available on
code-sharing repositories such as \href{https://github.com/}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]
The Comprehensive R Archive Network (CRAN) includes groupings of popular
packages related to a given applied programming task called
\href{https://cran.r-project.org/web/views/}{Task Views}. Explore the
available CRAN Task Views listings. Note the variety of areas (tasks)
that are covered in this listing. Now explore in more detail one of the
following task views which are directly related to topics covered in
this textbook noting the associated packages and their descriptions: (1)
Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4)
ReproducibleResearch.
\end{tcolorbox}

You will download a number of packages at different stages of this
textbook, but there is a set of packages that will be key to have from
the get go. Once you have access to a working R/ RStudio environment,
you can proceed to install the following packages.

Install the following packages from CRAN.

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse}
\item
  \texttt{rmarkdown}
\item
  \texttt{tinytex}
\item
  \texttt{devtools}
\item
  \texttt{usethis}
\item
  \texttt{swirl}
\end{itemize}

You can do this by running the following code in the RStudio Console
pane.

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# install key packages from CRAN}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{, }\StringTok{"usethis"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Or you can use the RStudio Packages pane and click `Install' and type
the names of the packages.

This textbook includes a support package \texttt{tadr} which is
available on GitHub (\href{https://github.com/lin380/tadr}{source
code}). To install this package from a GitHub repository, you run the
following code in the RStudio Console pane:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install the tadr package from GitHub}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"lin380/tadr"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Finally, although not a package we will need to download the interactive
R programming lessons for this textbook that will be accessed with the
\texttt{swirl} package. Download these lessons by running the following
code in the RStudio Console pane.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install the swirl lessons for this textbook}
\NormalTok{swirl}\SpecialCharTok{::}\FunctionTok{install\_course\_github}\NormalTok{(}\StringTok{"lin380"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Later in this Preface and then at the beginning of each subsequent
chapter there will be swirl lessons to complete. To load and choose a
lesson to start, you will run the following code in the RStudio Console
pane.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the swirl package}
\FunctionTok{library}\NormalTok{(swirl) }
\CommentTok{\# run swirl}
\FunctionTok{swirl}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You will then follow the prompts to select and complete the desired
lesson.

\hypertarget{git-and-github}{%
\subsection*{Git and GitHub}\label{git-and-github}}
\addcontentsline{toc}{subsection}{Git and GitHub}

\href{https://github.com/}{GitHub} is a code sharing website. Modern
computing is highly collaborative and GitHub is a very popular platform
for sharing and collaborating on coding projects. The
\href{https://github.com/stars/francojc/lists/labs}{lab exercises for
this textbook} are shared on GitHub. To access and complete these
exercises you will need to
\href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{sign
up for a (free) GitHub account} and then set up the version control
software \texttt{git} on your computing environment. \texttt{git} is the
conduit to interfacing GitHub and for many \texttt{git} will already be
installed on your computer (or cloud computing environment). To verify
your installation (or for installation instructions) and to set up your
\texttt{git} configuration, consult the very useful
\href{https://happygitwithr.com/}{Happy Git and GitHub for the useR}
chapter \href{https://happygitwithr.com/install-git.html}{Install Git}.

\hypertarget{getting-help}{%
\subsection*{Getting help}\label{getting-help}}
\addcontentsline{toc}{subsection}{Getting help}

The technologies employed in this approach to text analysis will include
a somewhat steep learning curve. And in all honesty, the learning never
stops! Experienced programmers and novices alike require support.
Fortunately there is a very large community of programmers who have
developed many official support resources and who actively contribute to
unofficial discussion forums. Together these resources provide ample
methods for overcoming any challenge.

The easiest and most convenient place to get help with either R or
RStudio is through the RStudio ``Help'' toolbar menu. There you will
find links to help resources, guides, and manuals. R packages often
include ``Vignettes'' (long-form documentation and demonstrations).
These can be accessed either by running \texttt{browseVignettes()} in
the RStudio Console pane or by searching for the package using a search
engine in your web browser and consulting the package documentation
there (e.g.~\href{https://usethis.r-lib.org/}{\texttt{usethis}}). For
some of the more common packages you can find
\href{https://www.rstudio.com/resources/cheatsheets/}{cheatsheets} on
the RStudio website.

For Git and GitHub I recommend \href{https://happygitwithr.com/}{Happy
Git and GitHub for the useR} but the official
\href{https://git-scm.com/doc}{Git} and
\href{https://docs.github.com/en}{GitHub} documentation pages are great
resources as well.

There are a number of very popular discussion forum websites where the
programming community asks and answers questions to real-world issues.
These sites often have subsections dedicated to particular programming
languages or software. Here is a list of some of the most useful in my
experience:

\begin{itemize}
\tightlist
\item
  StackOverflow: \href{https://stackoverflow.com/questions/tagged/r}{R},
  \href{https://stackoverflow.com/questions/tagged/git}{Git},
  \href{https://stackoverflow.com/questions/tagged/rstudio}{RStudio},
  \href{https://stackoverflow.com/questions/tagged/github}{GitHub}
\item
  Reddit: \href{https://www.reddit.com/r/rstats/}{R},
  \href{https://www.reddit.com/r/git/}{Git},
  \href{https://www.reddit.com/r/RStudio/}{RStudio},
  \href{https://www.reddit.com/r/github/}{Github}
\item
  \href{https://community.rstudio.com/}{RStudio Community}
\end{itemize}

If you post a question on one of these communities ensure that if your
question involves some coding issue or error that you provide enough
background such that the community will be able to help you.

\begin{itemize}
\item
  https://reprex.tidyverse.org/
\item
  https://github.com/MilesMcBain/datapasta
\end{itemize}

The take-home message here is that you are not alone. There are many
people world-wide that are learning to program and/ or contribute to the
learning of others. The more you engage with these resources and
communities the more successful your learning will be. As soon as you
are able, pay it forward. Posting questions and offering answers helps
the community and engages and refines your skills --a win-win.

\hypertarget{summary}{%
\section*{Summary}\label{summary}}
\addcontentsline{toc}{section}{Summary}

In this preface I've provided the rationale and aims of this textbook.
The structure of the texbook and the associated resources work to
scaffold your learning and proficiency in the areas of Data literacy,
Research skills, and Programming skills. The textbook include a series
of conventions to signal important concepts, questions to explore, and
resources available. As in the area of Data Science in general,
quantitative text analysis is most effectively conducted using
programmatic approaches. The process will not be without challenges but
the gains are well worth the effort. I've outlined key resources to
obtain support that are invaluable for the novice as well as the
seasoned practitioner.

\hypertarget{activities}{%
\section*{Activities}\label{activities}}
\addcontentsline{toc}{section}{Activities}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Swirl}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Intro to Swirl}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_1.html}{Literate
programming I}\\
\textbf{How}: Read Recipe 1 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To introduce the concept of Literate Programming using R,
RStudio, and R Markdown.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_1}{Literate
programming I}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 1
into practice. Specifically, you will create and edit an R Markdown
document and render a report in PDF format.
\end{tcolorbox}

\part{Foundations}

In this section the aims are to: 1) provide an overview of quantitative
research and their applications, by both highlighting visible
applications and notable research in various fields, 2) consider how
quantitative research contributes to language research, and 3) layout
the main types of research and situate quantitative text analysis inside
these.

\hypertarget{sec-text-analysis-in-context}{%
\chapter{Text analysis in context}\label{sec-text-analysis-in-context}}

\begin{quote}
Science walks forward on two feet, namely theory and
experiment\ldots Sometimes it is one foot which is put forward first,
sometimes the other, but continuous progress is only made by the use of
both.

---
\href{https://www.nobelprize.org/uploads/2018/06/millikan-lecture.pdf}{Robert
A. Millikan} (1923)
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What is the role and goals of data analysis in and outside of
  academia?
\item
  In what ways is quantitative language research approached?
\item
  What are some of the applications of text analysis?
\end{itemize}

\end{tcolorbox}

In this chapter I will aim to introduce the topic of text analysis and
text analytics and frame the approach of this textbook. The aim is to
introduce the context needed to understand how text analysis fits in a
larger universe of data analysis and see the commonalities in the
ever-ubiquitous field of data analysis, with attention to how
linguistics and language-related studies employ data analysis down to
the particular area of text analysis. To round out this chapter, I will
provide a general overview of the rest of the textbook motivating the
general structure and sequencing as well as setting the foundation for
programmatic approaches to data analysis.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Variables and
vectors, Workspace}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To explore some key building blocks of the R programming
language and to examine your local workspace in R and understand the
relationship between your R workspace and the file system of your
machine.
\end{tcolorbox}

\hypertarget{making-sense-of-a-complex-world}{%
\section{Making sense of a complex
world}\label{making-sense-of-a-complex-world}}

The world around us is full of actions and interactions so numerous that
it is difficult to really comprehend. Through this lens each individual
sees and experiences this world. We gain knowledge about this world and
build up heuristic knowledge about how it works and how we do and can
interact with it. This happens regardless of your educational
background. As humans we are built for this. Our minds process countless
sensory inputs many of which never make it to our conscious mind. They
underlie skills and abilities that we take for granted like being able
to predict what will happen if you see someone about to knock a wine
glass off a table and onto a concrete floor. You've never seen this
object before and this is the first time you've been to this winery, but
somehow and from somewhere you `instinctively' make an effort to warn
the would-be-glass-breaker before it is too late. You most likely have
not stopped to consider where this predictive knowledge has come from,
or if you have, you may have just chalked it up to `common sense'. As
common as it may be, it is an incredible display of the brain's capacity
to monitor your environment, relate the events and observations that
take place, and store that information all the time not making a big
fuss to tell your conscious mind what it's up to.

So wait, this is a textbook on text analytics and language, right? So
what does all this have to do with that? Well, there are two points to
make that are relevant for framing our journey: (1) the world is full of
countless information which unfold in real-time at a scale that is
daunting and (2) for all the power of the brain that works so
efficiently behind the scene making sense of the world, we are one
individual living one life that has a limited view of the world at
large. Let me expand on these two points a little more.

First let's be clear. There is no way for anyone to experience all
things at all times. But even extremely reduced slices of reality are
still vastly outside of our experiential capacity, at least in
real-time. One can make the point that since the inception of the
internet an individual's ability to experience larger slices of the
world has increased. But could you imagine reading, watching, and
listening to every file that is currently accessible on the web? Or has
been? (See the \href{https://web.archive.org/}{Wayback Machine}.) Scale
this down even further; let's take Wikipedia, the world's largest
encyclopedia. Can you imagine reading every wiki entry? As large as a
resource such as Wikipedia is \footnote{As of 22 July 2021, there are
  6,341,359 articles in the
  \href{https://en.wikipedia.org/wiki/English_Wikipedia}{English
  Wikipedia} containing over 3.9 billion words occupying around 19
  gigabytes of information.}, it is still a small fragment of the
written language that is produced on the web, just the web \footnote{For
  reference, \href{https://commoncrawl.org/big-picture/}{Common Crawl}
  has millions of gigabytes collected since 2008}. Consider that for a
moment.

To my second framing point, which is actually two points in one. I
underscored the efficiency of our brain's capacity to make sense of the
world. That efficiency comes from some clever evolutionary twists that
lead our brain to take in the world but it makes some shortcuts that
compress the raw experience into heuristic understanding. What that
means is that the brain is not a supercomputer. It does not store every
experience in raw form, we do not have access to the records of our
experience like we would imagine a computer would have access to the
records logged in a database. Where our brains do excel is in making
associations and predictions that help us (most of the time) navigate
the complex world we inhabit. This point is key --our brains are doing
some amazing work, but that work can give us the impression that we
understand the world in more detail that we actually do. Let's do a
little thought experiment. Close your eyes and think about the last time
you saw your best friend. What were they wearing? Can you remember the
colors? If your like me, or any other human, you probably will have a
pretty confident feeling that you know the answers to these questions
and there is a chance you a right. But it has been demonstrated in
numerous experiments on human memory that our confidence does not
correlate with accuracy (Talarico and Rubin 2003; Roediger and McDermott
2000). You've experienced an event, but there is no real reason that we
should bet our lives on what we experienced. It's a little bit scary,
for sure, but the magic is that it works `good enough' for practical
purposes.

So here's the deal: as humans we are (1) clearly unable to experience
large swaths of experience by the simple fact that we are individuals
living individual lives and (2) the experiences we do live are not
recorded with precision and therefore we cannot `trust' our intuitions,
at least in an absolute sense.

What does that mean for our human curiosity about the world around us
and our ability to reliably make sense of it? In short it means that we
need to approach understanding our world with the tools of science.
Science is so powerful because it makes strides to overcome our inherit
limitations as humans (breadth of our experience and recall and
relational abilities) and bring a complex world into a more digestible
perspective. Science starts with question, identifies and collects data,
careful selected slices of the complex world, submits this data to
analysis through clearly defined and reproducible procedures, and
reports the results for others to evaluate. This process is repeated,
modifying, and manipulating the procedures, asking new questions and
positing new explanations, all in an effort to make inroads to bring the
complex into tangible view.

In essence what science does is attempt to subvert our inherent
limitations in understanding by drawing on carefully and purposefully
collected slices of observable experience and letting the analysis of
these observations speak, even if it goes against our intuitions (those
powerful but sometime spurious heuristics that our brains use to make
sense of the world).

\hypertarget{data-analysis}{%
\section{Data analysis}\label{data-analysis}}

At this point I've sketched an outline strengths and limitations of
humans' ability to make sense of the world and why science is used to
address these limitations. This science I've described is the one you
are familiar with and it has been an indispensable tool to make sense of
the world. If you are like me, this description of science may be
associated with visions of white coats, labs, and petri dishes. While
science's foundation still stands strong in the 21st century, a series
of intellectual and technological events mid-20th century set in motion
changes that have changed aspects about how science is done, not why it
is done. We could call this Science 2.0, but let's use the more
popularized term \index{Data Science}\textbf{Data Science}. The
recognized beginnings of Data Science are attributed to work in the
``Statistics and Data Analysis Research'' department at Bell Labs during
the 1960s. Although primarily conceptual and theoretic at the time, a
framework for quantitative data analysis took shape that would
anticipate what would come: sizable datasets which would
``{[}\ldots{]}require advanced statistical and computational techniques
{[}\ldots{]} and the software to implement them.'' (Chambers 2020) This
framework emphasized both the inference-based research of traditional
science, but also embraced exploratory research and recognized the need
to address practical considerations that would arise when working with
and deriving insight from an abundance of machine-readable data.

Fast-forward to the 21st century a world in which machine-readable data
is truly in abundance. With increased computing power and innovative
uses of this technology the world wide web took flight. To put this in
perspective, in 2019 it was estimated that every minute 511 thousand
tweets were posted, 18.1 million text messages were sent, and 188
million emails were sent ({``Data Never Sleeps 7.0 Infographic''} 2019).
The data flood has not been limited to language, there are more sensors
and recording devices than ever before which capture evermore swaths of
the world we live in (Desjardins 2019). Where increased computing power
gave rise to the influx of data, it is also one of the primary methods
for gathering, preparing, transforming, analyzing, and communicating
insight derived from this data (Donoho 2017). The vision laid out in the
1960s at Bell Labs had come to fruition.

The interest in deriving insight from the available data is now almost
ubiquitous. The science of data has now reached deep into all aspects of
life where making sense of the world is sought. Predicting whether a
loan applicant will get a loan (Bao, Lianju, and Yue 2019), whether a
lump is cancerous (Saxena and Gyanchandani 2020), what films to
recommend based on your previous viewing history (Gomez-Uribe and Hunt
2015), what players a sports team should sign (Lewis 2004) all now
incorporate a common set of data analysis tools.

These advances, however, are not predicated on data alone. As envisioned
by researchers at Bell Labs, turning data into insight it takes
\textbf{computing skills} (i.e.~programming), \textbf{statistical
knowledge}, and, importantly, \textbf{domain expertise}. This triad has
been popularly represented in a Venn diagram
Figure~\ref{fig-intro-data-science-venn}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/text-analysis/data-science-venn-paper.png}

}

\caption{\label{fig-intro-data-science-venn}Data Science Venn Diagram
adapted from
\href{http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram}{Drew
Conway}.}

\end{figure}

This same toolbelt underlies well-known public-facing language
applications. From the language-capable personal assistant applications,
plagiarism detection software, machine translation, and search engines,
tangible results of quantitative approaches to language are becoming
standard fixtures in our lives.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/text-analysis/well-known-language-applications.png}

}

\caption{\label{fig-intro-language-applications}Well-known language
applications}

\end{figure}

The spread of quantitative data analysis too has taken root in academia.
Even in areas that on first blush don't appear to be approachable in a
quantitative manner such as fields in the social sciences and
humanities, data science is making important and sometimes disciplinary
changes to the way that academic research is conducted. This textbook
focuses in on a domain that cuts across many of these fields; namely
language. At this point let's turn to quantitative approaches to
language.

\hypertarget{language-analysis}{%
\section{Language analysis}\label{language-analysis}}

Language is a defining characteristic of our species. As such, the study
of language is of key concern to a wide variety of fields, not just
linguistics. The goals of various fields, however, and as such
approaches to language research, vary. On the one hand some language
research traditions within linguistics, namely those closely associated
with Noam Chomsky, eschewed quantitative approaches to language research
during the later half of the 20th century and instead turned to
qualitative assessment of language structure through introspective
methods. On the other hand many language research programs, in and
outside linguistics, turned to and/or developed quantitative research
methods either by necessity or through theoretical principles. These
quantitative research trajectories share much of the common data
analysis toolbox described in the previous section. This means to a
large extent language analysis projects share a common research language
with other language research but also with research beyond outside of
language. However, there is never a one-size-fits all approach to
anything --much less data analysis. And in quantitative analysis there
is a key distinction in data collection that has downstream effects in
terms of procedure but also in terms of interpretation. The key
distinction, that we need to make at this point, which will provide
context for our exploration of text analysis, comes down to the approach
to collecting language data and the nature of that data. This
distinction is between \textbf{experimental
data}\index{experimental data} and \textbf{observational
data}\index{observational data}. Experimental approaches start with a
intentionally designed hypothesis and lay out a research methodology
with appropriate instruments and a plan to collect data that shows
promise for shedding light on the validity of the hypothesis.
Experimental approaches are conducted under controlled contexts, usually
a lab environment, in which participants are recruited to perform a
language related task with stimuli that have been carefully curated by
researchers to elicit some aspect of language behavior of interest.
Experimental approaches to language research are heavily influenced by
procedures adapted from psychology. This link is logical as language is
a central area of study in cognitive psychology. This approach looks a
much like the white-coat science that we made reference to earlier but,
as in most quantitative research, has now taken advantage of the data
analysis toolbelt to collect and organize much larger quantities of data
and conduct statistically more robust analysis procedures and
communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the
rationale for the study; they may either start with a testable
hypothesis or in other cases may start with a more open-ended research
question to explore. But a more fundamental distinction between the two
is drawn in the amount of control the researcher has on contexts and
conditions in which the language behavior data to be collected is
produced. Observational approaches seek out records of language behavior
that is produced by language speakers for communicative purposes in
natural(istic) contexts. This may take place in labs (language
development, language disorders, \emph{etc.}), but more often than not,
language is collected from sources where speakers are performing
language as part of their daily lives --whether that be posting on
social media, speaking on the telephone, making political speeches,
writing class essays, reporting the latest news for a newspaper, or
crafting the next novel destined to be a New York Times best-seller.
What is more, data collected from the `wild' varies more in structure
relative to data collected in experimental approaches and requires a
number of steps to prepare the data to sync up with the data analysis
toolbelt.

I liken this distinction between experimental and observational data
collection to the difference between farming and foraging. Experimental
approaches are like farming; the groundwork for a research plan is
designed, much as a field is prepared for seeding, then the researcher
performs as series of tasks to produce data, just as a farmer waters and
cares for the crops, the results of the process bear fruit, data in our
case, and this data is harvested. Observational approaches are like
foraging; the researcher scans the available environmental landscape for
viable sources of data from all the naturally existing sources, these
sources are assessed as to their usefulness and value to address the
research question, the most viable is selected, and then the data is
collected.

The data acquired from both of these approaches have their trade-offs,
just as farming and foraging. Experimental approaches directly elicit
language behavior in highly controlled conditions. This directness and
level of control has the benefit of allowing researchers to precisely
track how particular experimental conditions effect language behavior.
As these conditions are an explicit part of the design and therefore the
resulting language behavior can be more precisely attributed to the
experimental manipulation. The primary shortcoming of experimental
approaches is that there is a level of artificialness to this directness
and control. Whether it is the language materials used in the task, the
task itself, or the fact that the procedure takes place under
supervision the language behavior elicited can diverge quite
significantly from language behavior performed in natural communicative
settings. Observational approaches show complementary strengths and
shortcomings. Whereas experimental approaches may diverge from natural
language use, observational approaches strive to identify and collected
language behavior data in natural, uncontrolled, and unmonitored
contexts. In this way observational approaches do not have to question
to what extent the language behavior data is or is not performed as a
natural communicative act. On the flipside, the contexts in which
natural language communication take place are complex relative to
experimental contexts. Language collected from natural contexts are
nested within the complex workings of a complex world and as such
inevitably include a host of factors and conditions which can prove
challenging to disentangle from the language phenomenon of interest but
must be addressed in order to draw reliable associations and
conclusions.

The upshot, then, is twofold: (1) data collection methods matter for
research design and interpretation and (2) there is no single best
approach to data collection, each have their strengths and shortcomings.
In the ideal, a robust science of language will include insight from
both experimental and observational approaches (Gilquin and Gries 2009).
And evermore there is greater appreciation for the complementary nature
of experimental and observational approaches and a growing body of
research which highlights this recognition. Given their particular
trade-offs observational data is often used as an exploratory starting
point to help build insight and form predictions that can then be
submitted to experimental conditions. In this way studies based on
observational data serve as an exploratory tool to gather a better and
more externally valid view of language use which can then serve to make
prediction that can be explored with more precision in an experimental
paradigm. However, this is not always the case; observational data is
also often used in hypothesis-testing contexts as well. And furthermore,
some in some language-related fields, a hypothesis-testing is not the
ultimate goal for deriving knowledge and insight.

\hypertarget{text-analysis}{%
\section{Text analysis}\label{text-analysis}}

\textbf{Text analysis} is the application of data analysis procedures
from data science to derive insight from textual data collected through
observational methods. I have deliberately chosen the term `text
analysis' to avoid what I see are the pitfalls of using some other
common terms in the literature such as Corpus Linguistics, Computational
Linguistics, or Digital Humanities. There are plenty of learning
resources that focus specifically on one of these three fields when
discussing the quantitative analysis of text. But from my perspective
what is missing is a resource which underscores the fact that text
analysis research and the methods employed span across a wide variety of
academic fields and applications in industry. This textbook aims to
introduce you to these areas through the lens of the data and analysis
procedures and not through a particular field. This approach, I hope,
provides a wider view of the potential applications of using text as
data and inspires you to either employ quantitative text analysis in
your research and/ or to raise your awareness of the advantages of text
analysis for making sense of language-related and linguistic-based
phenomenon.

So what are some applications of text analysis? The most public facing
applications stem from Computational Linguistic research, often known as
Natural Language Processing (NLP) by practitioners. Whether it be using
search engines, online translators, submitting your paper to plagiarism
detection software, \emph{etc.} the text analysis methods we will cover
are at play. These uses of text analysis are production-level
applications and there is big money behind developing evermore robust
text analysis methods.

In academia the use of quantitative text analysis is even more
widespread, despite the lack of public fanfare. Let's run through some
select studies to give you an idea of some areas that employ text
analysis, to highlight a range of topics researchers address with text
analysis, and to whet your interest for conducting your own text
analysis project.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Eisenstein et al. (2012) track the geographic spread of neologisms from
city to city in the United States using Twitter data collected between
6/2009 and 5/2011. They only used tweets with geolocation data and then
associated each tweet with a zip code using the US Census. The most
populous metropolitan areas were used. They also used the demographics
from these areas to make associations between lexical innovations and
demographic attributes. From this analysis they are able to reconstruct
a network of linguistic influence. One of the main findings is that
demographically-similar cities are more likely to share linguistic
influence. At the individual level, there is a strong, potentially
stronger role of demographics than geographical location.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Voigt et al. (2017) explore potential racial disparities in officer
respect in police body camera footage. The dataset is based on body
camera footage from the Oakland Police Department during April 2014. At
total of 981 stops by 245 different officers were included (black 682,
white 299) and resulted in 36,738 officer utterances. The authors found
evidence for racial disparities in respect but not formality of
utterances, with less respectful language used with the black community
members.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Conway et al. (2012) investigate whether the established drop in
language complexity of rhetoric in election seasons is associated with
election outcomes. The authors used US Democratic Primary Debates from
2004. The results suggest that although there was no overall difference
in complexity between winners and losers, their pattern differed over
time. Winners tended to drop the complexity of their language closer to
the upcoming election.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Kloumann et al. (2012) explore the extent to which languages are
positively, neutrally, or negatively biased. Using Twitter, Google Books
(1520-2008), NY Times newspaper (1987-2007), and music lyrics
(1960-2007) the authors extract the top 5,000 most frequent words from
each source and have participants rate each word for happiness (9-point
scale). The results show that positive words strongly outnumber negative
words overall suggesting English is positive-, and pro-social- biased.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Bychkovska and Lee (2017) investigates possible differences between
L1-English and L1-Chinese undergraduate students' use of lexical
bundles, multiword sequences which are extended collocations (i.e.~as
the result of), in argumentative essays. The authors used the Michigan
Corpus of Upper-Level Student Papers (MICUSP) corpus using the
argumentative essay section for L1-English and the Corpus of Ohio
Learner and Teacher English (COLTE) for the L1-Chinese English essays.
They found that L1-Chinese writers used more than 2 times as many bundle
types than L1-English peers which they attribute to L1-Chinese writers
attempt to avoid uncommon expressions and/or due to their lack of
register awareness (conversation has more bundles than writing
generally).
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Jaeger and Snider (2007) use a corpus study to investigate the
phenomenon of syntactic persistence, the increased tendency for speakers
to use a particular syntactic form over an alternate when the syntactic
form is recently processed. The authors attempt to distinguish between
two competing explanations for the phenomenon: (1) transient activation,
where the increased tendency is short-lived and time-bound and (2)
implicit learning, where the increased tendency is a reflect of learning
mechanisms. The use of a speech corpora (Switchboard and spoken BNC)
were used to avoid the artificialness that typically occurs in
experimental settings. The authors investigated the ditransitive
alternation (NP PP/ NP NP), voice alternation (active/ passive), and
complementizer/ relativizer omission. In these alternations structural
bias was established by measuring the probability for a verb form to
appear in one of the two syntactic forms. Then the probability that that
form (target) would change given previous exposure to the alternative
form (prime) was calculated; what the authors called surprisal. Distance
between the prime structure and the target verb were considered in the
analysis. In these alternations, the less common structure was used in
the target more often when the when it corresponded to the prime form
(higher surprisal) suggesting that implicit learning underlies syntactic
persistence effects.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Wulff, Stefanowitsch, and Gries (2007) explore differences between
British and American English at the lexico-syntactic level in the
\emph{into}-causative construction (ex. `He tricked me into employing
him.'). The analysis uses newspaper text (The Guardian and LA Times) and
the findings suggest that American English uses this construction in
verbal persuasion verbs whereas British English uses physical force
verbs.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Mosteller and Wallace (1963) provide a method for solving the authorship
debate surrounding The Federalist papers \footnote{https://guides.loc.gov/federalist-papers/full-text}.
They employ a probabilistic approach using the word frequency profiles
of the articles with known authors to predict authorship of the disputed
12 papers. The results suggest that the disputed papers were most likely
authored by Madison.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-caution-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Case study}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm]
Olohan (2008) investigate the extent to which translated texts differ
from native texts. In particular the author explores the notion of
explicitation in translated texts (the tendency to make information in
the source text explicit in the target translation). The study makes use
of the Translational English Corpus (TEC) for translation samples and
comparable sections of the British National Corpus (BNC) for the native
samples. The results suggest that there is a tendency for syntactic
explicitation in the translational corpus (TEC) which is assumed to be a
subconscious process employed unwittingly by translators.
\end{tcolorbox}

This sample of studies include research from areas such as translation,
stylistics, language variation, dialectology, psychology,
psycholinguistics, political science, and sociolinguistics which
highlights the diversity of fields and subareas which employ
quantitative text analysis. Text analysis is at the center of these
studies as they share a set of common goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To detect and retrieve patterns from text too subtle or too numerous
  to be done by hand
\item
  To challenge assumptions and/ or provide other views from textual
  sources
\item
  To explore new questions and/ or provide novel insight
\end{enumerate}

\hypertarget{structure}{%
\section{Structure}\label{structure}}

Let's now turn to the last section of this chapter which will provide an
overview of the rationale for learning to do text analysis, the
structure of the content covered, and a justification for the approach
we will take to perform text analysis.

In this section I will provide a general overview of the rest of the
textbook motivating the general structure and sequencing as well as
setting the foundation for programmatic approaches to data analysis. Let
me highlight why I think this is a valuable area of study, what I hope
you gain from this textbook, and how the structure of this textbook is
configured to help scaffold your conceptual and practical knowledge of
text analysis.

In \textbf{Part I ``Foundations''} the aims are to: 1) provide an
overview of quantitative research and their applications, by both
highlighting visible applications and notable research in various
fields, 2) consider how quantitative research contributes to language
research, and 3) layout the main types of research and situate
quantitative text analysis inside these.

In \textbf{Part II ``Orientation''} we will build up a framework to
contextualize quantitative data analysis using the Data to Insight
(DIKI) Hierarchy in Figure~\ref{fig-diki-hierarchy} \footnote{Adapted
  from Ackoff (1989)}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/text-analysis/diki-hierarchy-paper.png}

}

\caption{\label{fig-diki-hierarchy}Data to Insight Hierarchy (DIKI)}

\end{figure}

The DIKI Hierarchy highlights the stages and intermediate steps required
to derive insight from data.
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''} will cover both Data and Information covering the conceptual
topics of populations versus samples and how language data samples are
converted to information and the forms that they can take. In
\protect\hyperlink{sec-approaching-analysis}{Chapter 3 ``Approaching
analysis''} I will discuss the distinction between descriptive and
analytic statistics. In brief they are both important for conducting
data analysis, but descriptive statistics serve as a sanity check on the
dataset before submitting it to interrogation --which is the goal of
analytic statistics. We will also cover some of the main distinctions
between analytics approaches including inference-, exploration-, and
prediction-based methods. With a fundamental understanding of data,
information, and knowledge we will then move to
\protect\hyperlink{framing-research-chapter}{Chapter 4 ``Framing
research''} where we will discuss how to develop a research plan, or
what I will call a `research blueprint'. At this point we will directly
address Research Skills and elaborate on how research really comes
together; how to bring yourself up to speed with the literature on a
topic, how to develop a research goal or hypothesis, how to select data
which is viable to address the research goal or hypothesis, how to
determine the necessary information and appropriate measures to prepare
for analysis, how to perform diagnostic statistics on the data and make
adjustments before analysis, how to select and perform the relevant
analytic statistics given the research goals, how to report your
findings, and finally, how to structure your project so that it is
well-documented and reproducible.

\textbf{Part III ``Preparation''} and \textbf{Part IV ``Analysis''}
serve as practical and more detailed guides to the R programming
strategies to conduct text analysis research and as such develop your
Programming Skills. In \protect\hyperlink{acquire-data-chapter}{Chapter
5 ``Acquire data''} I will discuss three main strategies for accessing
data: direct downloads, Automatic Programming Interfaces (APIs), and web
scraping. In \protect\hyperlink{curate-datasets-chapter}{Chapter 6
``Curate data(sets)''} I will outline the process for converting or
augmenting the acquired data or dataset into a (more) structured format,
therefore creating information. This will include organizing linguistic
and non-linguistic metadata into one dataset. In
\protect\hyperlink{transform-datasets-chapter}{Chapter 7 ``Transform
datasets''} I describe how to work with a curated dataset to derive more
detailed information and appropriate dataset structures that are
appropriate for the subsequent analysis.

\protect\hyperlink{inference-chapter}{Chapters 8 ``Inference''},
\protect\hyperlink{prediction-chapter}{9 ``Prediction''}, and
\protect\hyperlink{exploration-chapter}{10 ``Exploration''} focus on
different categories of statistical analysis each associated with
distinct research goals. Inference deals with analysis methods
associated with standard hypothesis-testing. This will include some
common statistical models employed in text analysis: chi-squared,
logistic regression, and linear regression. Prediction covers methods
for modeling associations in data with the aim to accurately predict
outcomes using new textual data. I will cover some standard methods for
text classification including Nive Bayes, \emph{k}-nearest neighbors
(\emph{k}-NN), and decisions tree and random forest models. Exploration
covers a variety of analysis methods such as association measures,
clustering, topic modeling, and vector-space models. These methods are
aligned with research goals that aim to interpret patterns that arise in
from the data itself.

\textbf{Part V ``Communication''} covers the steps in presenting the
findings of the research both as a research document and as a
reproducible research project. Both research documents and reproducible
projects are fundamental components of modern scientific inquiry. On the
one hand a research document, covered in Chapter
\protect\hyperlink{reporting-chapter}{11 ``Reporting''}, provides
readers a detailed summary of the main import of the research study. On
the other hand making the research project available to interested
readers, covered in Chapter \protect\hyperlink{collaboration-chapter}{12
``Collaboration''}, ensures that the scientific community can gain
insight into the process implemented in the research and thus enables
researchers to vet and extend this research to build a more robust and
verifiable research base.

\hypertarget{summary-1}{%
\section*{Summary}\label{summary-1}}
\addcontentsline{toc}{section}{Summary}

In this chapter I started with some general observations about the
difficulty of making sense of a complex world. The standard approach to
overcoming inherent human limitations in sense making is science. In the
21st century the toolbelt for doing scientific research and exploration
has grown in terms of the amount of data available, the statistical
methods for analyzing the data, and the computational power to manage,
store, and share the data, methods, and results from quantitative
research. The methods and tools for deriving insight from data have made
significant inroads in and outside academia, and increasingly figure in
the quantitative investigation of language. Text analysis is a
particular branch of this enterprise based on observational data from
real-world language and is used in a wide variety of fields.

This textbook aims to develop your knowledge and skills in three
fundamental areas: Data Literacy, Research Skills, and Programming
Skills. (\ldots{} add more on how the structure leads to developing this
knowledge and skills\ldots)

In the end I hope that you enjoy this exploration into text analysis.
Although the learning curve at times may seem steep --the experience you
will gain will not only improve your data literacy, research skills, and
programmings skills but also enhance your appreciation for the richness
of human language and its important role in our everyday lives.

\hypertarget{actitivies}{%
\section*{Actitivies}\label{actitivies}}
\addcontentsline{toc}{section}{Actitivies}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_2.html}{Literate
programming II}\\
\textbf{How}: Read Recipe 2 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore additional functionality in R Markdown:
numbered sections, table of contents, in-line citations and a
document-final references list, and cross-referenced tables and figures.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_2}{Literate
programming II}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 2.\\
\textbf{Why}: To put into practice R Markdown functionality to
communicate the aim(s) and main finding(s) from a primary research
article and to interpret a related plot.
\end{tcolorbox}

\part{Orientation}

Before working on the specifics of a data project, it is important to
establish a fundamental understanding of the characteristics of each of
the levels in the ``Data, Information, Knowledge, and Insight Hierarchy
(DIKI)'' (see Figure~\ref{fig-diki-hierarchy}) and the roles each of
these levels have in deriving insight from data. In
\protect\hyperlink{understanding-data-chapter}{Chapter 2} we will
explore the Data and Information levels drawing a distinction between
two main types of data (populations and samples) and then cover how data
is structured and transformed to generate information (datasets) that is
fit for statistical analysis. In
\protect\hyperlink{approaching-analysis-chapter}{Chapter 3} I will
outline the importance and distinct types of statistical procedures
(descriptive and analytic) that are commonly used in text analysis.
\protect\hyperlink{framing-research-chapter}{Chapter 4} aims to tie
these concepts together and cover the required steps for preparing a
research blueprint to conduct an original text analysis project.

\hypertarget{sec-understanding-data}{%
\chapter{Understanding data}\label{sec-understanding-data}}

\begin{quote}
The plural of anecdote is not data.

--- Marc Bekoff
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Essential questions}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What are the distinct types of data and how do they differ?
\item
  What is information and what form does it take?
\item
  What is the importance of documentation in reproducible research?
\end{itemize}

\end{tcolorbox}

In this chapter I cover the starting concepts in our journey to
understand how to derive insight from data, illustrated in the DIKI
Hierarchy (Figure~\ref{fig-diki-hierarchy}), focusing specifically on
the first two levels: Data and Information. We will see that what is
commonly referred to as `data' everyday uses is broken into three
distinct categories, two of which are referred to as data and the third
is known as information. We will also cover the importance of
documentation of data and datasets in quantitative research.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interactive programming}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Objects, Packages
and functions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To introduce you to the main types of objects in R and to
understand the role and use of functions and packages in R programming.
\end{tcolorbox}

\hypertarget{data}{%
\section{Data}\label{data}}

Data is data, right? The term `data' is so common in popular vernacular
it is easy to assume we know what we mean when we say `data'. But as in
most things, where there are common assumptions there are important
details the require more careful consideration. Let's turn to the first
key distinction that we need to make to start to break down the term
`data': the difference between populations and samples.

\hypertarget{populations}{%
\subsection{Populations}\label{populations}}

The first thing that comes to many people's mind when the term
population is used is human populations. Say for example --What's the
population of Milwuakee? When we speak of a population in these terms we
are talking about the total sum of people living within the geographical
boundaries of Milwaukee. In concrete terms, a
\index{population}\textbf{population} is the objective make up of an
idealized set of objects and events in reality. Key terms here are
objective and idealized. Although we can look up the US Census report
for Milwaukee and retrieve a figure for the population, this cannot
truly be the population. Why is that? Well, whatever method that was
used to derive this numerical figure was surely incomplete. If not
incomplete, by the time someone recorded the figure some number of
residents of Milwaukee moved out, moved in, were born, or passed away
--the figure is no longer the true population.

Likewise when we talk about populations in terms of language we dealing
with an objective and idealized aspect of reality. Let's take the words
of the English language as an analog to our previous example population.
In this case the words are the people and English is the bounding
characteristic. Just as people, words move out, move in, are born, and
pass away. Any compendium of the words of English at any moment is
almost instananeously incomplete. This is true for all populations, save
those in which the bounding characteristics select a narrow slice of
reality which is objectively measurable and whose membership is fixed
(the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We objectively
hold them to exist, but in practical terms we often cannot nail down the
specifics of populations. So how do researchers go about studying
populations if they are theoretically impossible to access directly? The
strategy employed is called sampling.

\hypertarget{sampling}{%
\subsection{Sampling}\label{sampling}}

A \index{sample}\textbf{sample} is the product of a subjective process
of selecting a finite set of observations from an objective population
with the goal of capturing the relevant characteristics of the target
population. Although there are strategies to minimize the mismatch
between the characteristics of the subjective sample and objective
population, it is important to note that it is almost certainly true
that any given sample diverges from the population it aims to represent
to some degree. The aim, however, is to employ a series of sampling
decisions, which are collectively known as a sampling frame, that
maximize the chance of representing the population.

What are the most common sampling strategies? First
\index{sample size}\textbf{sample size}. A larger sample will always be
more representative than a smaller sample. Sample size, however, is not
enough. It is not hard to imagine a large sample which by chance
captures only a subset of the features of the population. A next step to
enhance sample representativeness is apply \textbf{random sampling}.
Together a large random sample has an even better chance of reflecting
the main characteristics of the population better than a large or random
sample. But, random as random is, we still run the risk of acquiring a
skewed sample (i.e a sample which does not mirror the target
population).

To help mitigate these issues, there are two more strategies that can be
applied to improve sample representativeness. Note, however, that while
size and random samples can be applied to any sample with little
information about internal characteristics of the population, these next
two strategies require decisions depend on the presumed internal
characteristics of the population. The first of these more informed
sampling strategies is called \textbf{stratified sampling}. Stratified
samples make (educated) assumptions about sub-components within the
population of interest. With these sub-populations in mind, large random
samples are acquired for each sub-population, or strata. At a minimum,
stratified samples can be no less representative than random sampling
alone, but the chances that the sample is better increases. Can there be
problems in the approach? Yes, and on two fronts. First knowledge of the
internal components of a population are often based on a limited or
incomplete knowledge of the population. In other words, strata are
selected subjectively by researchers using various heuristics some of
which are based on some sense of `common knowledge'. The second front on
which stratified sampling can err concerns the relative sizes of the
sub-components relative to the whole population. Even if the relevant
sub-components are identified, their relative size adds another
challenge which researchers must address in order to maximize the
representativeness of a sample. To attempt to align, or
\textbf{balance}, the relative sizes of the samples for the strata is
the second population-informed sampling strategy.

A key feature of a sample is that it is purposely selected. Samples are
not simply a collection or set of data from the population. Samples are
rigorously selected with an explicit target population in mind. In text
analysis a purposely sampled collection of texts, of the type defined
here, is known as a \textbf{corpus.} For this same reason a set of texts
or documents which have not been selected along a purposely selected
sampling frame is not a corpus. The sampling frame, and therefore the
populations modeled, in any given corpus most likely will vary and for
this reason it is not a safe assumption that any given corpus is equally
applicable for any and every research question. Corpus development
(\emph{i.e.} sampling) is purposeful, and the characteristics of the
corpus development process should be made explicit through
documentation. Therefore vetting a corpus sample for its applicability
to a research goal is a key step in that a research must take to ensure
the integrity of the research findings.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-important-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm]
The Brown Corpus is widely recognized as one of the first large,
machine-readable corpora. It was compiled by Kucera and Francis (1967).
Consult the
\href{http://korpus.uib.no/icame/brown/bcm.html}{documentation for this
corpus}. Can you determine what language population this corpus aims to
represent? Given the sampling frame for this corpus (in the
documentation and summarized in Figure~\ref{fig-brown-distribution}),
what types of research might this corpus support or not support?
\end{tcolorbox}

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./understanding-data_files/figure-pdf/fig-brown-distribution-1.pdf}

}

\caption{\label{fig-brown-distribution}Brown Corpus of Written American
English}

\end{figure}

\hypertarget{corpora}{%
\subsection{Corpora}\label{corpora}}

\hypertarget{types}{%
\subsubsection{Types}\label{types}}

With the notion of sampling frames in mind, some corpora are compiled
with the aim to be of general purpose (general or \textbf{reference
corpora}), and some with much more specialized sampling frames
(\textbf{specialized corpora}). For example, the
\href{https://www.anc.org/}{American National Corpus (ANC)} or the
\href{http://www.natcorp.ox.ac.uk/}{British National Corpus (BNC)} are
corpora which aim to model (represent/ reflect) the general
characteristics of the English language, the former of American English
and the later British English. These are ambitious projects, and require
significant investments of time in corpus design and then in
implementation (and continued development) that are usually undertaken
by research teams (del 2020).

Specialized corpora aim to represent more specific populations. The
\href{https://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa
Barbara Corpus of Spoken American English (SBCSAE)}, as you can imagine
from the name of the resource, aims to model spoken American English. No
claim to written English is included. There are even more specific types
of corpora which attempt to model other types of sub-populations such as
scientific writing,
\href{https://www.clarin.eu/resource-families/cmc-corpora}{computer-mediated
communication (CMC)}, language use in specific
\href{http://ice-corpora.net/ice/index.html}{regions of the world}, a
\href{https://cesa.arizona.edu}{country}, or a
\href{https://cesa.arizona.edu}{region}, \emph{etc}.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-important-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm]
Grieve, Nini, and Guo (2018) compiled a 8.9 billion-word corpus of
geotagged posts from Twitter between 2013-2014 in the United States. The
authors provide a \href{https://isogloss.shinyapps.io/isogloss/}{search
interface} to explore relationship between lexical usage and geographic
location. Explore this corpus searching for terms related to slang
(``hella'', ``wicked''), geographical (``mountain'', ``river''),
meteorological (``snow'', ``rain''), and/ or any other term types. What
types of patterns do you find? What are the benefits and/ or limitations
of this type of data and/ or interface?
\end{tcolorbox}

Another set of specialized corpora are resources which aim to compile
texts from different languages or different language varieties for
direct or indirect comparison. Corpora that are directly comparable,
that is they include source and translated texts, are called
\textbf{parallel corpora}. Parallel corpora include different languages
or language varieties that are indexed and aligned at some linguistic
level (\emph{i.e.} word, phrase, sentence, paragraph, or document), see
\href{https://opus.nlpl.eu/}{OPUS}. Corpora that are compiled with
different languages or language varieties but are not directly aligned
are called \textbf{comparable corpora}. The comparable language or
language varieties are sampled with the same or similar sampling frame,
for example
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0402}{Brown}
and
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0167}{LOB}
corpora.

The aim of the quantitative text researcher is to select the corpus or
corpora (plural of corpus) which best aligns with the purpose of the
research. Therefore a general corpus such as the ANC may be better
suited to address a question dealing with the way American English
works, but this general resource may lack detail in certain areas, such
as \href{http://www.hd.uib.no/icame/ij22/vihla.pdf}{medical language},
that may be vital for a research project aimed at understanding changes
in medical terminology.

\hypertarget{sources}{%
\subsubsection{Sources}\label{sources}}

The most common source of data used in contemporary quantitative
research is the internet. On the web an investigator can access corpora
published for research purposes and language used in natural settings
that can be coerced by the investigator into a corpus. Many
organizations exist around the globe that provide access to corpora in
browsable catalogs, or \textbf{repositories}. There are repositories
dedicated to language research, in general, such as the
\href{https://www.ldc.upenn.edu/}{Language Data Consortium} or for
specific language domains, such as the language acquisition repository
\href{http://talkbank.org/}{TalkBank}. It is always advisable to start
looking for the available language data in a repository. The advantage
of beginning your data search in repositories is that a repository,
especially those geared towards the linguistic community, will make
identifying language corpora faster than through a general web search.
Furthermore, repositories often require certain standards for corpus
format and documentation for publication. A standardized resource many
times will be easier to interpret and evaluate for its appropriateness
for a particular research project.

In the table below I've compiled a list of some corpus repositories to
help you get started.

\begin{table}

\caption{A list of some corpus repositories}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://corpus.byu.edu/">BYU corpora</a> & A repository of corpora that includes billions of words of data.\\
\hline
<a href="http://corporafromtheweb.org/">COW (COrpora from the Web)</a> & A collection of linguistically processed gigatoken web corpora\\
\hline
<a href="http://wortschatz.uni-leipzig.de/en/download/">Leipzig Corpora Collection</a> & Corpora in different languages using the same format and comparable sources.\\
\hline
<a href="https://www.ldc.upenn.edu/">Linguistic Data Consortium</a> & Repository of language corpora\\
\hline
<a href="http://www.resourcebook.eu/searchll.php">LRE Map</a> & Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).\\
\hline
<a href="http://www.nltk.org/nltk\_data/">NLTK language data</a> & Repository of corpora and language datasets included with the Python package NLTK.\\
\hline
<a href="http://opus.lingfil.uu.se/">OPUS - an open source parallel corpus</a> & Repository of translated texts from the web.\\
\hline
<a href="http://talkbank.org/">TalkBank</a> & Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.\\
\hline
<a href="https://corpus1.mpi.nl/ds/asv/?4">The Language Archive</a> & Various corpora and language datasets\\
\hline
<a href="http://ota.ox.ac.uk/">The Oxford Text Archive (OTA)</a> & A collection of thousands of texts in more than 25 different languages.\\
\hline
\end{tabular}
\end{table}

Repositories are by no means the only source of corpora on the web.
Researchers from around the world provide access to corpora and other
data sources on their own sites or through data sharing platforms.
Corpora of various sizes and scopes will often be accessible on a
dedicated homepage or appear on the homepage of a sponsoring
institution. Finding these resources is a matter of doing a web search
with the word `corpus' and a list of desired attributes, including
language, modality, register, \emph{etc}. As part of a general movement
towards reproducibility more corpora are available on the web than ever
before. Therefore data sharing platforms supporting reproducible
research, such as \href{https://github.com/}{GitHub},
\href{https://zenodo.org/}{Zenodo},
\href{http://www.re3data.org/}{Re3data}, \href{https://osf.io/}{OSF},
\emph{etc}., are a good place to look as well, if searching repositories
and targeted web searches do not yield results.

In the table below you will find a list of corpus resources and
datasets.

\begin{table}

\caption{Corpora and language datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.socsci.uci.edu/\textasciitilde{}lpearl/CoLaLab/CHILDESTreebank/childestreebank.html">CHILDES Treebank</a> & A corpus derived from several corpora from the American English section of CHILDES with the goal to annotate child-directed speech utterance transcriptions with phrase structure tree information.\\
\hline
<a href="http://www.cs.cornell.edu/\textasciitilde{}cristian/Cornell\_Movie-Dialogs\_Corpus.html">Cornell Movie-Dialogs Corpus</a> & A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.\\
\hline
<a href="http://www.lllf.uam.es/\textasciitilde{}fmarcos/informes/corpus/coarginl.html">Corpus Argentino</a> & Corpus of Argentine Spanish\\
\hline
<a href="https://cesa.arizona.edu/">Corpus of Spanish in Southern Arizona</a> & Spanish varieties spoken in Arizona.\\
\hline
<a href="https://www.statmt.org/europarl/">Europarl Parallel Corpus</a> & A parallel corpus extracted from the proceedings of the European Parliament Proceedings between 1996-2011.\\
\hline
<a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">Google Ngram Viewer</a> & Google web corpus\\
\hline
<a href="http://ice-corpora.net/ice/">International Corpus of English (ICE)</a> & The International Corpus of English (ICE) began in 1990 with the primary aim of collecting material for comparative studies of English worldwide.\\
\hline
<a href="http://opus.lingfil.uu.se/OpenSubtitles\_v2.php">OpenSubtitles2011</a> & A collection of documents from http://www.opensubtitles.org/.\\
\hline
<a href="http://www.ruscorpora.ru/en/">Russian National Corpus</a> & A corpus of modern Russian language incorporating over 300 million words.\\
\hline
<a href="https://quantumstat.com//dataset">The Big Bad NLP Database - Quantum Stat</a> & NLP datasets\\
\hline
<a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">The Switchboard Dialog Act Corpus</a> & A corpus of 1155 5-minute conversations in American English, comprising 205,000 utterances and 1.4 million words, from the Switchboard corpus of telephone conversations.\\
\hline
<a href="http://langsnap.soton.ac.uk/">Welcome to LANGSNAP - LANGSNAP</a> & The aim of this repository is to promote research on the learning of French and Spanish as L2, by making parallel learner corpora for each language freely available to the research community.\\
\hline
<a href="http://www.psych.ualberta.ca/\textasciitilde{}westburylab/downloads/usenetcorpus.download.html">Westbury Lab Web Site: Usenet Corpus Download</a> & This corpus is a collection of public USENET postings. This corpus was collected between Oct 2005 and Jan 2011, and covers 47,860 English language, non-binary-file news groups (see list of newsgroups included with the corpus for details)\\
\hline
\end{tabular}
\end{table}

Language corpora prepared by researchers and research groups listed on
repositories or hosted by the researchers themselves is often the first
place to look for data. The web, however, contains a wealth of language
and language-related data that can be accessed by researcher to compile
their own corpus. There are two primary ways to attain language data
from the web. The first is through the process of web scraping.
\textbf{Web scraping} is the process of harvesting data from the web
either manually or (semi-)automatically from the actual public-facing
web. The second way to acquire data from the web is through an
\textbf{Application Programming Interface} (API). APIs are, as the title
suggests, programming interfaces which allow access, under certain
conditions, to information that a website or database accessible via the
web contains.

The table below lists some R packages that serve to interface language
data directly through R.

\begin{table}

\caption{R Package interfaces to language corpora and datasets.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://ropensci.org/tutorials/arxiv\_tutorial.html">aRxiv</a> & R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.\\
\hline
<a href="https://github.com/ropensci/crminer">crminer</a> & R package interface focusing on getting the user full text via the Crossref search API.\\
\hline
<a href="https://github.com/ropensci/dvn">dvn</a> & R package interface to access to the Dataverse Network APIs.\\
\hline
<a href="https://ropensci.org/tutorials/fulltext\_tutorial.html">fulltext</a> & R package interface to query open access journals, such as PLOS.\\
\hline
<a href="https://ropensci.org/tutorials/gutenbergr\_tutorial.html">gutenbergr</a> & R package interface to download and process public domain works from the Project Gutenberg collection.\\
\hline
<a href="https://ropensci.org/tutorials/internetarchive\_tutorial.html">internetarchive</a> & R package interface to query the Internet Archive.\\
\hline
<a href="https://github.com/hrbrmstr/newsflash">newsflash</a> & R package interface to query the Internet Archive and GDELT Television Explorer\\
\hline
<a href="https://github.com/ropensci/oai">oai</a> & R package interface to query any OAI-PMH repository, including Zenodo.\\
\hline
<a href="https://github.com/ropensci/rfigshare">rfigshare</a> & R package interface to query the data sharing platform FigShare.\\
\hline
<a href="https://github.com/ropensci/rtweet">rtweet</a> & R client for interacting with Twitter's APIs\\
\hline
\end{tabular}
\end{table}

Data for language research is not limited to (primary) text sources.
Other sources may include processed data from previous research; word
lists, linguistic features, \emph{etc}.. Alone or in combination with
text sources this data can be a rich and viable source of data for a
research project.

Below I've included some processed language resources.

\begin{table}

\caption{Language data from previous research and meta-studies.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="http://elexicon.wustl.edu/WordStart.asp">English Lexicon Project</a> & Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.\\
\hline
<a href="https://github.com/ropensci/lingtypology">lingtypology</a> & R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.\\
\hline
<a href="https://nyu-mll.github.io/CoLA/">The Corpus of Linguistic Acceptability (CoLA)</a> & A corpus that consists of 10657 sentences from 23 linguistics publications, expertly annotated for acceptability (grammaticality) by their original authors.\\
\hline
<a href="http://icon.shef.ac.uk/Moby/">The Moby lexicon project</a> & Language wordlists and resources from the Moby project.\\
\hline
\end{tabular}
\end{table}

The list of data available for language research is constantly growing.
I've document very few of the wide variety of resources. Below I've
included attempts by others to provide a summary of the corpus data and
language resources available.

\begin{table}

\caption{Lists of corpus resources.}
\centering
\begin{tabular}[t]{l|l}
\hline
Resource & Description\\
\hline
<a href="https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html">Learner corpora around the world</a> & A listing of learner corpora around the world\\
\hline
<a href="https://paperswithcode.com/datasets">Machine Learning Datasets | Papers With Code</a> & A free and open resource with Machine Learning papers, code, and evaluation tables.\\
\hline
<a href="http://nlp.stanford.edu/links/statnlp.html\#Corpora">Stanford NLP corpora</a> & Listing of corpora and language resources aimed at the NLP community.\\
\hline
<a href="https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/">Where can you find language data on the web?</a> & Listing of various corpora and language datasets.\\
\hline
\end{tabular}
\end{table}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-important-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm]
Explore some of the resources listed above and consider their sampling
frames. Can you think of a research question or questions that this
resource may be well-suited to support research into? What types of
questions would be less-than-adequate for a given resource?
\end{tcolorbox}

\hypertarget{formats}{%
\subsubsection{Formats}\label{formats}}

A corpus will often include various types of non-linguistic attributes,
or \index{meta-data}\textbf{meta-data}, as well. Ideally this will
include information regarding the source(s) of the data, dates when it
was acquired or published, and other author or speaker information. It
may also include any number of other attributes that were identified as
potentially important in order to appropriately document the target
population. Again, it is key to match the available meta-data with the
goals of your research. In some cases a corpus may be ideal in some
aspects but not contain all the key information to address your research
question. This may mean you will need to compile your own corpus if
there are fundamental attributes missing. Before you consider compiling
your own corpus, however, it is worth investigating the possibility of
augmenting an available corpus to bring it inline with your particular
goals. This may include adding new language sources, harnessing software
for linguistic annotation (part-of-speech, syntactic structure, named
entities, \emph{etc}.), or linking available corpus meta-data to other
resources, linguistic or non-linguistic.

Corpora come in various formats, the main three being: running text,
structured documents, and databases. The format of a corpus is often
influenced by characteristics of the data but may also reflect an
author's individual preferences as well. It is typical for corpora with
few meta-data characteristics to take the form of running text.

Running text sample from the
\href{https://www.statmt.org/europarl/}{Europarle Parallel Corpus}.

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on Friday
17 December 1999, and I would like once again to wish you a happy new year in
the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed to
materialise, still the people in a number of countries suffered a series of
natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next few
days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a number of
Members have requested, on behalf of all the victims concerned, particularly
those of the terrible storms, in the various countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been a number
of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr Kumar
Ponnambalam, who had visited the European Parliament just a few months ago.
\end{verbatim}

In corpora with more meta-data, a header may be appended to the top of
each running text document or the meta-data may be contained in a
separate file with appropriate coding to coordinate meta-data attributes
with each text in the corpus.

Meta-data header sample from the \href{}{Switchboard Dialog Act Corpus}.

\begin{verbatim}
> FILENAME: 4325_1632_1519
> TOPIC#: 323
> DATE: 920323
> TRANSCRIBER: glp
> UTT_CODER: tc
> DIFFICULTY: 1
> TOPICALITY: 3
> NATURALNESS: 2
> ECHO_FROM_B: 1
> ECHO_FROM_A: 4
> STATIC_ON_A: 1
> STATIC_ON_B: 1
> BACKGROUND_A: 1
> BACKGROUND_B: 2
> REMARKS: None.
>
> =========================================================================
>
>
> o A.1 utt1: Okay.  /
> qw A.1 utt2: {D So, }
>
> qy^d B.2 utt1: [ [ I guess, +
>
> + A.3 utt1: What kind of experience [ do you, + do you ] have, then with
child care? /
>
> + B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
>
> qy A.5 utt1: Does it say something? /
>
> sd B.6 utt1: I think it usually does.  /
> ad B.6 utt2: You might try, {F uh, } /
> h B.6 utt3: I don't know, /
> ad B.6 utt4: hold it down a little longer, /
> ad B.6 utt5: {C and } see if it, {F uh, } -/
\end{verbatim}

When meta-data and/ or linguistic annotation increases in complexity it
is common to structure each corpus document more explicitly with a
markup language such as XML (Extensible Markup Language) or organize
relationships between language and meta-data attributes in a database.

XML format for meta-data (and linguistic annotation) from the
\href{http://www.nltk.org/nltk_data/}{Brown Corpus}.

\begin{verbatim}
> <TEI
xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>Sample
A01 from The Atlanta Constitution</title><title type="sub"> November 4, 1961,
p.1 "Atlanta Primary ..."
> "Hartsfield Files"
> August 17, 1961, "Urged strongly ..."
> "Sam Caldwell Joins"
> March 6,1961, p.1 "Legislators Are Moving" by Reg Murphy
> "Legislator to fight" by Richard Ashworth
> "House Due Bid..."
> p.18 "Harry Miller Wins..."
> </title></titleStmt><editionStmt><edition>A part of the XML version of the
Brown Corpus</edition></editionStmt><extent>1,988 words 431 (21.7%) quotes 2
symbols</extent><publicationStmt><idno>A01</idno><availability><p>Used by
permission of The Atlanta ConstitutionState News Service (H), and Reg Murphy
(E).</p></availability></publicationStmt><sourceDesc><bibl> The Atlanta
Constitution</bibl></sourceDesc></fileDesc><encodingDesc><p>Arbitrary Hyphen:
multi-million [0520]</p></encodingDesc><revisionDesc><change
when="2008-04-27">Header auto-generated for TEI
version</change></revisionDesc></teiHeader>
> <text xml:id="A01" decls="A">
> <body><p><s n="1"><w type="AT">The</w> <w type="NP" subtype="TL">Fulton</w>
<w type="NN" subtype="TL">County</w> <w type="JJ" subtype="TL">Grand</w> <w
type="NN" subtype="TL">Jury</w> <w type="VBD">said</w> <w type="NR">Friday</w>
<w type="AT">an</w> <w type="NN">investigation</w> <w type="IN">of</w> <w
type="NPg">Atlanta's</w> <w type="JJ">recent</w> <w type="NN">primary</w> <w
type="NN">election</w> <w type="VBD">produced</w> <c type="pct">``</c> <w
type="AT">no</w> <w type="NN">evidence</w> <c type="pct">''</c> <w
type="CS">that</w> <w type="DTI">any</w> <w type="NNS">irregularities</w> <w
type="VBD">took</w> <w type="NN">place</w> <c type="pct">.</c> </s>
> </p>
\end{verbatim}

Although there has been a push towards standardization of corpus
formats, most available resources display some degree of idiosyncrasy.
Being able to parse the structure of a corpus is a skill that will
develop with time. With more experience working with corpora you will
become more adept at identifying how the data is stored and whether its
content and format will serve the needs of your analysis.

\hypertarget{information}{%
\section{Information}\label{information}}

Identifying an adequate corpus resource for the target research question
is the first step in moving a quantitative text research project
forward. The next step is to select the components or characteristics of
this resource that are relevant for the research and then move to
organize the attributes of this data into a more useful and informative
format. This is the process of converting a corpus into a
\index{dataset}\textbf{dataset} --a tabular representation of the
information to be leveraged in the analysis.

\hypertarget{structure-1}{%
\subsection{Structure}\label{structure-1}}

Data alone is not informative. Only through explicit organization of the
data in a way that makes relationships accessible does the data become
information. This is a particularly salient hurdle in text analysis
research. Many textual sources are
\index{unstructured data}\textbf{unstructured data} --that is, the
relationships that will be used in the analysis have yet to be
explicitly drawn and organized from the text to make the relationships
meaningful and useful for analysis.

For the running text in the Europarle Corpus, we know that there are
files which are the source text (original) and files that correspond to
the target text (translation). In Table~\ref{tbl-structure-europarle} we
see that this text has been organized so that there are columns
corresponding to the \texttt{type} and \texttt{sentence} with an
additional \texttt{sentence\_id} column to keep an index of how the
sentences are aligned.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
It is conventional to work with column names for datasets in R using the
same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference. There are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the column names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the column and is
values consult the resource's documentation.
\end{tcolorbox}

\hypertarget{tbl-structure-europarle}{}
\begin{table}
\caption{\label{tbl-structure-europarle}First 10 source and target sentences in the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudacin del perodo de sesiones\\
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el perodo de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Seoras mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
\addlinespace
Target & 3 & Como todos han podido comprobar, el gran "efecto del ao 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros pases han sido vctimas de catstrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
Target & 4 & Sus Seoras han solicitado un debate sobre el tema para los prximos das, en el curso de este perodo de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las vctimas de las tormentas, en los distintos pases de la Unin Europea afectados.\\
\addlinespace
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
Source & 8 & Madam President, on a point of order.\\
\addlinespace
Target & 8 & Seora Presidenta, una cuestin de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabr usted por la prensa y la televisin que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visit el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Other corpus resources are \textbf{semi-structured data} --that is,
there are some characteristics which are structured, but other which are
not.

The Switchboard Dialog Act Corpus is an example of a semi-structured
resource. It has meta-data associated with each of the 1,155
conversations in the corpus. In Table~\ref{tbl-structure-swda} a
language-relevant sub-set of the meta-data is associated with each
utterance.

\hypertarget{tbl-structure-swda}{}
\begin{table}
\caption{\label{tbl-structure-swda}First 5 utterances from the Switchboard Dialog Act Corpus. }\tabularnewline

\centering
\begin{tabular}{lrrllllll}
\toprule
doc\_id & speaker\_id & topic\_num & topicality & naturalness & damsl\_tag & speaker & utterance\_num & utterance\_text\\
\midrule
4325 & 1632 & 323 & 3 & 2 & o & A & 1 & Okay.  /\\
4325 & 1632 & 323 & 3 & 2 & qw & A & 2 & \{D So, \}\\
4325 & 1519 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 1 & {}[ [ I guess, +\\
4325 & 1632 & 323 & 3 & 2 & + & A & 1 & What kind of experience [ do you, + do you ] have, then with child care? /\\
4325 & 1519 & 323 & 3 & 2 & + & B & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. /\\
\bottomrule
\end{tabular}
\end{table}

Relatively fewer resources are \textbf{structured datasets}. In these
cases a high amount of meta-data and/ or linguistic annotation is
included in the corpus. The format convention, however, varies from
resource to resource. Some of the formats are programming general (.csv,
.xml, .json, \emph{etc}.) and others are resource specific (.cha, .utt,
.prd, \emph{etc}.). In Table~\ref{tbl-structure-brown} the XML version
of the Brown Corpus is represented in tabular format. Note that along
with other meta-data variables, it also contains a variable with
linguistic annotation for grammatical category (\texttt{pos}
part-of-speech) of each word.

\hypertarget{tbl-structure-brown}{}
\begin{table}
\caption{\label{tbl-structure-brown}First 10 words from the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{llll}
\toprule
document\_id & category & words & pos\\
\midrule
01 & A & The & AT\\
01 & A & Fulton & NP\\
01 & A & County & NN\\
01 & A & Grand & JJ\\
01 & A & Jury & NN\\
\addlinespace
01 & A & said & VBD\\
01 & A & Friday & NR\\
01 & A & an & AT\\
01 & A & investigation & NN\\
01 & A & of & IN\\
\bottomrule
\end{tabular}
\end{table}

In this textbook, the selection of the attributes from a corpus and the
juxtaposition of these attributes in a relational format, or dataset,
that converts data into information will be referred to as \textbf{data
curation}. The process of data curation minimally involves creating a
base dataset, or \emph{derived dataset}, which establishes the main
informational associations according to philosophical approach outlined
by Wickham (2014). In this work, a \textbf{tidy dataset} refers both to
the structural (physical) and informational (semantic) organization of
the dataset. Physically, a tidy dataset is a tabular data structure
where each \emph{row} is an observation and each \emph{column} is a
variable that contains measures of a feature or attribute of each
observation. Each cell where a given row-column intersect contains a
\emph{value} which is a particular attribute of a particular observation
for the particular observation-feature pair also known as a \emph{data
point}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/understanding-data/tidy-format-paper.png}

}

\caption{\label{fig-tidy-format-image}Visual summary of the tidy
format.}

\end{figure}

Semantic value in a tidy dataset is derived from the association of this
physical structure along the two dimensions of this rectangular format.
First, each column is a \textbf{variable} which reflects measures for a
particular attribute. In the Europarle Corpus dataset, in Table
Table~\ref{tbl-structure-europarle}, for example, the \texttt{type}
column measures the type of text, either \texttt{Source} or
\texttt{Target}. Columns can contain measures which are qualitative or
quantitative, that is character-based or numeric. Second, each row is an
\textbf{observation} that contains all of the variables associated with
the primary unit of observation. The primary unit of observation the
variable that is the essential focus of the informational structure. In
this same dataset the first observation contains the \texttt{type},
\texttt{sentence\_id}, and the \texttt{sentence}. As this dataset is
currently structured the primary unit of investigation is the
\texttt{sentence} as each of the other variables have measures that
characterize each value of \texttt{sentence}.

The decision as to what the primary unit of observation is is
fundamentally guided by the research question, and therefore highly
specific to the particular research project. Say instead we wanted to
focus on words instead of sentences. The dataset would need to be
transformed such that a new variable (\texttt{words}) would be created
to contain each word in the corpus.

\hypertarget{tbl-tidy-words-europarle}{}
\begin{table}
\caption{\label{tbl-tidy-words-europarle}Europarle Paralle Corpus with words as primary unit of investigation. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & words\\
\midrule
Source & 1 & Resumption\\
Source & 1 & of\\
Source & 1 & the\\
Source & 1 & session\\
Target & 1 & Reanudacin\\
\addlinespace
Target & 1 & del\\
Target & 1 & perodo\\
Target & 1 & de\\
Target & 1 & sesiones\\
\bottomrule
\end{tabular}
\end{table}

The values for the variables \texttt{type} and \texttt{sentence\_id}
maintain the necessary description for each \texttt{word} to ensure the
required semantic relationships to identify the particular attributes
for each word observation. This dataset may seem redundant in that the
values for \texttt{type} and \texttt{sentence\_id} are repeated numerous
times but this `redundancy' makes the relationship between each variable
associated with the primary unit of investigation explicit. This format
makes a tidy dataset a versatile format for researchers to conduct
analyses in a powerful and flexible way, as we will see throughout this
textbook.

It is important to make clear that data in tabular format in itself does
not constitute a dataset, in the tidy sense we will be using. Data can
be organized in many ways which do not make relationships between
variables and observations explicit.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-important-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Consider}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-important-color!10!white, toprule=.15mm]
All tabular data does not have the `tidy' format that I have described
here. Can you think of examples of tabular information that would not be
in a tidy format?
\end{tcolorbox}

\hypertarget{transformation}{%
\subsection{Transformation}\label{transformation}}

At this point have introduced the first step in data curation in which
the original data is converted into a relational dataset (derived
dataset) and highlighted the importance of this informational structure
for setting the stage for data analysis. However, the primary derived
dataset is often not the final organizational step before proceeding to
statistical analysis. Many times, if not always, the derived dataset
requires some manipulation or transformation to prepare the dataset for
the specific analysis approach to be taken. This is another level of
human intervention and informational organization, and therefore another
step forward in our journey from data to insight and as such a step up
in the DIKI hierarchy. Common types of transformations include cleaning
variables (normalization), separating or eliminating variables
(recoding), creating new variables (generation), or incorporating others
datasets which integrate with the existing variables (merging). The
results of these transformations build on and manipulate the derived
dataset and produce an \emph{analysis dataset}. Let's now turn to
provide a select set of examples of each of these transformations using
the datasets we have introduced in this chapter.

\hypertarget{normalization}{%
\subsubsection{Normalization}\label{normalization}}

The process of normalization aims to \emph{sanitize} the values within a
variable or set of variables. This may include removing whitespace,
punctuation, numerals, or special characters or substituting uppercase
for lowercase characters, numerals for word versions, acronyms for their
full forms, irregular or incorrect spelling for accepted forms, or
removing common words (\textbf{stopwords}), \emph{etc}.

On inspecting the Europarle dataset
(Table~\ref{tbl-structure-europarle}) we will see that there are
sentence lines which do not represent actual parliament speeches. In
Table~\ref{tbl-normalize-non-speech-identify-europarle} we see these
lines.

\hypertarget{tbl-normalize-non-speech-identify-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-non-speech-identify-europarle}Non-speech lines in the Europarle dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & Resumption of the session\\
Target & 1 & Reanudacin del perodo de sesiones\\
Source & 7 & (The House rose and observed a minute' s silence)\\
Target & 7 & (El Parlamento, de pie, guarda un minuto de silencio)\\
\bottomrule
\end{tabular}
\end{table}

A research project aiming to analyze speech would want to normalize this
dataset removing these lines, as seen in
Table~\ref{tbl-normalize-non-speech-remove-europarle}.

\hypertarget{tbl-normalize-non-speech-remove-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-non-speech-remove-europarle}The Europarle dataset with non-speech lines removed. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & Declaro reanudado el perodo de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Seoras mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & Como todos han podido comprobar, el gran "efecto del ao 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros pases han sido vctimas de catstrofes naturales verdaderamente terribles.\\
Source & 4 & You have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & Sus Seoras han solicitado un debate sobre el tema para los prximos das, en el curso de este perodo de sesiones.\\
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Target & 5 & A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las vctimas de las tormentas, en los distintos pases de la Unin Europea afectados.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
Target & 6 & Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & Madam President, on a point of order.\\
Target & 8 & Seora Presidenta, una cuestin de procedimiento.\\
Source & 9 & You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\\
Target & 9 & Sabr usted por la prensa y la televisin que se han producido una serie de explosiones y asesinatos en Sri Lanka.\\
Source & 10 & One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\
\addlinespace
Target & 10 & Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visit el Parlamento Europeo.\\
\bottomrule
\end{tabular}
\end{table}

Another feature of this dataset which may require attention is the fact
that the English lines include whitespace between possessive nouns.

\hypertarget{tbl-normalize-whitespace-identify-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-whitespace-identify-europarle}Lines with possessives with extra whitespace in the Europarle dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute' s silence.\\
\bottomrule
\end{tabular}
\end{table}

This may affect another transformation process or subsequent analysis,
so it may be a good idea to normalize these forms by removing the extra
whitespace.

\hypertarget{tbl-normalize-whitespace-remove-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-whitespace-remove-europarle}The Europarle dataset with whitespace from possessives removed. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 5 & In the meantime, I should like to observe a minute's silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\
Source & 6 & Please rise, then, for this minute's silence.\\
\bottomrule
\end{tabular}
\end{table}

A final normalization case scenario involves changing converting all the
text to lowercase. If the goal for the research is to count words at
some point the fact that a word starts a sentence and by convention the
first letter is capitalized will result distinct counts for words that
are in essence the same (\emph{i.e.} ``In'' vs.~``in'').

\hypertarget{tbl-normalize-lowercase-europarle}{}
\begin{table}
\caption{\label{tbl-normalize-lowercase-europarle}The Europarle dataset with lowercasing applied. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 2 & i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\
Target & 2 & declaro reanudado el perodo de sesiones del parlamento europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a sus seoras mi deseo de que hayan tenido unas buenas vacaciones.\\
Source & 3 & although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\
Target & 3 & como todos han podido comprobar, el gran "efecto del ao 2000" no se ha producido. en cambio, los ciudadanos de varios de nuestros pases han sido vctimas de catstrofes naturales verdaderamente terribles.\\
Source & 4 & you have requested a debate on this subject in the course of the next few days, during this part-session.\\
\addlinespace
Target & 4 & sus seoras han solicitado un debate sobre el tema para los prximos das, en el curso de este perodo de sesiones.\\
Source & 5 & in the meantime, i should like to observe a minute's silence, as a number of members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the european union.\\
Target & 5 & a la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las vctimas de las tormentas, en los distintos pases de la unin europea afectados.\\
Source & 6 & please rise, then, for this minute's silence.\\
Target & 6 & invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\\
\addlinespace
Source & 8 & madam president, on a point of order.\\
Target & 8 & seora presidenta, una cuestin de procedimiento.\\
Source & 9 & you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka.\\
Target & 9 & sabr usted por la prensa y la televisin que se han producido una serie de explosiones y asesinatos en sri lanka.\\
Source & 10 & one of the people assassinated very recently in sri lanka was mr kumar ponnambalam, who had visited the european parliament just a few months ago.\\
\addlinespace
Target & 10 & una de las personas que recientemente han asesinado en sri lanka ha sido al sr. kumar ponnambalam, quien hace pocos meses visit el parlamento europeo.\\
\bottomrule
\end{tabular}
\end{table}

Note that lowercasing text, and normalization steps in general, can come
at a cost. For example, lowercasing the Europarle dataset sentences
means we lose potentially valuable information; namely the ability to
identify proper names (\emph{i.e.} ``Mr Kumar Ponnambalam'') and titles
(\emph{i.e.} ``European Parliament'') directly from the orthographic
forms. There are, however, transformation steps that can be applied
which aim to recover `lost' information in situations such as this and
others.

\hypertarget{recoding}{%
\subsubsection{Recoding}\label{recoding}}

The process of recoding aims to \emph{recast} the values of a variable
or set of variables to a new variable or set of variables to enable more
direct access. This may include extracting values from a variable,
stemming or lemmatization of words, tokenization of linguistic forms
(words, ngrams, sentences, \emph{etc}.), calculating the lengths of
linguistic units, removing variables that will not be used in the
analysis, \emph{etc}.

Words that we intuitively associate with a `base' word can take many
forms in language use. For example the word forms `investigation',
`investigation', `investigate', `investigated', \emph{etc}. are
intuitively linked. There are two common methods that can be applied to
create a new variable to facilitate the identification of these
associations. The first is stemming. \textbf{Stemming} is a rule-based
heuristic to reduce word forms to their stem or root form.

\hypertarget{tbl-recoding-stemming-brown-example}{}
\begin{table}
\caption{\label{tbl-recoding-stemming-brown-example}Results for stemming the first words in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & Counti\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Juri\\
\addlinespace
01 & A & said & VBD & said\\
01 & A & Friday & NR & Fridai\\
01 & A & an & AT & an\\
01 & A & investigation & NN & investig\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

A few things to note here. First there are a number of stemming
algorithms both for individual languages and distinct languages
\footnote{https://snowballstem.org/algorithms/}. Second not all words
can be stemmed as they do not have alternate morphological forms
(\emph{i.e.} ``The'', ``of'', \emph{etc}.). This generally related to
the distinction between closed-class (articles, prepositions,
conjunctions, \emph{etc}.) and open-class (nouns, verbs, adjectives,
\emph{etc}.) grammatical categories. Third the stem generated for those
words that can be stemmed result in forms that are not words themselves.
Nonetheless, stems can be very useful for more easily extracting a set
of related word forms.

As an example, let's identify all the word forms for the stem
`investig'.

\hypertarget{tbl-recoding-stemming-brown-search}{}
\begin{table}
\caption{\label{tbl-recoding-stemming-brown-search}Results for filter word stems for ``investig'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_stems\\
\midrule
01 & A & investigation & NN & investig\\
01 & A & investigate & VB & investig\\
03 & A & investigation & NN & investig\\
03 & A & investigation & NN & investig\\
07 & A & investigations & NNS & investig\\
\addlinespace
07 & A & investigate & VB & investig\\
08 & A & investigation & NN & investig\\
09 & A & investigation & NN & investig\\
09 & A & investigating & VBG & investig\\
09 & A & investigation & NN & investig\\
\bottomrule
\end{tabular}
\end{table}

We can see from the results in
Table~\ref{tbl-recoding-stemming-brown-search} that searching for
\texttt{word\_stems} that match `investig' returns a set of stem-related
forms. But it is worth noting that these forms cut across a number of
grammatical categories. If instead you want to draw a distinction
between grammatical categories, we can apply \textbf{lemmatization.}
This process is distinct from stemming in two important ways: (1)
inflectional forms are grouped by grammatical category and (2) the
resulting forms are lemmas or `base' forms of words.

\hypertarget{tbl-recoding-lemmatization-brown-example}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-example}Results for lemmatization of the first words in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & The & AT & The\\
01 & A & Fulton & NP & Fulton\\
01 & A & County & NN & County\\
01 & A & Grand & JJ & Grand\\
01 & A & Jury & NN & Jury\\
\addlinespace
01 & A & said & VBD & say\\
01 & A & Friday & NR & Friday\\
01 & A & an & AT & a\\
01 & A & investigation & NN & investigation\\
01 & A & of & IN & of\\
\bottomrule
\end{tabular}
\end{table}

To appreciate the difference between stemming and lemmatization, let's
compare a filter for \texttt{word\_lemmas} which match `investigation'.

\hypertarget{tbl-recoding-lemmatization-brown-investigation}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-investigation}Results for filter word stems for ``investigation'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
03 & A & investigation & NN & investigation\\
07 & A & investigations & NNS & investigation\\
08 & A & investigation & NN & investigation\\
\addlinespace
09 & A & investigation & NN & investigation\\
09 & A & investigation & NN & investigation\\
23 & A & investigation & NN & investigation\\
25 & A & investigation & NN & investigation\\
41 & A & investigation & NN & investigation\\
\bottomrule
\end{tabular}
\end{table}

Only lemma forms of `investigate' which are nouns appear. Let's run a
similar search but for the lemma `be'.

\hypertarget{tbl-recoding-lemmatization-brown-be}{}
\begin{table}
\caption{\label{tbl-recoding-lemmatization-brown-be}Results for filter word stems for ``be'' in the Brown Corpus. }\tabularnewline

\centering
\begin{tabular}{lllll}
\toprule
document\_id & category & words & pos & word\_lemmas\\
\midrule
01 & A & was & BEDZ & be\\
01 & A & been & BEN & be\\
01 & A & was & BEDZ & be\\
01 & A & was & BEDZ & be\\
01 & A & are & BER & be\\
\addlinespace
01 & A & are & BER & be\\
01 & A & be & BE & be\\
01 & A & is & BEZ & be\\
01 & A & was & BEDZ & be\\
01 & A & be & BE & be\\
\bottomrule
\end{tabular}
\end{table}

Again only words of the same grammatical category are returned. In this
case the verb `be' has many more inflectional forms than `investigate'.

Another form of recoding is to detect a pattern in the values of an
existing variable and create a new variable whose values are the
extracted pattern or register that the pattern occurs and/ or how many
times it occurs. As an example, let's count the number of disfluencies
(`uh' or `um') that occur in each utterance in \texttt{utterance\_text}
from the Switchboard Dialog Act Corpus. \emph{Note I've simplified the
dataset dropping the non-relevant variables for this example.}

\hypertarget{tbl-recoding-extract-switchboard}{}
\begin{table}
\caption{\label{tbl-recoding-extract-switchboard}Disfluency counts in the first 10 utterance text values from the
Switchboard Corpus. }\tabularnewline

\centering
\begin{tabular}{lr}
\toprule
utterance\_text & disfluency\_count\\
\midrule
Okay.  / & 0\\
\{D So, \} & 0\\
{}[ [ I guess, + & 0\\
What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\addlinespace
Does it say something? / & 0\\
I think it usually does.  / & 0\\
You might try, \{F uh, \}  / & 1\\
I don't know,  / & 0\\
hold it down a little longer,  / & 0\\
\bottomrule
\end{tabular}
\end{table}

One of the most common forms of recoding in text analysis is
tokenization. \textbf{Tokenization} is the process of recasting the text
into smaller linguistic units. When working with text that has not been
linguistically annotated, the most feasible linguistic tokens are words,
ngrams, and sentences. While word and sentence tokens are easily
understandable, ngram tokens need some explanation. An \textbf{ngram} is
a sequence of either characters or words where \emph{n} is the length of
this sequence. The ngram sequences are drawn incrementally, so the
bigrams (two-word sequences) for the sentence ``This is an input
sentence.'' are:

this is, is an, an input, input sentence

We've already seen word tokenization exemplified with the Europarle
Corpus in subsection \protect\hyperlink{structure-1}{Structure} in
Table~\ref{tbl-tidy-words-europarle}, so let's create (word) bigram
tokens for this corpus.

\hypertarget{tbl-recoding-tokenization-europarle-bigram-words}{}
\begin{table}
\caption{\label{tbl-recoding-tokenization-europarle-bigram-words}The first 10 word bigrams of the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & word\_bigrams\\
\midrule
Source & 2 & i declare\\
Source & 2 & declare resumed\\
Source & 2 & resumed the\\
Source & 2 & the session\\
Source & 2 & session of\\
\addlinespace
Source & 2 & of the\\
Source & 2 & the european\\
Source & 2 & european parliament\\
Source & 2 & parliament adjourned\\
Source & 2 & adjourned on\\
\bottomrule
\end{tabular}
\end{table}

As I just mentioned, ngrams sequences can be formed of characters as
well. Here are character trigram (three-character) sequences.

\hypertarget{tbl-recoding-tokenization-europarle-trigram-chars}{}
\begin{table}
\caption{\label{tbl-recoding-tokenization-europarle-trigram-chars}The first 10 character trigrams of the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & char\_trigrams\\
\midrule
Source & 2 & ide\\
Source & 2 & dec\\
Source & 2 & ecl\\
Source & 2 & cla\\
Source & 2 & lar\\
\addlinespace
Source & 2 & are\\
Source & 2 & rer\\
Source & 2 & ere\\
Source & 2 & res\\
Source & 2 & esu\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{generation}{%
\subsubsection{Generation}\label{generation}}

The process of generation aims to \emph{augment} a variable or set of
variables. In essence this aims to make implicit attributes explicit to
that they are directly accessible. This often targeted at the automatic
generation of linguistic annotations such as grammatical category
(part-of-speech) or syntactic structure.

In the examples below I've added linguistic annotation to a target
(English) and source (Spanish) example sentence from the Europarle
Parallel Corpus. First, note the variables that are added to our dataset
that correspond to grammatical category. In addition to the
\texttt{type} and \texttt{sentence\_id} we have an assortment of
variables which replace the \texttt{sentence} variable. As part of the
process of annotation the input text to be annotated \texttt{sentence}
is tokenized \texttt{token} and indexed \texttt{token\_id}. Then
\texttt{upos} contains the Universal Part of Speech tags\footnote{\href{https://universaldependencies.org/u/pos/}{Descriptions
  of the UPOS tagset}}, and a detailed list of features is included in
\texttt{feats}. The syntactic annotation is reflected in the
\texttt{token\_id\_source} and \texttt{syntactic\_relation} variables.
These variables correspond to the type of syntactic parsing that has
been done, in this case Dependency Parsing (using the
\href{https://universaldependencies.org/}{Universal Dependencies}
framework). Another common syntactic parsing framework is phrase
constituency parsing (Jurafsky and Martin 2020).

\hypertarget{tbl-generation-europarle-en-example}{}
\begin{table}
\caption{\label{tbl-generation-europarle-en-example}Automatic linguistic annotation for grammatical category and syntactic
structure for an example English sentence from the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Target & 6 & 1 & Invito & ADP & NA & 3 & case\\
Target & 6 & 2 & a & DET & Definite=Ind|PronType=Art & 3 & det\\
Target & 6 & 3 & todos & NOUN & Number=Plur & 6 & nmod\\
Target & 6 & 4 & a & DET & Definite=Ind|PronType=Art & 6 & det\\
Target & 6 & 5 & que & ADJ & Degree=Pos & 6 & amod\\
\addlinespace
Target & 6 & 6 & nos & NOUN & Number=Plur & 0 & root\\
Target & 6 & 7 & pongamos & X & NA & 13 & goeswith\\
Target & 6 & 8 & de & X & Foreign=Yes & 13 & goeswith\\
Target & 6 & 9 & pie & X & NA & 13 & goeswith\\
Target & 6 & 10 & para & X & NA & 13 & goeswith\\
\addlinespace
Target & 6 & 11 & guardar & X & NA & 13 & goeswith\\
Target & 6 & 12 & un & X & NA & 13 & goeswith\\
Target & 6 & 13 & minuto & NOUN & Number=Sing & 6 & appos\\
Target & 6 & 14 & de & PROPN & Number=Sing & 15 & compound\\
Target & 6 & 15 & silencio & PROPN & Number=Sing & 13 & flat\\
\addlinespace
Target & 6 & 16 & . & PUNCT & NA & 6 & punct\\
\bottomrule
\end{tabular}
\end{table}

Now compare the English example sentence dataset in
Table~\ref{tbl-generation-europarle-en-example} with the parallel
sentence in Spanish. Note that the grammatical features are language
specific. For example, Spanish has gender which is apparent when
scanning the \texttt{feats} variable.

\hypertarget{tbl-generation-europarle-es-example}{}
\begin{table}
\caption{\label{tbl-generation-europarle-es-example}Automatic linguistic annotation for grammatical category and syntactic
structure for an example Spanish sentence from the Europarle Corpus. }\tabularnewline

\centering
\begin{tabular}{lrllllll}
\toprule
type & sentence\_id & token\_id & token & upos & feats & token\_id\_source & syntactic\_relation\\
\midrule
Source & 6 & 1 & Please & PROPN & Gender=Fem|Number=Sing & 4 & nsubj\\
Source & 6 & 2 & rise & PROPN & Number=Sing & 1 & flat\\
Source & 6 & 3 & , & PUNCT & NA & 1 & punct\\
Source & 6 & 4 & then & VERB & Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin & 0 & root\\
Source & 6 & 5 & , & PUNCT & NA & 9 & punct\\
\addlinespace
Source & 6 & 6 & for & ADP & NA & 9 & compound\\
Source & 6 & 7 & this & X & NA & 9 & compound\\
Source & 6 & 8 & minute's & X & Gender=Masc|Number=Sing & 9 & compound\\
Source & 6 & 9 & silence & X & Gender=Masc|Number=Sing & 4 & conj\\
Source & 6 & 10 & . & PUNCT & NA & 4 & punct\\
\bottomrule
\end{tabular}
\end{table}

There is much more to explore with linguistic annotation, and syntactic
parsing in particular, but at this point it will suffice to note that it
is possible to augment a dataset with grammatical information
automatically.

There are strengths and shortcomings with automatic linguistic
annotation that a research should be aware of. First, automatic
linguistic annotation provides quick access to rich and highly reliable
linguistic information for a large number of languages. However,
part-of-speech taggers and syntactic parsers are not magic. They are
resources that are built by training a computational algorithm to
recognize patterns in manually annotated datasets producing a language
model. This model is then used to predict the linguistic annotations for
new language (as we just did in the previous examples). The shortcomings
of automatic linguistic annotation is first, not all languages have
trained language models and second, the data used to train the model
inevitably reflect a particular variety, register, modality, \emph{etc}.
The accuracy of the linguistic annotation is highly dependent on
alignment between the language sampling frame of the trained data and
the language data to be automatically annotated. Many (most) of the
language models available for automatic linguistic annotation are based
on language that is most readily available and for most languages this
has traditionally been newswire text. It is important to be aware of
these characteristics when using linguistic annotation tools.

\hypertarget{merging}{%
\subsubsection{Merging}\label{merging}}

The process of merging aims to \emph{join} a variable or set of
variables with another variable or set of variables from another
dataset. The option to merge two (or more) datasets requires that there
is a shared variable that indexes and aligns the datasets.

To provide an example let's look at the Switchboard Diaglog Act Corpus.
Our existing, disfluency recoded, version includes the following
variables.

\begin{verbatim}
#> Rows: 5
#> Columns: 11
#> $ doc_id           <chr> "4325", "4325", "4325", "4325", "4325"
#> $ speaker_id       <dbl> 1632, 1632, 1519, 1632, 1519
#> $ topic_num        <dbl> 323, 323, 323, 323, 323
#> $ topicality       <chr> "3", "3", "3", "3", "3"
#> $ naturalness      <chr> "2", "2", "2", "2", "2"
#> $ damsl_tag        <chr> "o", "qw", "qy^d", "+", "+"
#> $ speaker          <chr> "A", "A", "B", "A", "B"
#> $ turn_num         <chr> "1", "1", "2", "3", "4"
#> $ utterance_num    <chr> "1", "2", "1", "1", "1"
#> $ utterance_text   <chr> "Okay.  /", "{D So, }", "[ [ I guess, +", "What kind ~
#> $ disfluency_count <int> 0, 0, 0, 0, 1
\end{verbatim}

It turns out that on the
\href{https://catalog.ldc.upenn.edu/docs/LDC97S62/}{corpus website} a
number of meta-data files are available, including files pertaining to
speakers and the topics of the conversations.

The speaker meta-data for this corpus is in a the
\texttt{caller\_tab.csv} file and contains a \texttt{speaker\_id}
variable which corresponds to each speaker in the corpus and other
potentially relevant variables for a language research project including
\texttt{sex}, \texttt{birth\_year}, \texttt{dialect\_area}, and
\texttt{education}.

\hypertarget{tbl-merging-swda-speaker}{}
\begin{table}
\caption{\label{tbl-merging-swda-speaker}Speaker meta-data for the Switchboard Dialog Act Corpus. }\tabularnewline

\centering
\begin{tabular}{rlrlr}
\toprule
speaker\_id & sex & birth\_year & dialect\_area & education\\
\midrule
1632 & FEMALE & 1962 & WESTERN & 2\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
1632 & FEMALE & 1962 & WESTERN & 2\\
1519 & FEMALE & 1971 & SOUTH MIDLAND & 1\\
\bottomrule
\end{tabular}
\end{table}

Since both datasets contain a shared index, \texttt{speaker\_id} we can
merge these two datasets. The result is found in
Table~\ref{tbl-merging-swda-speaker-added}.

\hypertarget{tbl-merging-swda-speaker-added}{}
\begin{table}
\caption{\label{tbl-merging-swda-speaker-added}Merged conversations and speaker meta-data for the Switchboard Dialog
Act Corpus. }\tabularnewline

\centering
\begin{tabular}{lrlrlrrlllllllr}
\toprule
doc\_id & speaker\_id & sex & birth\_year & dialect\_area & education & topic\_num & topicality & naturalness & damsl\_tag & speaker & turn\_num & utterance\_num & utterance\_text & disfluency\_count\\
\midrule
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & o & A & 1 & 1 & Okay.  / & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & qw & A & 1 & 2 & \{D So, \} & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & qy\textasciicircum{}d & B & 2 & 1 & {}[ [ I guess, + & 0\\
4325 & 1632 & FEMALE & 1962 & WESTERN & 2 & 323 & 3 & 2 & + & A & 3 & 1 & What kind of experience [ do you, + do you ] have, then with child care? / & 0\\
4325 & 1519 & FEMALE & 1971 & SOUTH MIDLAND & 1 & 323 & 3 & 2 & + & B & 4 & 1 & I think, ] + \{F uh, \} I wonder ] if that worked. / & 1\\
\bottomrule
\end{tabular}
\end{table}

In this example case the dataset that was merged was already in a
structured format (.csv). Many corpus resources contain meta-data in
stand-off files that are structured.

In some cases a researcher would like to merge information that does not
already accompany the corpus resource. This is possible as long as a
dataset can be created that contains a variable that is shared. Without
a shared variable to index the datasets the merge cannot take place.

In sum, the transformation steps described here collectively aim to
produce higher quality datasets that are relevant in content and
structure to submit to analysis. The process may include one or more of
the previous transformations but is rarely linear and is most often
iterative. It is typical to do some normalization then generation, then
recoding, and then return to normalizing, and so forth. This process is
highly idiosyncratic given the characteristics of the derived dataset
and the ultimate goals for the analysis dataset.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
Note in some cases we may convert our tidy tabular dataset to other data
formats that may be required for some particular statistic approaches
but at all times the relationship between the variables should be
maintained in line with our research purpose. We will touch on examples
of other types of data formats (\emph{e.g.} Corpus and Document-Term
Matrix (DTM) objects in R) when we dive into particular statistical
approaches that require them later in the textbook.
\end{tcolorbox}

\hypertarget{documentation}{%
\section{Documentation}\label{documentation}}

As we have seen in this chapter that acquiring data and converting that
data into information involves a number of conscious decisions and
implementation steps. As a favor to ourselves as researchers and to the
research community, it is crucial to document these decisions and steps.
This makes it both possible to retrace our own steps and also provides a
guide for future researchers that want to reproduce and/ or build on
your research. A programmatic approach to quantitative research helps
ensure that the implementation steps are documented and reproducible but
it is also vital that the decisions that are made are documented as
well. This includes the creation/ selection of the corpus data, the
description of the variables chosen from the corpus for the derived
dataset, and the description of the variables created from the derived
dataset for the analysis dataset.

For an existing corpus sample acquired from a repository (\emph{e.g.}
\href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard Dialog Act
Corpus}, Language Data Consortium), a research group (\emph{e.g.}
\href{http://cedel2.learnercorpora.com/}{CEDEL2}), or an individual
researcher (\emph{e.g.}
\href{https://www.dt.fee.unicamp.br/~tiago/smsspamcollection/}{SMS Spam
Collection}), there is often documentation provided describing key
attributes of the resource. This documentation should be included with
the acquisition of the corpus and added to the research project. For a
corpus that a researcher compiles themselves, they will need to generate
this documentation.

The curation and transformation steps conducted on the original corpus
data to produce the datasets should also be documented. The steps
themselves can be included in the programming scripts as code comments
(or in prose if using a literate programming strategy (\emph{e.g.} R
Markdown)). The structure of each resulting dataset should include what
is called a \textbf{data dictionary}. This is a table which includes the
variable names, the values they contain, and a short prose description
of each variable (\emph{e.g.} \href{https://osf.io/9jafz/}{ACTIV-ES
Corpus}).

\hypertarget{activities-1}{%
\section*{Activities}\label{activities-1}}
\addcontentsline{toc}{section}{Activities}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_3.html}{Reading,
inspecting, and writing data}\\
\textbf{How}: Read Recipe 3 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To use literate programming in R markdown to work with R
coding strategies for reading, inspecting, and writing datasets.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_3}{Reading,
inspecting, and writing data}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 3.\\
\textbf{Why}: To read datasets from packages and from plain-text files,
inspect and report characteristics of datasets, and write datasets to
plain-text files.
\end{tcolorbox}

\hypertarget{summary-2}{%
\section*{Summary}\label{summary-2}}
\addcontentsline{toc}{section}{Summary}

In this chapter we have focused on data and information --the first two
components of DIKI Hierarchy. This process is visualized in
Figure~\ref{fig-understanding-data-vis-sum}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/understanding-data/understanding-data_visual-summary-paper.png}

}

\caption{\label{fig-understanding-data-vis-sum}Understanding data:
visual summary}

\end{figure}

First a distinction is made between populations and samples, the latter
being a intentional and subjective selection of observations from the
world which attempt to represent the population of interest. The result
of this process is known as a corpus. Whether developing a corpus or
selecting an existing a corpus it is important to vet the sampling frame
for its applicability and viability as a resource for a given research
project.

Once a viable corpus is identified, then that corpus is converted into a
derived dataset which adopts the tidy dataset format where each column
is a variable, each row is an observation, and the intersection of
columns and rows contain values. This derived dataset serves to
establish the base informational relationships from which your research
will stem.

The derived dataset will most likely require transformations including
normalization, recoding, generation, and/ or merging to enhance the
usefulness of the information to analysis. An analysis dataset is the
result of this process.

Finally, documentation should be implemented at each stage of the
analysis project process. Employing a programmatic approach establishes
documentation of the implementation steps but the motivation behind the
decisions taken and the content of the corpus data and datasets
generated also need documentation to ensure transparent and reproducible
research.

\hypertarget{sec-approaching-analysis}{%
\chapter{Approaching analysis}\label{sec-approaching-analysis}}

\begin{quote}
Statistical thinking will one day be as necessary for efficient
citizenship as the ability to read and write.

--- H.G. Wells
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What is the role of statistics in data analysis?
\item
  What is the importance of descriptive assessment in data analysis?
\item
  In what ways are main approaches to data analysis similar and
  different?
\end{itemize}

\end{tcolorbox}

In this chapter I will build on the notions of data and information from
the previous chapter. The aim of statistics in quantitative analysis is
to uncover patterns in datasets. Thus statistics is aimed at deriving
knowledge from information, the next step in the DIKI Hierarchy
Figure~\ref{fig-diki-hierarchy}. Where the creation of information from
data involves human intervention and conscious decisions, as we have
seen, deriving knowledge from information involves even more conscious
subjective decisions on what information to assess, and what method to
select to interrogate the information, and ultimately how to interpret
the findings. The first step is to conduct a descriptive assessment of
the information, both at the individual variable level and also between
variables, the second is to interrogate the dataset either through
inferential, predictive, or exploratory analysis methods, and the third
is to interpret and report the findings.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interactive programming}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Data
visualization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To explore data visually in text and in graphics.
\end{tcolorbox}

\hypertarget{description}{%
\section{Description}\label{description}}

A descriptive assessment of the dataset includes a set of diagnostic
measures and tabular and visual summaries which provide researchers a
better understanding of the structure of a dataset, prepare the
researcher to make decisions about which statistical methods and/ or
tests are most appropriate, and to safeguard against false assumptions
(missing data, data distributions, etc.). In this section we will first
cover the importance of understanding the informational value that
variables can represent and then move to use this understanding to
approach summarizing individual variables and relationships between
variables.

To ground this discussion I will introduce a new dataset. This dataset
is drawn from the
\href{https://slabank.talkbank.org/access/English/BELC.html}{Barcelona
English Language Corpus (BELC)}, which is found in the
\href{http://talkbank.org/}{TalkBank repository}. I've selected the
``Written composition'' task from this corpus which contains writing
samples from second language learners of English at different ages.
Participants were given the task of writing for 15 minutes on the topic
of ``Me: my past, present and future''. Data was collected for many (but
not all) participants up to four times over the course of seven years.
In Table~\ref{tbl-belc-overview} I've included the first 10 observations
from the dataset which reflects structural and transformational steps
I've done so we start with a tidy dataset.

\hypertarget{tbl-belc-overview}{}
\begin{table}
\caption{\label{tbl-belc-overview}First 10 observations of the BELC dataset for demonstration. }\tabularnewline

\centering
\begin{tabular}{l|l|l|r|r|r}
\hline
participant\_id & age\_group & sex & num\_tokens & num\_types & ttr\\
\hline
L02 & 10-year-olds & female & 12 & 12 & 1.000\\
\hline
L05 & 10-year-olds & female & 18 & 15 & 0.833\\
\hline
L10 & 10-year-olds & female & 36 & 26 & 0.722\\
\hline
L11 & 10-year-olds & female & 10 & 8 & 0.800\\
\hline
L12 & 10-year-olds & female & 41 & 23 & 0.561\\
\hline
L16 & 10-year-olds & female & 13 & 12 & 0.923\\
\hline
L22 & 10-year-olds & female & 47 & 30 & 0.638\\
\hline
L27 & 10-year-olds & female & 8 & 8 & 1.000\\
\hline
L28 & 10-year-olds & female & 84 & 34 & 0.405\\
\hline
L29 & 10-year-olds & female & 53 & 34 & 0.642\\
\hline
\end{tabular}
\end{table}

The entire dataset includes 79 observations from 36 participants. Each
observation in the BELC dataset corresponds to an individual learner's
composition. It includes which participant wrote the composition
(\texttt{participant\_id}), the age group they were part of at the time
(\texttt{age\_group}), their sex (\texttt{sex}), and the number of
English words they produced (\texttt{num\_tokens}), the number of unique
English words they produced (\texttt{num\_types}). The final variable
(\texttt{ttr}) is the calculated ratio of number of unique words
(\texttt{num\_types}) to total words (\texttt{num\_tokens}) for each
composition. This is known as the Type-Token Ratio and it is a standard
metric for measuring lexical diversity.

\hypertarget{information-values}{%
\subsection{Information values}\label{information-values}}

Understanding the informational value, or \textbf{level of measurement},
of a variable or set of variables in key to preparing for analysis as it
has implications for what visualization techniques and statistical
measures we can use to interrogate the dataset. There are two main
levels of measurement a variable can take: categorical and continuous.
\textbf{Categorical variables} reflect class or group values.
\textbf{Continuous variables} reflect values that are measured along a
continuum.

The BELC dataset contains three categorical variables
(\texttt{participant\_id}, \texttt{age\_group}, and \texttt{sex}) and
three continuous variables (\texttt{num\_tokens}, \texttt{num\_types},
and \texttt{ttr}). The categorical variables identify class or group
membership; which participant wrote the composition, what age group they
were in, and their biological sex. The continuous variables measure
attributes that can take a range of values without a fixed limit and the
differences between each value are regular. The number of words and
number of unique words for each composition can range from 1 to \(n\)
and the Type-Token Ratio being derived from these two variables is also
continuous for the same reason. Furthermore, the differences between the
each of values of these measures is on a defined interval, so for
example a composition which has a word count (\texttt{num\_tokens}) of
40 is exactly two times as large as a composition with a word count of
20.

The distinction between categorical an continuous levels of measurement,
as mentioned above, are the main two and for some statistical approaches
the only distinction that needs to be made to conduct an analysis.
However, categorical and continuous can each be broken down into
subcategories and for some descriptive and analytic purposes these
distinctions are important. For categorical variables a distinction can
be made between variables in which there is a structured relationship
between the values and those in which there is not. \emph{Nominal
variables} contain values which are labels denoting the membership in a
class in which there is no relationship between the labels.
\emph{Ordinal variables} also contain labels of classes, but in contrast
to nominal variables, there is a relationship between the classes,
namely one in which there is a precedence relationship or order. With
this in mind, our categorical variables be sub-classified. There is no
order between the values of \texttt{participant\_id} and \texttt{sex}
and they are therefore nominal whereas the values of \texttt{age\_group}
are ordered, each value refers to a sequential age group, and therefore
it is ordinal.

Turning to continuous variables, another subdivision can be made which
hinges on the existence of a non-arbitrary zero or not. \emph{Interval
variables} contain values in which the difference between the values is
regular and defined, but the measure has an arbitrary zero value. A
typically cited example of an interval variable is temperature
measurements on the Fahrenheit scale. A value of 0 on this scale does
not mean there is 0 temperature. \emph{Ratio variables} have all the
properties of interval variables but also include a non-arbitrary
definition of zero. All of the continuous variables in the BELC dataset
(\texttt{num\_tokens}, \texttt{num\_types}, and \texttt{ttr}) are ratio
variables as a value of 0 would indicate the lack of this attribute.

An hierarchical overview of the relationship between the two main and
four sub-types of levels of measurement appear in
Figure~\ref{fig-info-values}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/Informational-values-paper.png}

}

\caption{\label{fig-info-values}Levels of measurement graphic
representation.}

\end{figure}

A few notes of practical importance; First, the distinction between
interval and ratio variables is often not applicable in text analysis
and therefore often treated together as continuous variables. Second,
the distinction between ordinal and interval/continuous variables is not
as clear cut as it may seem. Both variables contain values which have an
ordered relationship. By definition the values of an ordinal variable do
not reflect regular intervals between the units of measurement. But in
practice interval/continuous variables with a defined number of values
(say from a Likert scale used on a survey) may be treated as an ordinal
variable as they may be better understood as reflecting class
membership. Third, all continuous variables can be converted to
categorical variables, but the reverse is not true. We could, for
example, define a criterion for binning the word counts in
\texttt{num\_tokens} for each composition into ordered classes such as
``low'', ``mid'', ``high''. On the other hand, \texttt{sex} (as it has
been measured here) cannot take intermediate values on a unfixed range.
The upshot is that variables can be down-typed but not up-typed. In most
cases it is preferred to treat continuous variables as such, if the
nature of the variable permits it, as the down-typing of continuous data
to categorical data results in a loss of information --which will result
in a loss of information and hence statistical power which may lead to
results that obscure meaningful patterns in the data (Baayen 2004).

\hypertarget{summaries}{%
\subsection{Summaries}\label{summaries}}

It is always key to gain insight into shape of the information through
numeric, tabular and/ or visual summaries before jumping in to analytic
statistical approaches. The most appropriate form of summarizing
information will depend on the number and informational value(s) of our
target variables. To get a sense of how this looks, let's continue to
work with the BELC dataset and pose different questions to the data with
an eye towards seeing how various combinations of variables are
descriptively explored.

\hypertarget{single-variables}{%
\subsubsection{Single variables}\label{single-variables}}

The way to statistically summarize a variable into a single measure is
to derive a \textbf{measure of central tendency}. For a continuous
variable the most common measure is the (arithmetic) \emph{mean}, or
average, which is simply the sum of all the values divided by the number
of values. As a measure of central tendency, however, the mean can be
less-than-reliable as it is sensitive to outliers which is to say that
data points in the variable that are extreme relative to the overall
distribution of the other values in the variable affect the value of the
mean depending on how extreme the deviate. One way to assess the effects
of outliers is to calculate a \textbf{measure of dispersion}. The most
common of these is the \emph{standard deviation} which estimates the
average amount of variability between the values in a continuous
variable. Another way to assess, or rather side-step, outliers is to
calculate another measure of central tendency, the \emph{median}. A
median is calculated by sorting all the values in the variable and then
selecting the value which falls in the middle of all the other values. A
median is less sensitive to outliers as extreme values (if there are
few) only indirectly affect the selection of the middle value. Another
measure of dispersion is to calculate quantiles. A \emph{quantile}
slices the data in four percentile ranges providing a five value numeric
summary of the spread of the values in a continuous variable. The spread
between the first and third quantile is known as the Interquartile Range
(IQR) and is also used as a single statistic to summarize variability
between values in a continuous variable.

Below is a list of central tendency and dispersion scores for the
continuous variables in the BELC dataset.

\textbf{Variable type: numeric}

\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & mean & sd & p0 & p25 & p50 & p75 & p100 & iqr\\
\hline
num\_tokens & 0 & 1 & 66.23 & 43.90 & 1.00 & 29.00 & 55.00 & 90.00 & 185 & 61.00\\
\hline
num\_types & 0 & 1 & 40.25 & 22.80 & 1.00 & 22.00 & 38.00 & 54.00 & 97 & 32.00\\
\hline
ttr & 0 & 1 & 0.67 & 0.13 & 0.41 & 0.57 & 0.64 & 0.73 & 1 & 0.16\\
\hline
\end{tabular}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The descriptive statistics returned above were generated by the
\texttt{skimr} package.
\end{tcolorbox}

In the above summary, we see the mean, standard deviation (sd), and the
quantiles (the five-number summary, p0, p25, p50, p75, and p100). The
middle quantile (p50) is the median and the IQR is listed last.

These are important measures for assessing the central tendency and
dispersion and will be useful for reporting purposes, but to get a
better feel of how a variable is distributed, nothing beats a visual
summary. A boxplot graphically summarizes many of these metrics. In
Figure~\ref{fig-summaries-boxplots-belc} we see the same three
continuous variables, but now in graphical form.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-boxplots-belc}Boxplots for each of the
continuous variables in the BELC dataset.}

\end{figure}

In a boxplot, the bold line is the median. The surrounding box around
the median is the interquantile range. The extending lines above and
below the IQR mark the largest and lowest value that is within 1.5 times
either the 3rd (top of the box) or 1st (bottom of the box). Any values
that fall outside, above or below, the extending lines are considered
statistical outliers and are marked as dots (in this case red dots).
\footnote{Note that each of these three variables are to be considered
  separately here (vertically). Later we will see the use of boxplots to
  compare a continuous variable across levels of a categorical variable
  (horizontally).}

Boxplots provide a robust and visually intuitive way of assessing
central tendency and variability in a continuous variable but this type
of plot can be complemented by looking at the overall distribution of
the values in terms of their frequencies. A histogram provides a
visualization of the frequency (and density in this case with the blue
overlay) of the values across a continuous variable binned at regular
intervals.

In Figure~\ref{fig-summaries-histograms-belc} I've plotted histograms in
the top row and density plots in the bottom row for the same three
continuous variables from the BELC dataset.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-histograms-belc-1.pdf}

}

\caption{\label{fig-summaries-histograms-belc}Histograms and density
plots for the continuous variables in the BELC dataset.}

\end{figure}

Histograms provide insight into the distribution of the data. For our
three continuous variables, the distributions happen not to be too
strikingly distinct. They are, however, not the same either. When we
explore continuous variables with histograms we are often trying to
assess whether there is skew or not. There are three general types of
skew, visualized in Figure~\ref{fig-summaries-skew-graphic}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/skew-types-paper.png}

}

\caption{\label{fig-summaries-skew-graphic}Examples of skew types in
density plots.}

\end{figure}

In histograms/ density plots in which the distribution is either left or
right, the median and the mean are not aligned. The \emph{mode}, which
indicates the most frequent value in the variable is also not aligned
with the other two measures. In a left-skewed distribution the mean will
be to the left of the median which is left of the mode whereas in a
right-skewed distribution the opposite occurs. In a distribution with
absolutely no skew these three measures are the same. In practice these
measures rarely align perfectly but it is very typical for these three
measures to approximate alignment. It is common enough that this
distribution is called the Normal Distribution \footnote{formally known
  as a Gaussian Distribution} as it is very common in real-world data.

Another and potentially more informative way to inspect the normality of
a distribution is to create Quantile-Quantile plots (QQ Plot). In
Figure~\ref{fig-summaries-qqnorm-plot-belc} I've created QQ plots for
our three continuous variables. The line in each plot is the normal
distribution and the more points that fall off of this line, the less
likely that the distribution is normal.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-qqnorm-plot-belc-1.pdf}

}

\caption{\label{fig-summaries-qqnorm-plot-belc}QQ Plots for the
continuous variables in the BELC dataset.}

\end{figure}

A visual inspection can often be enough to detect non-normality, but in
cases which visually approximate the normal distribution (such as these)
we can perform the Shapiro-Wilk test of normality. This is an
inferential test that compares a variable's distribution to the normal
distribution. The likelihood that the distribution differs from the
normal distribution is reflected in a \(p\)-value. A \(p\)-value below
the .05 threshold suggests the distribution is non-normal. In
Table~\ref{tbl-summaries-normality-test-belc} we see that given this
criterion only the distribution of \texttt{num\_types} is normally
distributed.

\hypertarget{tbl-summaries-normality-test-belc}{}
\begin{table}
\caption{\label{tbl-summaries-normality-test-belc}Results from Shapiro-Wilk test of normality for continuous variables in
the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{lrr}
\toprule
variable & statistic & p\_value\\
\midrule
Number of tokens & 0.942 & 0.001\\
Number of types & 0.970 & 0.058\\
Type-Token Ratio & 0.947 & 0.003\\
\bottomrule
\end{tabular}
\end{table}

Downstream in the analytic analysis, the distribution of continuous
variables will need to be taken into account for certain statistical
tests. Tests that assume `normality' are parametric tests, those that do
not are non-parametric. Distributions which approximate the normal
distribution can sometimes be transformed to conform to the normal
distribution either by outlier trimming or through statistical
procedures (e.g.~square root, log, or inverse transformation), if
necessary. At this stage, however, the most important thing is to
recognize whether the distributions approximate or wildly diverge from
the normal distribution.

Before we leave continuous variables, let's consider another approach
for visually summarizing a single continuous variable. The Empirical
Cumulative Distribution Frequency, or \emph{ECDF}, is a summary of the
cumulative proportion of each of the values of a continuous variable. An
ECDF plot can be useful in determining what proportion of the values
fall above or below a certain percentage of the data.

In Figure~\ref{fig-summarize-ecdf-belc} we see ECDF plots for our three
continuous variables.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summarize-ecdf-belc-1.pdf}

}

\caption{\label{fig-summarize-ecdf-belc}ECDF plots for the continuous
variables in the BELC dataset.}

\end{figure}

Take, for example, the number of tokens (\texttt{num\_tokens}) per
composition. The ECDF plot tells us that 50\% of the values in this
variable are 56 words or less. In the three variables plotted, the
cumulative growth is quite steady. In some cases it is not. When it is
not, an ECDF goes a long way to provide us a glimpse into key bends in
the proportions of values in a variable.

Now let's turn to the descriptive assessment of categorical variables.
For categorical variables, central tendency can be calculated as well
but only a subset of measures given the reduced informational value of
categorical variables. For nominal variables where there is no
relationship between the levels the central tendency is simply the mode.
The levels of ordinal variables, however, are relational and therefore
the median, in addition to the mode, can also be used as a measure of
central tendency. Note that a variable with one mode is unimodal, two
modes, bimmodal, and in variables that have two or more modes
multimodal.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
To get numeric value of the median for an ordinal variable the levels of
the variable will need to be numeric as well. Non-numeric levels can be
recoded to numeric for this purpose if necessary.
\end{tcolorbox}

Below is a list of the central tendency metrics for the categorical
variables in the BELC dataset.

\textbf{Variable type: factor}

\begin{tabular}{l|r|r|l|r|l}
\hline
skim\_variable & n\_missing & complete\_rate & ordered & n\_unique & top\_counts\\
\hline
participant\_id & 0 & 1 & FALSE & 36 & L05: 3, L10: 3, L11: 3, L12: 3\\
\hline
age\_group & 0 & 1 & TRUE & 4 & 10-: 24, 16-: 24, 12-: 16, 17-: 15\\
\hline
sex & 0 & 1 & FALSE & 2 & fem: 48, mal: 31\\
\hline
\end{tabular}

In practice when a categorical variable has few levels it is common to
simply summarize the counts of each level in a table to get an overview
of the variable. With ordinal variables with more numerous levels, the
five-score summary (quantiles) can be useful to summarize the
distribution. In contrast to continuous variables where a graphical
representation is very helpful to get perspective on the shape of the
distribution of the values, the exploration of single categorical
variables is rarely enhanced by plots.

\hypertarget{multiple-variables}{%
\subsubsection{Multiple variables}\label{multiple-variables}}

In addition to the single variable summaries (univariate), it is very
useful to understand how two (bivariate) or more variables
(multivariate) are related to add to our understanding of the shape of
the relationships in the dataset. Just as with univariate summaries, the
informational values of the variables frame our approach.

To explore the relationship between two continuous variables we can
statistically summarize a relationship with a \textbf{coefficient of
correlation} which is a measure of \textbf{effect size} between
continuous variables. If the continuous variables approximate the normal
distribution \emph{Pearson's r} is used, if not \emph{Kendall's tau} is
the appropriate measure. A correlation coefficient ranges from -1 to 1
where 0 is no correlation and -1 or 1 is perfect correlation (either
negative or positive). Let's assess the correlation coefficient for the
variables \texttt{num\_tokens} and \texttt{ttr}. Since these variables
are not normally distributed, we use Kendall's tau. Using this measure
the correlation coefficient is \(-0.563\) suggesting there is a
correlation, but not a particularly strong one.

Correlation measures are important for reporting but to really
appreciate a relationship it is best to graphically represent the
variables in a \emph{scatterplot}. In
Figure~\ref{fig-summaries-bivariate-scatterplot-belc} we see the
relationship between \texttt{num\_tokens} and \texttt{ttr}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-bivariate-scatterplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-scatterplot-belc}Scatterplot\ldots{}}

\end{figure}

In both plots \texttt{ttr} is on the y-axis and \texttt{num\_tokens} on
the x-axis. The points correspond to the intersection between these
variables for each single observation. In the left pane only the points
are represented. Visually (and given the correlation coefficient) we can
see that there is a negative relationship between the number of tokens
and the Type-Token ratio: in other words, the more tokens a composition
has the lower the Type-Token Ratio. In this case this trend is quite
apparent, but in other cases is may not be. To provide an additional
visual cue a trend line is often added to a scatterplot. In the right
pane I've added a linear trend line. This line demarcates the optimal
central tendency across the relationship, assuming a linear
relationship. The steeper the line, or slope, the more likely the
correlation is strong. The band, or ribbon, around this trend line
indicates the \textbf{confidence interval} which means that real central
tendency could fall anywhere within this space. The wider the ribbon,
the larger the variation between the observations. In this case we see
that the ribbon widens when the number of tokens is either low or high.
This means that the trend line could be potentially be drawn either
steeper (more strongly correlated) or flatter (less strongly
correlated).

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
In plots comparing two or more variables, the choice of which variable
to plot on the x- and y-axis is contingent on the research question and/
or the statistical approach. The language varies between statistical
approaches: in inferential methods the x-axis is used to plot what is
known as the dependent variable and the y-axis an independent variable.
In predictive methods the dependent variable is known as the outcome and
the independent variable a predictor. Exploratory methods do not draw
distinctions between variables along these lines so the choice between
which variable to plot along the x- and y-axis is often arbitrary.
\end{tcolorbox}

Let's add another variable to the mix, in this case the categorical
variable \texttt{sex}, taking our bivariate exploration to a
multivariate exploration. Again each point corresponds to an observation
where the values for \texttt{num\_tokens} and \texttt{ttr} intersect.
But now each of these points is given a color that reflects which level
of \texttt{sex} it is associated with.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-multivariate-scatterplot-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-scatterplot-belc}Scatterplot
visualizing the relationship between \texttt{num\_tokens} and
\texttt{ttr}.}

\end{figure}

In this multivariate case, the scatterplot without the trend line is
more difficult to interpret. The trend lines for the levels of
\texttt{sex} help visually understand the variation of the relationship
of \texttt{num\_tokens}and \texttt{ttr} much better. But it is important
to note that when there are multiple trend lines there is more than one
slope to evaluate. The correlation coefficient can be calculated for
each level of \texttt{sex} (i.e.~`male' and `female') independently but
the relationship between the each slope can be visually inspected and
provide important information regarding each level's relative
distribution. If the trend lines are parallel (ignoring the ribbons for
the moment), as it appears in this case, this suggests that the
relationship between the continuous variables is stable across the
levels of the categorical variable, with males showing more lexical
diversity than females declining at a similar rate. If the lines were to
cross, or suggest that they would cross at some point, then there would
be a potentially important difference between the levels of the
categorical variable (known as an interaction). Now let's consider the
meaning of the ribbons. Since the ribbons reflect the range in which the
real trend line could fall, and these ribbons overlap, the differences
between the levels of our categorical variable are likely not distinct.
So at a descriptive level, this visual summary would suggest that there
are no differences between the relationship between \texttt{num\_tokens}
and \texttt{ttr} for the distinct levels of \texttt{sex}.

Characterizing the relationship between two continuous variables, as we
have seen is either performed through a correlation coefficient metric
or visually. The approach for summarizing a bivariate relationship which
combines a continuous and categorical variable is distinct. Since a
categorical variable is by definition a class-oriented variable, a
descriptive evaluation can include a tabular representation, with some
type of summary statistic. For example, if we consider the relationship
between \texttt{num\_tokens} and \texttt{age\_group} we can calculate
the mean for \texttt{num\_tokens} for each level of \texttt{age\_group}.
To provide a metric of dispersion we can include either the standard
error of the mean (SEM) and/ or the confidence interval (CI).

\hypertarget{tbl-summarize-bivariate-cont-cat-belc}{}
\begin{table}
\caption{\label{tbl-summarize-bivariate-cont-cat-belc}Summary table for tokens by age\_group. }\tabularnewline

\centering
\begin{tabular}{lrrr}
\toprule
age\_group & mean\_num\_tokens & sem & ci\\
\midrule
10-year-olds & 27.8 & 3.69 & 6.07\\
12-year-olds & 57.4 & 7.12 & 11.71\\
16-year-olds & 81.7 & 6.15 & 10.11\\
17-year-olds & 112.4 & 12.98 & 21.35\\
\bottomrule
\end{tabular}
\end{table}

The SEM is a metric which summarizes variation based on the number of
values and the CI, as we have seen, summarizes the potential range of in
which the mean may fall given a likelihood criterion (usually the same
as the \(p\)-value, .05).

Because we are assessing a categorical variable in combination with a
continuous variable a table is an available visual summary. But as I
have said before, a graphic summary is hard to beat. In the following
figure (Figure~\ref{fig-summaries-bivariate-barplot-belc}) a barplot is
provided which includes the means of \texttt{num\_tokens} for each level
of \texttt{age\_group}. The overlaid bars represent the confidence
interval for each mean score.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-bivariate-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-barplot-belc}Barplot comparing
the mean \texttt{num\_tokens} by \texttt{age\_group} from the BELC
dataset.}

\end{figure}

When CI ranges overlap, just as with ribbons in scatterplots, the
likelihood that the differences between levels are `real' is diminished.

To gauge the effect size of this relationship we can use
\emph{Spearman's rho} for rank-based coefficients. The score is 0.708
indicating that the relationship between \texttt{age\_group} and
\texttt{num\_tokens} is quite strong. \footnote{To calculate effect
  sizes for the difference between two means, \emph{Cohen's d} is used.}

Now, if we want to explore a multivariate relationship and add
\texttt{sex} to the current descriptive summary, we can create a summary
table, but let's jump straight to a barplot.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-multivariate-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-barplot-belc}Barplot
comparing the mean \texttt{num\_tokens} by \texttt{age\_group} and
\texttt{sex} from the BELC dataset.}

\end{figure}

We see in Figure~\ref{fig-summaries-multivariate-barplot-belc} that on
the whole, the appears to be general trend towards more tokens in a
composition for more advanced learner levels. However, the non-overlap
in CI bars for the `12-year-olds' for the levels of \texttt{sex} (`male'
and `female') suggest that 12-year-old females may produce more tokens
per composition than males --a potential divergence from the overall
trend.

Barplots are a familiar and common visualization for summaries of
continuous variables across levels of categorical variables, but a
boxplot is another useful visualization of this type of relationship.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-bivariate-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-boxplots-belc}Boxplot of the
relationship between \texttt{age\_group} and \texttt{num\_tokens} from
the BELC dataset.}

\end{figure}

As seen when summarizing single continuous variables, boxplots provide a
rich set of information concerning the distribution of a continuous
variable. In this case we can visually compare the continuous variable
\texttt{num\_tokens} with the categorical variable \texttt{age\_group}.
The plot in the right pane includes `notches'. Notches represent the
confidence interval, in boxplots this interval surrounds the median.
When compared horizontally across levels of a categorical variable the
overlap of notched spaces suggest that the true median may be within the
same range. Additionally, when the confidence interval goes outside the
interquantile range (the box) the notches hinge back to the either the
1st (lower) or the 3rd (higher) IQR range and suggests that the
variability is high.

We can also add a third variable to our exploration. As in the barplot
in Figure~\ref{fig-summaries-multivariate-barplot-belc}, the boxplot in
Figure~\ref{fig-summaries-multivariate-boxplots-belc} suggests that
there is an overall trend towards more tokens per composition as a
learner advances in experience, except at the `12-year-old' level where
there appears to be a difference between `males' and `females'.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-multivariate-boxplots-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-boxplots-belc}Boxplot of the
relationship between \texttt{age\_group}, \texttt{num\_tokens} and
\texttt{sex} from the BELC dataset.}

\end{figure}

Up to this point in our exploration of multiple variables we have always
included at least one continuous variable. The central tendency for
continuous variables can be summarized in multiple ways (mean, median,
and mode) and when calculating means and medians, measures of dispersion
are also provide helpful information summarize variability. When working
with categorical variables, however, measures of central tendency and
dispersion are more limited. For ordinal variables central tendency can
be summarized by the median or mode and dispersion can be assessed with
an interquantile range. For nominal variables the mode is the only
measure of central tendency and dispersion is not applicable. For this
reason relationships between categorical variables are typically
summarized using \textbf{contingency tables} which provide
cross-variable counts for each level of the target categorical
variables.

Let's explore the relationship between the categorical variables
\texttt{sex} and \texttt{age\_group}. In
Table~\ref{tbl-summaries-bivariate-categorical-table-belc} we see the
contingency table with summary counts and percentages.

\hypertarget{tbl-summaries-bivariate-categorical-table-belc}{}
\begin{table}
\caption{\label{tbl-summaries-bivariate-categorical-table-belc}Contingency table for age\_group and sex. }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
sex/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
female & 58\% (14) & 69\% (11) & 54\% (13) & 67\% (10) & 61\% (48)\\
male & 42\% (10) & 31\%  (5) & 46\% (11) & 33\%  (5) & 39\% (31)\\
Total & 100\% (24) & 100\% (16) & 100\% (24) & 100\% (15) & 100\% (79)\\
\bottomrule
\end{tabular}
\end{table}

As the size of the contingency table increases, visual inspection
becomes more difficult. As we have seen, a graphical summary often
proves more helpful to detect patterns.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-bivariate-categorical-barplot-belc-1.pdf}

}

\caption{\label{fig-summaries-bivariate-categorical-barplot-belc}Barplot\ldots{}}

\end{figure}

In Figure~\ref{fig-summaries-bivariate-categorical-barplot-belc} the
left pane shows the counts. Counts alone can be tricky to evaluate and
adjusting the barplot to account for the proportions of males to females
in each group, as shown in the right pane, provides a clearer picture of
the relationship. From these barplots we can see there were more females
in the study overall and particularly in the 12-year-olds and
17-year-olds groups. To gauge the association strength between
\texttt{sex} and \texttt{age\_group} we can calculate \emph{Cramer's V}
which, in spirit, is like our correlation coefficients for the
relationship between continuous variables. The Cramer's V score for this
relationship is 0 which is low, suggesting that there is not a strong
association between \texttt{sex} and \texttt{age\_group} --in other
words, the relationship is stable.

Let's look at a more complex case in which we have three categorical
variables. Now the dataset, as is, does not have a third categorical
variable for us to explore but we can recast the continuous
\texttt{num\_tokens} variable as a categorical variable if we bin the
scores into groups. I've binned tokens into three score groups with
equal ranges in a new variable called \texttt{rank\_tokens}.

Adding a second categorical independent variable ups the complexity of
our analysis and as a result our visualization strategy will change. Our
numerical summary will include individual two-way cross-tabulations for
each of the levels for the third variable. In this case it is often best
to use the variable with the fewest levels as the third variable, in
this case \texttt{sex}.

\hypertarget{tbl-summaries-multivariate-categorical-table-belc-female}{}
\begin{table}
\caption{\label{tbl-summaries-multivariate-categorical-table-belc-female}Contingency table for age\_group, rank\_tokens, and sex (female). }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 27\% (13) & 10\%  (5) & 4\%  (2) & 6\%  (3) & 48\% (23)\\
mid & 2\%  (1) & 13\%  (6) & 21\% (10) & 6\%  (3) & 42\% (20)\\
high & 0\%  (0) & 0\%  (0) & 2\%  (1) & 8\%  (4) & 10\%  (5)\\
Total & 29\% (14) & 23\% (11) & 27\% (13) & 21\% (10) & 100\% (48)\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tbl-summaries-multivariate-categorical-table-belc-male}{}
\begin{table}
\caption{\label{tbl-summaries-multivariate-categorical-table-belc-male}Contingency table for age\_group, rank\_tokens, and sex (male). }\tabularnewline

\centering
\begin{tabular}{llllll}
\toprule
rank\_tokens/age\_group & 10-year-olds & 12-year-olds & 16-year-olds & 17-year-olds & Total\\
\midrule
low & 32\% (10) & 13\% (4) & 13\%  (4) & 3\% (1) & 61\% (19)\\
mid & 0\%  (0) & 3\% (1) & 23\%  (7) & 6\% (2) & 32\% (10)\\
high & 0\%  (0) & 0\% (0) & 0\%  (0) & 6\% (2) & 6\%  (2)\\
Total & 32\% (10) & 16\% (5) & 35\% (11) & 16\% (5) & 100\% (31)\\
\bottomrule
\end{tabular}
\end{table}

Contingency tables with this many levels are notoriously difficult to
interpret. A plot that is often used for three-way contingency table
summaries is a mosaic plot. In
Figure~\ref{fig-summaries-multivariate-mosaic-belc} I have created a
mosaic plot for the three categorical variables in the previous
contingency tables.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-summaries-multivariate-mosaic-belc-1.pdf}

}

\caption{\label{fig-summaries-multivariate-mosaic-belc}Mosaic plot for
three categorical variables \texttt{age\_group}, \texttt{rank\_tokens},
and \texttt{sex} in the BELC dataset.}

\end{figure}

The mosaic plot suggests that the number of tokens per composition
increase as the learner age group increases and that females show more
tokens earlier.

In sum, a dataset is information but when the observations become
numerous or complex they are visually difficult to inspect and
understand at a pattern level. The descriptive methods described in this
section are indispensable for providing the researcher an overview of
the nature of each variable and any (potential) relationships between
variables in a dataset. Importantly, the understanding derived from this
exploration underlies all subsequent investigation and will counted on
to frame your approach to analysis regardless of the research goals and
the methods employed to derive more substantial knowledge.

\hypertarget{analysis}{%
\section{Analysis}\label{analysis}}

From identifying a target population, to selecting a data sample that
represents that population, and then to structuring the sample into a
dataset, the goals of a research project inform and frame the process.
So it will be unsurprising to know that the process of selecting an
approach to analysis is also intimately linked with a researcher's
objectives. The goal of analysis, generally, is to generate knowledge
from information. The type of knowledge generated and the process by
which it is generated, however, differ and can be broadly grouped into
three analysis types: inferential, predictive, and exploratory. In this
section I will provide an overview of how each of these analysis types
are tied to research goals and how the general goals of teach type
affect: (1) how to \emph{identify} the variables of interest, (2) how to
\emph{interrogate} these variables, and (3) how to \emph{interpret} the
results. I will structure the discussion of these analysis types moving
from the most structured (deductive) to least structured (inductive)
approach to deriving knowledge from information with the aim to provide
enough information to the would-be-researcher to identify these research
approaches in the literature and to make appropriate decisions as to
which approach their research should adopt.

\hypertarget{inferential-data-analysis}{%
\subsection{Inferential data analysis}\label{inferential-data-analysis}}

The most commonly recognized of the three data analysis approaches,
inferential data analysis (IDA) is the bread-and-butter of science. IDA
is a deductive, or top-down, approach to investigation in which every
step in research stems from a premise, or hypothesis, about the nature
of a relationship in the world and then aims to test whether this
relationship is statistically supported given the evidence. The aim is
to infer conclusions about a certain relationship in the population
based on a statistical evaluation of a (corpus) sample. So, if a
researcher's aim is to draw conclusions that generalize, then, this is
the analysis approach a researcher will take.

Given the fact that this approach aims at making claims that can be
generalized to the larger population, the IDA approach has the most
rigorous set of methodological restrictions. First and foremost of these
is the fact that a testable hypothesis must be formulated \emph{before}
research begins. The hypothesis guides the collection of data, the
organization of the data into a dataset and the transformation,
selection of the variables to be used to address the hypothesis, and the
interpretation of the results. To conduct an analysis and then draw a
hypothesis which conforms to the results is known as ``Hypothesis After
Result is Known'' (HARKing) (Kerr 1998) and this practice violates the
principles of significance testing. A second key stipulation is that the
reliability of the sample data, the corpus in text analysis, to provide
evidence to test the hypothesis must be representative of the
population. A corpus used in a study which is misaligned with the
hypothesis undermines the ability of the researcher to make valid claims
about the population. In essence, IDA is only as good as the primary
data is is based on.

At this point, let me elaborate on the potentially counterintuitive
nature of hypothesis formulation and testing. The IDA, or
Null-Hypothesis Significance Testing (NHST), paradigm is in fact
approached by proposing two mutually exclusive hypotheses. The first is
the \textbf{Alternative Hypothesis} (\(H_1\)). \(H_1\) is a precise
statement grounded in the previous literature outlining a predicted
relationship (and in some cases the directionality of a relationship).
This is the effect that the research aims to investigate. The second
hypothesis is the \textbf{Null Hypothesis} (\(H_0\)). \(H_0\) is the
flip-side of the hypothesis testing coin and states that there is no
difference or relationship. Together \(H_1\) and \(H_0\) cover all
logical outcomes.

So to provide an example consider a hypothetical study which is aimed at
investigating the claim that men and women differ in terms of the number
of questions they use in spontaneous conversations. The Alternative
Hypothesis would be formulated in this way:

\(H_1\): Men and women differ in the frequency of the use of questions
in spontaneous conversations.

The Null Hypothesis, then, would be a statement describing the remaining
logical outcomes. Formally:

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations.

Note that stated in this way our hypothesis makes no prediction about
the directionality of the difference between men and women, only that
there is a difference. It is a likely scenario that a hypothesis will
stake a claim on the direction of the difference. A directional
hypothesis would look like this:

\(H_1\): Women use more questions than men in spontaneous conversations.

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations or men use more questions than women.

A further aspect which may run counter to expectations is that the aim
of hypothesis testing is not to find evidence in support of \(H_1\), but
rather the aim is to assess the likelihood that we can reliably reject
\(H_0\). The default assumption is that \(H_0\) is true until there is
sufficient evidence to reject it and accept \(H_1\), the
\emph{alternative}. The metric used to determine if there is sufficient
evidence is based on the probability that given the nature of the
relationship and the characteristics of the data, the likelihood of
there being no difference or relationship is low. The threshold for
likelihood has traditionally been summarized in the p-value statistic.
In the Social Sciences, a p-value lower that .05 is considered
\emph{statistically significant} which when interpreted correctly means
that there is more than a 95\% chance that the observed relationship
would not be predicted by \(H_0\). Note that we are working in the realm
of probability, not in absolutes, therefore an analysis that produces a
significant result does not prove \(H_1\) is correct or that \(H_0\) is
incorrect, for that matter. A margin of error is always present.

Let's now turn to the identification of variables, the statistical
interrogation of these variables, and the interpretation of the
statistical results. First, since a clearly defined and testable
hypothesis is at the center of the IDA approach, the variables are in
some sense pre-defined. The goal of the researcher is to select data and
curate that data to produce variables that are operationalized
(practically measured) to test the hypothesis. A second consideration
are the roles that the variables will play in the analysis. In standard
IDA one variable will be the \textbf{dependent variable} and one or more
variables will be \textbf{independent variables}. The dependent
variable, sometimes referred to as the outcome or response variable, is
the variable which contains the information which is predicted to depend
on the information in the independent variable(s). It is the variable
whose variation a research study seeks to explain. An independent
variable, sometimes referred to as a predictor or explanatory variable,
is a variable whose variation is predicted to explain the variation in
the dependent variable.

Returning to our hypothetical study on the use of questions between men
and women in spontaneous conversation, the frequency of questions used
by each speaker would be our dependent variable and the biological sex
of the speakers our independent variable. This is so because hypothesis
(\(H_1\)) states the proposition that a speaker's sex will predict the
frequency of questions used.

In our hypothetical study we've identified two variables, one dependent
and one independent. It is important keep in mind that there can be
multiple independent variables in cases where the dependent variable's
variation is predicted to be related to multiple variables. This
relationship would need to be explicitly part of the original
hypothesis, however.

Say we formulate a more complex relationship where the educational level
of our speakers is also related to the number of questions. We can
update our hypothesis to reflect such a scenario.

\(H_1\): Less educated women use more questions than men in spontaneous
conversations.

\(H_0\): There is no difference between how men and women use questions
in spontaneous conversations regardless of educational level, or more
educated women use more questions than less educated women, or men use
more questions than women.

The hypothesis we have described predicts what is known as an
\emph{interaction}; the relationship between our independent variables
predict different variational patterns in the dependent variable. As you
most likely can appreciate the more independent variables we include in
our hypothesis, and by extension our analysis, the more difficult it
becomes to interpret. Due to the increasing difficulty for
interpretation, in practice, IDA studies rarely include more than two or
three independent variables in the same analysis.

Independent variables add to the complexity of a study because they are
part of our research focus, specifically our hypothesis. It is, however,
common to include other variables which are not of central focus, but
are commonly assumed to contribute to the explanation of the variation
of the dependent variable. Let's assume that the background literature
suggests that the age of speakers also plays a role in the number of
questions that men and women use in spontaneous conversation. Let's also
assume that the data we have collected includes information about the
age of speakers. If we would like to factor out the potential influence
of age on the use of questions and focus on the particular independent
variables we've defined in our hypothesis, we can include the age of
speakers as a \textbf{control variable}. A control variable will be
added to the statistical analysis and documented in our report but it
will not be included in the hypothesis nor interpreted in our results.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/inferential-variables.png}

}

\caption{\label{fig-aa-inferential-variables}Variable roles in
inferential analysis.}

\end{figure}

At this point let's look at the main characteristics that need to be
taken into account to statistically interrogate the variables we have
chosen to test our hypothesis. The type of statistical test that one
chooses is based on (1) the informational value of the dependent
variable and (2) the number of independent variables included in the
analysis. Together these two characteristics go a long way in
determining the appropriate class of statistical test, but other
considerations about the distribution of particular variables
(i.e.~normality), relationships between variables (i.e.~independence),
and expected directionality of the predicted effect may condition the
appropriate method to be applied.

As you can imagine, there are a host of combinations and statistical
tests that apply in particular scenarios, too many to consider in given
the scope of this coursebook (see Gries (2013) and Paquot and Gries
(2020) for a more exhaustive description). Below I've summarized some
common statistical scenarios and their associated tests which focus on
the juxtaposition of informational values and the number of variables,
leaving aside alternative tests which deal with non-normal
distributions, ordinal variables, non-independent variables, etc.

In Table~\ref{tbl-ida-statistical-monofactorial-listing} we see
\textbf{monofactorial tests}, tests with only one independent variable.

\hypertarget{tbl-ida-statistical-monofactorial-listing}{}
\begin{table}
\caption{\label{tbl-ida-statistical-monofactorial-listing}Common monofactorial tests. }\tabularnewline

\centering
\begin{tabular}{lll}
\toprule
\multicolumn{2}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-2}
Dependent & Independent & Test\\
\midrule
Categorical & Categorical & Pearson's Chi-squared test\\
Continuous & Categorical & Student's t-Test\\
Continuous & Continuous & Pearson's correlation test\\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tbl-ida-statistical-multifactorial-listing} includes a
listing of \textbf{multifactorial tests}, tests with more than one
independent and/ or control variables.

\hypertarget{tbl-ida-statistical-multifactorial-listing}{}
\begin{table}
\caption{\label{tbl-ida-statistical-multifactorial-listing}Common multifactorial tests. }\tabularnewline

\centering
\begin{tabular}{l>{}l>{}ll}
\toprule
\multicolumn{3}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-3}
Dependent & Independent & Control & Test\\
\midrule
Categorical & \em{varied} & \em{varied} & Logistic regression\\
Continuous & \em{varied} & \em{varied} & Linear regression\\
\bottomrule
\end{tabular}
\end{table}

One key point to make before we turn to how to interpret the statistical
results is concerns the use of the data in IDA. In contrast to the other
two analysis methods we will cover, the data in IDA is only used once.
That is to say, that the entire dataset is used a single time to
statistically interrogate the relationship(s) of interest. The resulting
confidence metrics (p-values, etc.) are evaluated and the findings are
interpreted. The practice of running multiple tests until a
statistically significant result is found is called ``p-hacking'' (Head
et al. 2015) and like HARKing (described earlier) violates statistical
hypothesis testing practice. For this reason it is vital to identify
your statistical approach from the outset of your research project.

Now let's consider how to approach interpreting the results from a
statistical test. As I have now made reference to multiple times, the
results of statistical procedure in hypothesis testing will result in a
confidence metric. The most standard and widely used of these confidence
metrics is the p-value. The p-value provides a probability that the
results of our statistical test could be explained by the null
hypothesis. When this probability crosses below the threshold of .05,
the result is considered statistically significant, otherwise we have a
`null result' (i.e.~non-significant). However, this sets up a binary
distinction that can be problematic. On the one hand what is one to do
if a test returns a p-value of .051 or something `marginally
significant'? According to standard practice these results would not be
statistically significant. But it is important to note that a p-value is
sensitive to the sample size. A small sample may return a
non-significant result, but a larger sample size with the same
underlying characteristics may very well return a significant result. On
the other hand, if we get a statistically significant result, do we move
on --case closed? As I just pointed out the sample size plays a role in
finding statistically significant results, but that does not mean that
the results are `important' for even small effects in large samples can
return a significant p-value.

It is important to underscore that the purpose of IDA is to draw
conclusions from a dataset which are generalizable to the population.
These conclusions require that there are rigorous measures to ensure
that the results of the analysis do not overgeneralize (suggest there is
a relationship when there is not one) and balance that with the fact
that we don't want to undergeneralize (miss the fact that there is an
relationship in the population, but our analysis was not capable of
detecting it). Overgeneralization is known as \textbf{Type I error} or
false positive and undergeneralization is a \textbf{Type II error} or
false negative.

For these reasons it is important to calculate the size and magnitude of
the result to gauge the uncertainty of our result in standardized,
sample size-independent way. This is performed by analyzing the
\textbf{effect size} and reporting a \textbf{confidence interval (CI)}
for the results. The wider the CI the more uncertainty surrounds our
statistical result, and therefore the more likely that our significant
p-value could be the result of Type I error. A non-significant p-value
and large effect size could be the result of Type II error. In addition
to vetting our p-value, the CI and effect size can help determine if a
significant result is reliable and `important'. Together effect size and
CIs aid in our ability to realistically interpret confidence metrics in
statistical hypothesis testing.

\hypertarget{predictive-data-analysis}{%
\subsection{Predictive data analysis}\label{predictive-data-analysis}}

Predictive data analysis (PDA) is the first of the two types of
statistical approaches we will cover that fall under \textbf{machine
learning}. A branch of artificial intelligence (AI), machine learning
aims to develop computer algorithms that can essentially learn patterns
from data automatically. In the case of PDA, also known as
\textbf{supervised learning}, the learning process is guided
(supervised) by directing an algorithm to associate patterns in a
variable or set of variables to single particular variable. The
particular variable is analogous to some degree to a dependent variable
in IDA, but in the machine learning literature this variable is known as
the \textbf{target} variable. The other variable or (more often than
not) variables are known as \textbf{features}. The goal of PDA is to
develop a statistical generalization that can accurately predict the
values of a target variable using the values of the feature variables.
PDA can be seen as a mix of deductive (top-down) and inductive
(bottom-up) methods in that the target variable is determined by a
research goal but the feature variables and choice of statistical method
(algorithm) are not fixed and can vary depending on their usefulness in
effectively predicting the target variable. PDA is a versatile method
that often employed to derive intelligent action from data, but it can
also be used for hypothesis generation and even hypothesis testing,
under certain conditions. If a researcher's aim is to create model that
can perform a language related task, explore association strength
between a target variable and various types and combinations of
features, or to perform emerging alternative approaches to hypothesis
testing \footnote{see Deshors and Gries (2016) and Baayen (2011)}, this
is the analysis approach a researcher will take.

At this point let's consider some departures from the inferential data
analysis (IDA) approach we covered in the last subsection that are
important to highlight to orient our overview of PDA. First, while the
cornerstone of IDA is the hypothesis, in PDA this is typically not the
case. A research question which identifies a source of potential
uncertainty in an area and outlines a strategy for addressing this
uncertainty is sufficient groundwork to embark on an analysis. A second
divergence, is the fact that the data is used in a very distinct way. In
IDA the entire dataset is statistically interrogated once and only once.
In PDA the dataset is (minimally) partitioned into a \textbf{training
set} and a \textbf{test set}. The training set is used to train a
statistical model and the test set is left to test the accuracy of the
statistical model. The training set typically constitutes a larger
portion of the data (typically around 75\%) and serves as the test bed
for iteratively applying one or more algorithms and/ or feature
combinations to produce the most successful learning model. The test set
is reserved for a final evaluation of the model's performance. Depending
on the application and the amount of available data, a third
\emph{development set} is sometimes created as a pseudo test set to
facilitate the testing of multiple approaches on data outside the
training set before the final evaluation on the test set is performed.
In this scenario the proportions of the partitions vary, but a good rule
of thumb is to reserve 60\% of the data for training, 20\% for
development, and 20\% for testing.

Let's now turn to the identification of variables, the statistical
interrogation of these variables, and the interpretation of the
statistical results. In IDA the variables (features) are pre-determined
by the hypothesis and the informational values and number of these
variables plays a significant role in selecting a statistical procedure
(algorithm). Lacking a hypothesis, a PDA approach's main goal is to make
accurate predictions on the target variable and is free to explore any
number of features and feature combinations to that end. The target
variable is the only variable which necessarily fixed and in this light
pre-determined.

To give an example, let's consider a language task in which the goal is
to take text messages (SMS) and develop a language model that predict if
a message is spam or not. Minimally we would need data which includes
individual text messages and each of these text message will need to be
labeled as being either spam or legitimate messages (`ham' in this
case). In Table~\ref{tbl-pda-sma-preview} we see the first ten of 5574
observations from the SMS Spam Collection (v.1) dataset collected by
Almeida, G'omez Hildago, and Yamakami (2011).

\hypertarget{tbl-pda-sma-preview}{}
\begin{table}
\caption{\label{tbl-pda-sma-preview}First ten observations from the SMS Spam Collection (v.1) }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
sms\_type & message\\
\midrule
ham & Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\
ham & Ok lar... Joking wif u oni...\\
spam & Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T\&C's apply 08452810075over18's\\
ham & U dun say so early hor... U c already then say...\\
ham & Nah I don't think he goes to usf, he lives around here though\\
\addlinespace
spam & FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, 1.50 to rcv\\
ham & Even my brother is not like to speak with me. They treat me like aids patent.\\
ham & As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\\
spam & WINNER!! As a valued network customer you have been selected to receivea 900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\\
spam & Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\\
\bottomrule
\end{tabular}
\end{table}

As it stands we have two variables; \texttt{sms\_type} is clearly the
target and \texttt{message} contain the full messages. The question is
how best to transform the information in the \texttt{message} variable
such that it will provide an algorithm useful information to predict
each value of \texttt{sms\_type}. Since the informational value of
\texttt{sms\_type} is categorical we will call the values
\textbf{classes}. The process of deciding on how to transform the
information in \texttt{message} into useful features is called
\textbf{feature engineering} and it is a process which is much an art as
a science. On the creative side of things it is often helpful to have a
mixture of relevant domain knowledge and clever hacking skills to
envision what features may work best. The more logistic side of things
requires some knowledge about the strengths and weaknesses of various
learning algorithms when dealing with certain number and informational
value feature combinations.

Leaving the choice of algorithm aside, let's focus on feature
engineering. Since each \texttt{message} value is a unique message, the
chance that using \texttt{message} as it is, is not likely to help us
make reliable predictions about the status of new message (`spam' or
`ham'). A simple first-pass approach to decomposing \texttt{message} to
draw out similarities and distinctions between the classes may be to
break each message into words. Now SMS messages are not your average
type of text --there are many non-standard forms. So our definition of
word may simply be character groupings broken apart by whitespace. To
avoid confusion between our common-sense understanding of word and the
types of character strings, it is often the case that language feature
values are called \textbf{terms}. Other term types may work better,
n-grams, character sequences, stems/lemmas, or even combinations of
these. Certain terms may be removed that are potentially uninformative
either based on their class (stopwords, numerals, punctuation, etc.) or
due to their distribution. The process of systematic isolation of terms
which are more informative than others is called \textbf{dimensionality
reduction} (Kowsari et al. 2019). With experience a research will become
more adept a recognizing advantages and potential issues and alternative
ways of approaching the creation of features but there is almost always
some level of trial and error in the process. Feature engineering is
very much an exploratory process. It is also iterative. You can try a
set of features with an algorithm and produce a language model and test
it on the training set --if is accurate, great. If not, you can
brainstorm some more --you are free to try further engineer the features
trying new features or feature measures (term weights) and/ or change
the learning algorithm.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/predictive-variables.png}

}

\caption{\label{fig-aa-predictive-variables}Variable roles in predictive
analysis.}

\end{figure}

Let's now turn to some considerations to take into account when
selecting a statistical algorithm. First, just as in IDA, variable
informational value plays a role in algorithm selection, specifically
the informational value of the target variable. If the target variable
is categorical, then we are looking for a \textbf{classification}
algorithm. If the target variable is continuous, we will employ a
\textbf{regression} algorithm. \footnote{The name regression can be a
  bit confusing given a very common classification algorithm is
  ``Logistic Regression''.} Some common classification algorithms are
listed in Table~\ref{tbl-pda-algorithms}.

\hypertarget{tbl-pda-algorithms}{}
\begin{table}
\caption{\label{tbl-pda-algorithms}Some common supervised learning algorithms. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Classification & Regression\\
\midrule
Logistic Regression & Linear Regression\\
Support Vector Machine & Support Vector Regression\\
Nave Bayes Classifier & Poisson Regression\\
Neural Network & \\
Decision Tree & \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}

\end{table}

\begin{table}

\end{table}

Another consideration to take into account is the whether the researcher
aims to go beyond simply using an algorithm to make accurate
predictions, but also wants to understand how the algorithm made its
predictions and what contribution features made in the process. There
are algorithms that produce models that allow the researcher to peer
into and understand its inner workings (e.g.~logistic regression, nave
bayes classifiers, inter alia) and those that do not (e.g.~neural
networks, support vector machines, inter alia). Those that do not are
called \textbf{`black-box' algorithms}. Neither type assures the best
prediction accuracy. Important trade-offs need to be considered,
however, if the best prediction comes from a black-box method, but the
goal of the research is to understand the contribution of the features
to the model's predictions.

Once we have identified our target variable, engineered a promising set
of features, and selected an algorithm to employ that meets our research
goals, it is now time to interrogate the dataset. The first step is to
partition the dataset into a training and test set. The training set is
the dataset we will use to try out different features and/ or algorithms
with the aim of developing a model which can most accurately predict the
target variable values in this training set. This is the second step and
it's done by first training an algorithm to associate the features with
the (actual) target values. Next, the resulting model is then applied to
the same training data, yet with the target variable removed, or hidden,
from the machine learner. The target values predicted by the model for
each observation are compared to the actual target values. The more
predicted and actual values for the target variable coincide, the more
accurate the model. If the model shows high accuracy, then we are ready
to move to evaluate this model on the test set (again removing the
target variable). If the model accuracy is low, it's back to the drawing
board either returning to feature engineering and/ or algorithm
selection in hopes to improve model performance. In this way, the
training data can be used multiple times, a clear divergence from
standard IDA methods in which the data is interrogated and analyzed once
and only once.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/predictive-phases.png}

}

\caption{\label{fig-aa-predictive-phases}Phases in predictive analysis.}

\end{figure}

For all applications of PDA the interpretation of the prediction model
includes some metric or metrics of accuracy comparing the extent to
which the models predictions and the actual targets align. In cases in
which the inner workings of the model are of interest, a researcher can
dive into features and their contributions to the prediction model in an
exploratory fashion according to the research goals. The exploration of
features, then, varies, so at this time let's focus on the metrics of
prediction accuracy.

The standard form for evaluating a model's performance differs between
classification models (naive bayes) and regression models (linear
regression). For classification models, a cross-tabulation of the
predicted and actual classes results in a \textbf{contingency table}
which can be used to calculate \textbf{accuracy} which is the sum of all
the correctly predicted observations divided by the total number of
observations in the test set. In addition to accuracy, there are various
other measures which aim to assess a model's performance to gain more
insight into the potential over- or under-generalization of the model
(\emph{Precision} and \emph{Recall}). For regression models, differences
between predicted and actual values can be assessed using a
\textbf{coefficient of correlation} (typically \(R^2\)). Again, more
fine-grained detail about the model's performance can be calculated
(\emph{Root Mean Square Error}).

Another component worthy of consideration when evaluating a model's
performance is how do we determine if the performance is actually good.
One the one hand, accuracy rates into the 90+\% range on the test set is
usually a good sign that the model is performing well. No model will
perform with perfect accuracy, however, and depending on the goal of the
research particular error patterns may be more important, and
problematic, than the overall prediction accuracy. On the other hand,
another eventuality is that the model performs very well on the training
set but that on the test set (new data) the performance drops
significantly. This is a sign that during the training phrase the
machine learning algorithm learned nuances in the data (`noise') that
obscure the signal pattern to be learned. This problem is called
\textbf{overfitting} and to avoid it researchers iteratively run
evaluations of the training data using resampling. The two most common
resampling methods are \textbf{bootstrapping} (resampling with
replacement) and \textbf{cross-validation} (resampling without
replacement). The performance of these multiple models are summarized
and the error between them is assessed. The goal is to minimize the
performance differences between the models while maximizing the overall
performance. These measures go a long way to avoiding overfitting and
therefore maximizing the chance that the training phase will produce a
model which is robust.

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory data analysis}\label{exploratory-data-analysis}}

The last of the three analysis types, exploratory data analysis (EDA)
includes a wide range of methods whose objective is to identify
structure in datasets using only the data itself. In this way, EDA is an
inductive, bottom-up approach to data analysis, which does not make any
formal assumptions about the relationship(s) between variables. EDA can
be roughly broken into two subgroups of analysis. \textbf{Unsupervised
learning}, like supervised learning (PDA), is a subtype of machine
learning. However, unlike prediction, unsupervised learning does not
include a target variable to guide associations. The second subgroup of
EDA methods can be seen as a (more robust) extension of the
\textbf{descriptive analysis methods} covered earlier in this chapter.
Either through unsupervised learning or descriptive methods, EDA employs
quantitative methods to summarize, reduce, and sort complex datasets and
statistically and visually interrogate a dataset in order to provide the
researcher novel perspective to be qualitatively assessed. These
qualitative assessments may prove useful to generate hypotheses or to
generate groupings to be used in predictive analyses. So, if a
researcher's aim is to probe a dataset in order to explore potential
relationships in an area where predictions and/ or hypotheses cannot be
clearly made, this is the analysis approach to choose.

In contrast to both IDA and even PDA in which there are assumptions made
about what relationship(s) to explore, EDA makes no such assumptions.
Furthermore, given the exploratory nature of the process, EDA is not an
approach which can in itself be used to make conclusive generalizations
about the populations from which the (corpus) sample in which it is
drawn. For IDA the fidelity of the sample and the process of selection
of the variables is of utmost importance to ensure that the statistical
results are reliably generalizable. Even in the case of PDA, the sample
and variables selected are key to building a robust predictive model.
However, in contrast to IDA, but similar to PDA, EDA methods may reuse
the data selecting different variables and/or methods as research goals
dictate. If a machine learning approach to EDA is adopted, the dataset
can be partitioned into training and test sets, in a similar fashion to
PDA. And as with PDA, the training set is used for refining statistical
measures and the test set is used to evaluate the refined measures.
Although the evaluation results still cannot be used to generalize, the
insight can be taken as stronger evidence that there is a potential
relationship, or set of relationships, worthy of further study.

Another notable point of contrast concerns the interpretation of EDA
results. Although quantitative in nature, exploratory methods involve a
high level of human interpretation. Human interpretation is a part of
each stage of data analysis, and each statistical approach, in general,
but exploratory methods produce results that require associative
thinking and pattern detection which is distinct from the other two
analysis approaches, in particular, IDA.

Again, as we have done for the other two analysis approaches, let's turn
to the process of variable identification, data interrogation, and
interpretation methods. As in the case of PDA, EDA only requires a
research goal. But in PDA, the research goal centered around predicting
a target variable. In EDA, there is no such focus. The research goal may
in fact be less defined and a researcher may consider various
relationships in turn or simultaneously. The curation of the variables,
however, does overlap in spirit to the process of \textbf{feature
engineering} that we touched on for creating variables for predictive
models. But in EDA the measure to gauge whether the engineered variables
are good, is left to the qualitative evaluation of the researcher.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/exploratory-variables.png}

}

\caption{\label{fig-aa-exploratory-variables}Variable roles in
exploratory analysis.}

\end{figure}

For illustrative purposes let's consider the State of the Union Corpus
(SOTU) (Benoit 2020). The presidential addresses and a set of meta-data
variables are included in the corpus. I've subsetted this corpus to only
include U.S. presidents since 1946. A tabular preview of the first 10
addresses (truncated for display) can be found in
Table~\ref{tbl-eda-sotu-corpus}.

\hypertarget{tbl-eda-sotu-corpus}{}
\begin{table}
\caption{\label{tbl-eda-sotu-corpus}First ten addresses from the SOTU Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l|l}
\hline
  & president & date & delivery & party & addresses\\
\hline
Truman-1946 & Truman & 1946-01-21 & written & Democratic & To the Congress of the United States: A quarter...\\
\hline
Truman-1947 & Truman & 1947-01-06 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman-1948 & Truman & 1948-01-07 & spoken & Democratic & Mr. President, Mr. Speaker, and Members of the ...\\
\hline
Truman-1949 & Truman & 1949-01-05 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman-1950 & Truman & 1950-01-04 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman-1951 & Truman & 1951-01-08 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman-1952 & Truman & 1952-01-09 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman-1953 & Truman & 1953-01-07 & written & Democratic & To the Congress of the United States: I have th...\\
\hline
Eisenhower-1953b & Eisenhower & 1953-02-02 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
Eisenhower-1954 & Eisenhower & 1954-01-07 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
\end{tabular}
\end{table}

A dataset such as this one could be leveraged to explore many different
types of research questions. Key to guiding the engineering of features,
however, is to clarify from the outset of the research project what the
entity of study is, or \textbf{unit of analysis}. In IDA and PDA
approaches, the unit of analysis forms an explicit part of the research
hypothesis or goal. In EDA the research question may have multiple
fronts, which may be reflected in differing units of analysis. For
example, based on the SOTU dataset, we could be interested in political
rhetoric, language of particular presidents, party ideology, etc.
Depending on the perspective we are interested in investigating, the
choice of how to approach engineering features to gain insight will
vary.

By the same token, approaches for interrogating the dataset can vary
widely, between and within the same research project, but for
instructive purposes we can draw a distinction between descriptive
methods and unsupervised learning methods.

\hypertarget{tbl-eda-approaches-table}{}
\begin{table}
\caption{\label{tbl-eda-approaches-table}Some common EDA analysis methods }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Descriptive & Unsupervised learning\\
\midrule
Term frequency analysis & Cluster analysis\\
Term keyness analysis & Topic Modeling\\
Collocation analysis & Dimensionality reduction\\
\bottomrule
\end{tabular}
\end{table}

EDA leans heavily on visual representations of both descriptive and
unsupervised learning methods. Visualizations enable humans to identify
and extrapolate associative patterns. Visualizations range from standard
barplots and scatterplots to network graphs and dendrograms and more.
Some sample visualizations based on the SOTU Corpus are found in
Figure~\ref{fig-eda-visualizations-grid}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./approaching-analysis_files/figure-pdf/fig-eda-visualizations-grid-1.pdf}

}

\caption{\label{fig-eda-visualizations-grid}Sample visualizations from
the SOTU Corpus (1946-2020).}

\end{figure}

Just as feature selection and analysis method, the interpretation of the
results in EDA are much more varied than in the other analysis methods.
EDA methods provide information which requires much more human
intervention and associative interpretation. In this way, EDA can be
seen as a quantitatively informed qualitative assessment approach. The
results from one approach can be used as the input to another. Findings
can lead to further exploration and probing of nuances in the data.
Speculative as they are the results from exploratory methods can be
highly informative and lead to new insight and inspire further study in
directions that may not have been expected.

\hypertarget{reporting}{%
\section{Reporting}\label{reporting}}

Much of the necessary reporting for an analysis features in prose as
part of the write-up of a report or article. This will include
descriptive summaries, a blueprint of the method(s) used, and the
results. Descriptive summaries will often include assessments of
individual variables and/ or relationships between variables (central
tendency, dispersion, association strength, etc.). Any procedures
applied to diagnose or to correct the data should also be included in
the final report. This information is key to helping readers assess the
results from the analysis. A blueprint of the methods used will describe
the variable selection process, how the variables were used in the
statistical analysis, and any other information that is relevant for a
reader to understand what was done and why it was done. Reporting
results from an analysis will depend on the type of analysis and the
particular method(s) employed. For inferential analyses this will
include the test statistic(s) (\(X^2\), \(R^2\), etc.) and some measure
of confidence (\(p\)-value, confidence interval, effect size). In
predictive analyses accuracy results and related information will need
to be reported. For exploratory analyses, the reporting of results will
vary and often include visualizations and metrics that require more
human interpretation than the other analysis types.

While a good article write-up will include the most vital information to
understand the procedures taken in an analysis, there are many more
details which do not traditionally appear in prose. If a research
project was conducted programmatically, however, the programming files
(scripts) used to generate the analysis can (and should) be shared.
While the scripts themselves are highly useful for other researchers to
consult and understand in fine-grained detail the steps that were taken,
it is important to also recognize that the research project should be
well documented --through organized project directory and file structure
as well as through code commenting. This description and instructions on
how to run the analysis form a \textbf{research compendium} which ensure
that the research conducted is easily understood and able to be
reproduced and/ or enhanced by other researchers.

\hypertarget{activities-2}{%
\section*{Activities}\label{activities-2}}
\addcontentsline{toc}{section}{Activities}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_4.html}{Descriptive
assessment of datasets}\\
\textbf{How}: Read Recipe 4 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore appropriate methods for summarizing variables
in datasets given the number and informational values of the
variable(s).
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_4}{Descriptive
assessment of datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 4.\\
\textbf{Why}: To identify and apply the appropriate descriptive methods
for a vector's informational value and to assess both single variables
and multiple variables with the appropriate statistical, tabular, and/
or graphical summaries.
\end{tcolorbox}

\hypertarget{summary-3}{%
\section*{Summary}\label{summary-3}}
\addcontentsline{toc}{section}{Summary}

In this chapter we have focused on description and analysis --the third
component of DIKI Hierarchy. This process is visually summarized in
Figure~\ref{fig-approaching-analysis-visual-summary-graphic}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/approaching-analysis/approaching-analysis-visual-summary-paper.png}

}

\caption{\label{fig-approaching-analysis-visual-summary-graphic}Approaching
analysis: visual summary}

\end{figure}

Building on the strategies covered in
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''} to derive a rich relational dataset, in this chapter we outlined
key points in approaching analysis. The first key step in any analysis
is to perform a descriptive assessment of the individual variables and
relationships between variables. To select the appropriate descriptive
measures we covered the various informational values that a variable can
take. In addition to providing key information for reporting purposes,
descriptive measures are important to explore so the researcher can get
a better feel for the dataset before conducting an analysis.

We covered three data analysis types in this chapter: inferential,
predictive, and exploratory. Each of these embodies very distinct
approaches to deriving knowledge from data. Ultimately the choice of
analysis type is highly dependent on the goals of the research.
Inferential analysis is centered around the goal of testing a
hypothesis, and for this reason it is the most highly structured
approach to analysis. This structure is aimed at providing the
mechanisms to draw conclusions from the results that can be generalized
to the target population. Predictive analysis has a less-ambitious but
at times more relevant goal of discovering the extent to which a given
relationship can be extrapolated from the data to provide a model of
language that can accurately predict an outcome using new data. While
many times predictive analysis is used to perform language tasks, it can
also be a highly effective methodology for applying different
algorithmic approaches and exploring relationships a target variable and
various configurations of variables. The ability to explore the data in
multiple ways, is also a key strength of employing an exploratory
analysis. The least structured and most variable of the analysis types,
exploratory analyses are a powerful approach to deriving knowledge from
data in an area where clear predictions cannot be made.

I rounded out this chapter with a short description of the importance of
reporting the metrics, procedures, and results from analysis. Reporting,
in its traditional form, is documented in prose in an article. This
reporting aims to provide the key information that a reader will need to
understand what was done, how it was done, and why it was done. This
information also provides the necessary information for reader's with a
critical eye to understand the analysis in more detail. Yet even the
most detailed reporting in a write-up still leaves many practical, but
key, points of the analysis obscured. A programming approach provides
the procedural steps taken that when shared provide the exact methods
applied. Together with the write-up a research compendium which provides
the scripts to run the analysis and documentation on how to run the
analysis forms an integral part of creating reproducible research.

\hypertarget{sec-framing-research}{%
\chapter{Framing research}\label{sec-framing-research}}

\begin{quote}
If we knew what it was we were doing, it would not be called research,
would it?

--Albert Einstein
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What are the strategies for selecting a research area and identifying
  a research problem?
\item
  How does a research problem and research aim frame the development of
  a research statement?
\item
  What is a `research blueprint' and how do the conceptual and practical
  steps involved in developing it aid the researcher as well as the
  scientific community?
\end{itemize}

\end{tcolorbox}

At this point in this part of the coursebook, we have covered Data,
Information, and Knowledge from the Data to Insight Hierarchy. The goal
has been to provide an orientation to the main building blocks of doing
text analysis. Insight is the last component of the hierarchy. However,
in practical terms, it is the first step to address in an research
project as goals of a research project influence all subsequent steps.
In this chapter we discuss how to frame research, that is how to
position your research project's findings to contribute insight to
understanding of the world. We will cover how to connect with the
literature, selecting a research area and identifying a research
problem, and how to design research best positioned to return relevant
findings that will connect with this literature, establishing a research
aim and research question. We will round out this chapter with a guide
on developing a research blueprint --a working plan to organize the
conceptual and practical steps to implement the research effectively and
in a way that supports communicating the research findings and the
process by which the findings were obtained.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Interactive programming}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Version control}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots.
\end{tcolorbox}

\hypertarget{keys-to-strong-research}{%
\section{Keys to strong research}\label{keys-to-strong-research}}

Together a research area, problem, aim and question and the research
blueprint that forms the conceptual and practical scaffolding of the
project ensure from the outset that the project is solidly grounded in
the main characteristics of good research. These characteristics,
summarized by Cross (2006), are found in
Table~\ref{tbl-fr-cross-research-char-table}.

\hypertarget{tbl-fr-cross-research-char-table}{}
\begin{table}
\caption{\label{tbl-fr-cross-research-char-table}Characteristics of research (Cross, 2006). }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
Characteristic & Description\\
\midrule
Purposive & Based on identification of an issue or problem worthy and capable of investigation\\
Inquisitive & Seeking to acquire new knowledge\\
Informed & Conducted from an awareness of previous, related research\\
Methodical & Planned and carried out in a disciplined manner\\
Communicable & Generating and reporting results which are feasible and accessible by others\\
\bottomrule
\end{tabular}
\end{table}

With these characteristics in mind, let's get started with the first
component to address --connecting with the literature.

\hypertarget{connect}{%
\section{Connect}\label{connect}}

\hypertarget{research-area}{%
\subsection{Research area}\label{research-area}}

The area of research is the first decision to make in terms of where to
make a contribution to understanding. At this point, the aim is to
identify a general area of interest where a researcher wants to derive
insight. For those with an established research trajectory in language,
the area of research to address through text analysis will likely be an
extension of their prior work. For others, which include new researchers
or researcher's that want to explore new areas of language research or
approach an area through a language-based lens, the choice of area may
be less obvious. In either case, the choice of a research area should be
guided by a desire to contribute something relevant to a theoretical,
social, and/ or practical matter of personal interest. Personal
relevance goes a long way to developing and carrying out
\textbf{purposive} and \textbf{inquisitive} research.

So how do we get started? The first step is to reflect on your own areas
of interest and knowledge, be it academic, professional, or personal.
Language is at the heart of the human experience and therefore found in
some fashion anywhere one seeks to find it. But it is a big world and
more often than not the general question about what area to explore
language use is sometimes the most difficult. To get the ball rolling,
it is helpful to peruse disciplinary encyclopedias or handbooks of
linguistics and language-related an academic fields
(e.g.~\href{https://www.sciencedirect.com/referencework/9780080448541/encyclopedia-of-language-and-linguistics}{Encyclopedia
of Language and Linguistics} (Brown 2005),
\href{https://www.sciencedirect.com/book/9781843345978/a-practical-guide-to-electronic-resources-in-the-humanities}{A
Practical Guide to Electronic Resources in the Humanities} (Dubnjakovic
and Tomlin 2010),
\href{https://www.routledgehandbooks.com/doi/10.4324/9781315749129}{Routledge
encyclopedia of translation technology} (Chan 2014))

A more personal, less academic, approach is to consult online forums,
blogs, etc. that one already frequents or can be accessed via an online
search. For example, \href{https://www.reddit.com/}{Reddit} has a wide
variety of active subreddits
(\href{https://www.reddit.com/r/LanguageTechnology/}{r/LanguageTechnology},
\href{https://www.reddit.com/r/linguistics/}{r/Linguistics},
\href{https://www.reddit.com/r/corpuslinguistics/}{r/corpuslinguistics},
\href{https://www.reddit.com/r/DigitalHumanities/}{r/DigitalHumanities},
etc.). Twitter and Facebook also have interesting posts on linguistics
and language-related fields worth following. Through one of these social
media site you may find particular people that maintain a blog worth
browsing. For example, I follow \href{https://juliasilge.com/}{Julia
Silge}, \href{http://www.rctatman.com/}{Rachel Tatman}, and
\href{https://tedunderwood.com/}{Ted Underwood}, inter alia. Perusing
these resources can help spark ideas and highlight the kinds of
questions that interest you.

Regardless of whether your inquiry stems from academic, professional, or
personal interest, try to connect these findings to academic areas of
research. Academic research is highly structured and well-documented and
making associations with this network will aid in subsequent steps in
developing a research project.

\hypertarget{research-problem}{%
\subsection{Research problem}\label{research-problem}}

Once you've made a rough-cut decision about the area of research, it is
now time to take a deeper dive into the subject area and jump into the
literature. This is where the rich structure of disciplinary research
will provide aid to traverse the vast world of academic knowledge and
identify a research problem. A research problem highlights a particular
topic of debate or uncertainty in existing knowledge which is worthy of
study.

Surveying the relevant literature is key to ensuring that your research
is \textbf{informed}, that is, connected to previous work. Identifying
relevant research to consult can be a bit of a `chicken or the egg'
problem --some knowledge of the area is necessary to find relevant
topics, some knowledge of the topics is necessary to narrow the area of
research. Many times the only way forward is to jump in conducting
searches. These can be world-accessible resources
(e.g.~\href{https://scholar.google.com/}{Google Scholar}) or
limited-access resources that are provided through an academic
institution
(e.g.~\href{https://about.proquest.com/en/products-services/llba-set-c}{Linguistics
and Language Behavior Abstracts}), \href{https://eric.ed.gov/}{ERIC},
\href{https://www.ebsco.com/products/research-databases/apa-psycinfo}{PsycINFO},
etc.). Some organizations and academic institutions provide
\href{https://guides.zsr.wfu.edu/linguistics}{research guides} to help
researcher's access the primary literature.

Another avenue to explore are journals dedicated to areas in which
linguistics and language-related research is published. In the following
tables I've listed a number of highly visible journals in linguistics,
digital humanities, and computational linguistics.

\hypertarget{tbl-pinboard-journals-linguistics}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-linguistics}A list of some linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://www.euppublishing.com/loi/cor">Corpora</a> & An international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\\
\hline
<a href="https://www.degruyter.com/journal/key/CLLT/html">Corpus Linguistics and Linguistic Theory</a> & Corpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\\
\hline
<a href="https://benjamins.com/catalog/ijcl">International Journal of Corpus Linguistics</a> & The International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\\
\hline
<a href="http://ijls.net/">International Journal of Language Studies</a> & It is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-child-language">Journal of Child Language</a> & A key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-linguistic-geography/all-issues">Journal of Linguistic Geography</a> & The Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\\
\hline
<a href="http://www.tandfonline.com/toc/njql20/current">Journal of Quantitative Linguistics</a> & Publishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-humanities}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-humanities}A list of some humanities journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.digitalhumanities.org/dhq/">Digital Humanities Quarterly</a> & Digital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\\
\hline
<a href="https://academic.oup.com/dsh">Digital Scholarship in the Humanities</a> & DSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\\
\hline
<a href="https://culturalanalytics.org/">Journal of Cultural Analytics</a> & Cultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-cl}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-cl}A list of some computational linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://direct.mit.edu/coli">Computational Linguistics</a> & Computational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\\
\hline
<a href="http://lrec-conf.org/">LREC Conferences</a> & The International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\\
\hline
<a href="https://transacl.org/index.php/tacl/index">Transactions of the Association for Computational Linguistics</a> & Transactions of the Association for Computational Linguistics (TACL) is anACL-sponsored journalpublished by MIT Pressthat publishes papers in all areas of computational linguistics and natural language processing.\\
\hline
\end{tabular}
\end{table}

To explore research related to text analysis it is helpful to start with
the (sub)discipline name(s) you identified in when selecting your
research area, more specific terms that occur to you or key terms from
the literature, and terms such as `corpus study' or `corpus-based'. The
results from first searches may not turn out to be sources that end up
figuring explicitly in your research, but it is important to skim these
results and the publications themselves to mine information that can be
useful to formulate better and more targeted searches. Relevant
information for honing your searches can be found throughout an academic
publication (article or book). However, pay particular attention to the
abstract, in articles, and the table of contents, in books, and the
cited references. Abstracts and tables of contents often include
discipline-specific jargon that is commonly used in the field. In some
articles there is even a short list of key terms listed below the
abstract which can be extremely useful to seed better and more precise
search results. The references section will contain relevant and
influential research. Scan these references for publications which
appear to narrowing in on topic of interest and treat it like a search
in its own right.

Once your searches begin to show promising results it is time to keep
track and organize these references. Whether you plan to collect
thousands of references over a lifetime of academic research or your aim
is centered around one project, software such as
\href{https://www.zotero.org/}{Zotero}\footnote{\href{https://guides.zsr.wfu.edu/zotero}{Zotero
  Guide}},
\href{https://www.mendeley.com/reference-management/reference-manager}{Mendeley},
or \href{https://bibdesk.sourceforge.io/}{BibDesk} provide powerful,
flexible, and easy-to-use tools to collect, organize, annotate, search,
and export references. Citation management software is indispensable for
modern research --and often free!

As your list of relevant references grows, you will want to start the
investigation process in earnest. Begin skimming (not reading) the
contents of each of these publications, starting with the most relevant
first\footnote{Or what appears to be most relevant. This may change as
  you start to take a closer look.}. Annotate these publications using
highlighting features of the citation management software to identify:
(1) the stated goal(s) of the research, (2) the data source(s) used, (3)
the information drawn from the data source(s), (4) the analysis approach
employed, and (5) the main finding(s) of the research as they pertain to
the stated goal(s). Next, in your own words, summarize these five key
areas in prose adding your summary to the notes feature of the citation
management software. This process will allow you to efficiently gather
and document references with the relevant information to guide the
identification of a research problem, to guide the formation of your
problem statement, and ultimately, to support the literature review that
will figure in your project write-up.

From your preliminary annotated summaries you will undoubtedly start to
recognize overlapping and contrasting aspects in the research
literature. These aspects may be topical, theoretical, methodological,
or appear along other lines. Note these aspects and continue to conduct
more refine searches, annotate new references, and monitor for any
emerging patterns of uncertainty or debate (gaps) which align with your
research interest(s). When a promising pattern takes shape, it is time
to engage with a more detailed reading of those references which appear
most relevant highlighting the potential gap(s) in the literature. At
this point you can focus energy on more nuanced aspects of a particular
gap in the literature with the goal to formulate a problem statement. A
problem statement directly acknowledges a gap in the literature and puts
a finer point on the nature and relevance of this gap for understanding.
This statement reflects your first deliberate attempt to establish a
line of inquiry. It will be a targeted, but still somewhat general,
statement framing the gap in the literature that will guide subsequent
research design decisions.

\hypertarget{findings}{%
\section{Findings}\label{findings}}

\hypertarget{research-aim}{%
\subsection{Research aim}\label{research-aim}}

With a problem statement in hand, it is now time to consider the goal(s)
of the research. A research aim frames the type of inquiry to be
conducted. Will the research aim to explain, evaluate, or explore? In
other words, will the research seek to test a particular relationship,
assess the potential strength of a particular relationship, or uncover
novel relationships? As you can appreciate, the research aim is directly
related to the analysis methods we touched upon in
\protect\hyperlink{sec-approaching-analysis}{Chapter 3}.

To gauge how to frame your research aim, reflect on the literature that
led you to your problem statement and the nature of the problem
statement itself. If the gap at the center of the problem statement is a
lack of knowledge, your research aim may be exploratory. If the gap
concerns a conjecture about a relationship, then your research may take
a predictive approach. When the gap points to the validation of a
relationship, then your research will likely be inferential in nature.
Before selecting your research aim it is also helpful to consult the
research aims of the primary literature that led you to your research
statement. Consider how your research statement relates the previous
literature. Do you aim to test a hypothesis based on previous
exploratory analyses? Are you looking to generate new knowledge in an
(apparently) uncharted area?

In general, a problem statement which addresses a smaller, nuanced gap
will tend to adopt similar research aims as the previous literature
while a larger, more divergent gap will tend to adopt a distinct
research aim. This is not a hard rule, but more of a heuristic, however,
and it is important to be familiar with both the previous literature,
the nature of different types of analysis, and the goals of the research
to ensure that the research is best-positioned to generate findings that
will contribute to the existing body of understanding in a principled
way.

\hypertarget{research-question}{%
\subsection{Research question}\label{research-question}}

The next step in research design is to craft the research question. A
research question is clearly defined statement which identifies an
aspect of uncertainty and the particular relationships that this
uncertainty concerns. The research question extends and narrows the line
of inquiry established in the research statement and research aim. The
research statement can be seen as the content and the research aim as
the form.

The form of a research question will vary based on the analysis
approach. For inferential-based research, the research question will
actually be a statement, not a question. This statement makes a testable
claim about the nature of a particular relationship --i.e.~asserts a
hypothesis. For illustration, let's return to one of the hypotheses we
previously sketched out in
\protect\hyperlink{sec-approaching-analysis}{Chapter 3}, leaving aside
the implicit null hypothesis.

Women use more questions than men in spontaneous conversations.

For predictive- and exploratory-based research, the research question is
in fact a question. A reframing of the example hypothesis for a
predictive-based research question might looks something like this.

Can the number of questions used in spontaneous conversations predict if
a speaker is male or female?

And a similar exploratory-based research question would take this form.

Do men and women differ in terms of the number of questions they use in
spontaneous conversations?

The central research interest behind these hypothetical research
questions is, admittedly, quite basic. But from these simplified
examples, we are able to appreciate the similarities and differences
between the forms of research statements that correspond to distinct
research aims.

In terms of content, the research question will make reference to two
key components. First, is the \textbf{unit of analysis}. The unit of
analysis is the entity which the research aims to investigate. For our
three example research aims, the unit of analysis is the same, namely
men and women. Note, however, that the current unit of analysis is
somewhat vague in the example research questions. A more precise unit of
analysis would include more information about the population from which
the men and women are drawn (e.g English speakers, American English
speakers, American English speakers of the Southeast, etc.).

The second key component is the \textbf{unit of observation}. The unit
of observation is the primary element on which the insight into the unit
of analysis is derived and in this way constitutes the essential
organization unit of the data to be collected. In our examples, the unit
of observation, again, is unchanged and is spontaneous conversations.
Note that while the unit of observation is key to identify as it forms
the organizational backbone of the research, it is very common for the
research to derive variables from this unit to provide evidence to
investigate the research question. In the previous examples, we
identified the number of conversations as part of the research question.
But in other cases a researcher may seek to understand other aspects of
questions in spontaneous conversations (i.e type of question, features
of questions, etc.). The unit of observation, however, would remain the
same.

\hypertarget{blueprint}{%
\section{Blueprint}\label{blueprint}}

Efforts to craft a research question are a very important aspect of
developing purposive, inquisitive, and informed research (returning to
Cross's characteristics of research). Moving beyond the research
question in the project means developing and laying out the research
design in a way such that the research is \textbf{Methodical} and
\textbf{Communicable}. In this coursebook, the method to achieve these
goals is through the development of a research blueprint. The blueprint
includes two components: (1) the process of identifying the data,
information, and methods to be used and (2) the creation of a plan to
structure and document the project.

As Ignatow and Mihalcea (2017) point out:

\begin{quote}
Research design is essentially concerned with the basic architecture of
research projects, with designing projects as systems that allow theory,
data, and research methods to interface in such a way as to maximize a
project's ability to achieve its goals {[}\ldots{]}. Research design
involves a sequence of decisions that have to be taken in a project's
early stages, when one oversight or poor decision can lead to results
that are ultimately trivial or untrustworthy. Thus, it is critically
important to think carefully and systematically about research design
before committing time and resources to acquiring texts or mastering
software packages or programming languages for your text mining project.
\end{quote}

\hypertarget{identify}{%
\subsection{Identify}\label{identify}}

Importance of identifying and documenting the key aspects required to
conduct the research cannot be understated. On the one hand this process
links concept to implementation. In doing so, a researcher is
better-positioned to conduct research with a clear view of what will be
entailed. On the other hand, a promising research question, on paper,
may present challenges that may require modification or reevaluation of
the viability of the project. It is not uncommon to encounter roadblocks
or even dead-ends for moving a well-founded research question forward
when considering the available data, a researcher's (current) technical
and/ or research skills, and the given time frame for the project. In
practice, the process of identifying the data, information, and methods
of analysis are considered in tandem with the investigative work to
develop a research aim and research question. In this subsection I will
cover the main characteristics to consider when developing a research
blueprint.

The first, and most important, part of establishing a research blueprint
is to \textbf{identify a viable data source}. Regardless of how you find
and access the data, it is essential to vet the corpus sample in light
of the research question. In the case that research is inferential in
nature, the sampling frame of the corpus is of primary importance as the
goal is to generalize the findings to a target population. A corpus
resource should align, to the extent feasible, with this target
population. For predictive and exploratory research, the goal to
generalize a claim is not central and for this reason the there is some
freedom in terms of how representative a corpus sample is of a target
population. Ideally a researcher will find and be able to model a
language population of target interest. Since the goal, however, is not
to test a hypothesis, but rather to explore particular or potential
relationships, either in an predictive or exploratory fashion, the
research can often continue with the stipulation that the results are
interpreted in the light of the characteristic of the available corpus
sample.

The second step is to \textbf{identify the key variables} need to
conduct the research are and then ensure that this information can be
derived from the corpus data. The research question will reference the
unit of analysis and the unit of observation, but it is important at
this point to then pinpoint what the key variables will be. If the unit
of observation is spontaneous conversations. The question as to what
aspects of these conversations will be used in the analysis. In the
research questions presented in this chapter, we will want to envision
what needs to be done to generate a variable which measures the number
of questions in each of the conversations. In other research, their may
be features that need to be extracted and recoded to address the
research question. Other variables of importance may be non-linguistic
in nature. Provided the corpus has the required meta-data for the
research, variables can be normalized, recoded, and generated from the
corpus itself to fit research needs. In cases where there the meta-data
is incomplete for the goals of the research, it is sometimes possible to
merge meta-data from other sources.

The third step is to \textbf{identify a method of analysis}. The
selection of the analysis approach that was part of the research aim and
then the research question goes a long way to narrowing the methods that
a researcher must consider. But there are a number of factors which will
make some methods more appropriate than others. In inferential research,
the number and information values of the variables to be analyzed will
be of key importance (Gries 2013). The informational value of the
dependent variable will again narrow the search for the appropriate
method. The number of independent variables also plays an important
role. For example, a study with a categorical dependent variable with a
single categorical independent variable will lead the researcher to the
Chi-squared test. A study with a continuous dependent variable with
multiple independent variables will lead to linear regression. Another
aspect of note for inference studies is the consideration of the
distribution of continuous variables --a normal distribution will use a
parametric test where a non-normal distribution will use a
non-parametric test. These details need not be nailed down at this
point, but it is helpful to have them on your radar to ensure that when
the time comes to analyze the data, the appropriate steps are taken to
test for normality and then apply the correct test.

For predictive-based research, the informational value of the target
variable is key to deciding whether the prediction will be a
classification task or a numeric prediction task. This has downstream
effects when it comes time to evaluate and interpret the results.
Although the feature engineering process in predictive analyses means
that the features do not need to be specified from the outset and can be
tweaked and changed as needed during an analysis, it is a good idea to
start with a basic sense of what features most likely will be helpful in
developing a robust predictive model. Furthermore, while the number and
informational values of the features (predictor variables) are not as
important to selecting a prediction method (algorithm) as they are in
inferential analysis methods, it is important to recognize that
algorithms have strengths and shortcomings when working large numbers
and/ or types of features (Lantz 2013).

Exploratory research is the least restricted of the three types of
analysis approaches. Although it may be the case that a research will
not be able to specify from the outset of a project what the exact
analysis methods will be, an attempt to consider what types of analysis
methods will be most promising to provide results to address the
research question goes a long way to steering a project in the right
direction and grounding the research. As with the other analysis
approaches, it is important to be aware of what the analysis methods
available and what type of information they produce in light of the
research question.

In sum, the identification of the data, information, and analysis
methods that will be used in the proposed research are key to ensuring
the research is viable. Be sure to document this process in prose and
describe the strengths and potential shortcomings of (1) the corpus data
selected, (2) the information to be extracted for analysis, and (3) the
analysis method(s) that are appropriate for the research aim and what
the evaluation method will be. Furthermore, not every eventuality can be
foreseen. It is helpful to include a description of aspects of this
process which may pose challenges and to include potential contingency
plans as part of this prose description.

\hypertarget{plan}{%
\subsection{Plan}\label{plan}}

The next step in creating a research blueprint is to consider how to
physically implement your project. This includes how to organize files
and directories in a fashion that both provides the researcher a logical
and predictable structure to work with but also ensures that the
research is \textbf{Communicable}. On the one hand, communicable
research includes a strong write-up of the research, but, on the other
hand, it is also important that the research is reproducible.
Reproducibility strategies are a benefit to the researcher (in the
moment and in the future) as it leads to better work habits and to
better teamwork and it makes changes to the project easier.
Reproducibility is also of benefit to the scientific community as shared
reproducible research enhances replicability and encourages cumulative
knowledge development (Gandrud 2015).

There are a set of guiding principles to accomplish these goals
(Gentleman and Temple Lang 2007; Marwick, Boettiger, and Mullen 2018).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files should be plain text which means they contain no formatting
  information other than whitespace.
\item
  There should be a clear separation between the data, method, and
  output of research. This should be apparent from the directory
  structure.
\item
  A separation between original data and derived data should be made.
  Original data should be treated as `read-only'. Any changes to the
  original data should be justified, generated by the code, and
  documented (see point 6).
\item
  Each analysis file (script) should represent a particular,
  well-defined step in the research process.
\item
  Each analysis script should be modular --that is, each file should
  correspond to a specific goal in the analysis procedure with input and
  output only corresponding to this step.
\item
  All analysis scripts should be tied together by a `master' script that
  is used to coordinate the execution of all the analysis steps.
\item
  Everything should be documented. This includes analysis steps, script
  code comments, data description in data dictionaries, information
  about the computing environment and packages used to conduct the
  analysis, and detailed instructions on how to reproduce the research.
\end{enumerate}

These seven principles can be physically implemented in countless ways.
In recent years, there has been a growing number of efforts to create R
packages and templates to quickly generate the scaffolding and tools to
facilitate reproducible research. Some notable R packages include
\href{https://jdblischak.github.io/workflowr/}{workflowr} and
\href{http://projecttemplate.net/}{ProjectTemplate} but there are many
other resources for R included on the
\href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{CRAN
Task View for Reproducible Research}. There are many advantages to
working with pre-existing frameworks for the savvy R programmer.

In this coursebook, however, I have developed a project template
(\href{https://github.com/lin380/project_template}{available on GitHub})
which I believe simplifies and makes the process more transparent for
beginning and intermediate R programmers, the directory structure is
provided below.

\begin{verbatim}
#> ../project_template/
#> +-- README.md
#> +-- _pipeline.R
#> +-- analysis
#> |   +-- 1_acquire_data.Rmd
#> |   +-- 2_curate_dataset.Rmd
#> |   +-- 3_transform_dataset.Rmd
#> |   +-- 4_analyze_dataset.Rmd
#> |   +-- 5_generate_article.Rmd
#> |   +-- _session-info.Rmd
#> |   +-- _site.yml
#> |   +-- index.Rmd
#> |   \-- references.bib
#> +-- data
#> |   +-- derived
#> |   \-- original
#> \-- output
#>     +-- figures
#>     \-- results
\end{verbatim}

Let me now describe how this template structure aligns with the seven
principles of quality reproducible research.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files are plain text (e.g.~\texttt{.R}, \texttt{.Rmd},
  \texttt{.csv}, \texttt{.txt}, etc.).
\item
  There are three main directories \texttt{analysis/}, \texttt{data/},
  and \texttt{ouput/}.
\item
  The \texttt{data/} directory contains sub-directories for
  \texttt{original} (`read-only') data and \texttt{derived} data.
\item
  The \texttt{analysis/} directory contains five scripts which are
  numbered to correspond with their sequential role in the research
  process.
\item
  Each of these analysis scripts are designed to be modular; input and
  output must be explicit and no intermediate objects are carried over
  to other analysis scripts. Dataset output should be written to and
  read from the \texttt{data/derived/} directory. Figures and
  statistical results should be written to and read from
  \texttt{output/figures/} and \texttt{output/results} respectively.
\item
  All of the analysis scripts, and therefore the entire project, are
  tied to the \texttt{\_pipeline.R} script. To reproduce the entire
  project only this script need be run.
\item
  Documentation takes place at many levels. The \texttt{README.md} file
  is the first file that a researcher will consult. It contains a brief
  description of the project goals and how to reproduce the analysis.
  Analysis scripts use the Rmarkdown format (\texttt{.Rmd}). This format
  allows researchers to interleave prose description and executable code
  in the same script. This ensures that the rationale for the steps
  taken are described in prose, the code is made available to consult,
  and that code comments can be added to every line. The
  \texttt{\_sesssion-info.Rmd} script is merged with each analysis
  script to provide information about the computing environment and
  packages used to conduct each step analysis. As this is a template, no
  data or datasets appear. However, once data is acquired and that data
  is curated and transformed, documentation for these resources should
  be documented for each resource in a data dictionary along side the
  data(set) itself.
\end{enumerate}

The aspects of the project template described in points 1-7 together
form the backbone for reproducible research. This template, however,
includes additional functionality to enhance efficient and communicable
research. The \texttt{\_pipeline.R} script executes the analysis scripts
in the \texttt{analysis} directory, but as a side effect also produces
\href{https://lin380.github.io/project_template_demo/}{a working
website} and a journal-ready article for publishing your analysis,
results, and findings to the web in
\href{https://lin380.github.io/project_template_demo/5_generate_article.html}{HTML}
and
\href{https://lin380.github.io/project_template_demo/article.pdf}{PDF}
format. The \texttt{index.Rmd} file is the splash page for the website
and is a good place to house your pre-analysis investigative work
including your research area, problem, aim, and question and to document
your research blueprint including the identification of viable data
resource(s), the key variables for the analysis, the analysis method,
and the method of assessment. All Rmarkdown files provide functionality
for citing and organizing references. The \texttt{references.bib} file
is where references are stored and can be used to include citations that
support your research throughout your project.

\hypertarget{prepare}{%
\subsection{Prepare}\label{prepare}}

This template will allow you to organize your research design and align
it with implementation steps to conduct quality reproducible research.
To prepare for your analysis, you will need to download or fork and
clone this template from the GitHub repository and then make some
adjustments to personalize this template for your research.

To create a local copy of this project template either:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download and decompress the
  \href{https://github.com/lin380/project_template/archive/refs/heads/main.zip}{.zip
  file}
\item
  If you have \href{https://github.com/git-guides/install-git}{git}
  installed on your machine and a
  \href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{GitHub
  account},
  \href{https://docs.github.com/en/get-started/quickstart/fork-a-repo\#forking-a-repository}{fork
  the repository} to your own GitHub account. Then open a terminal in
  the desired location and
  \href{https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository-from-github/cloning-a-repository\#cloning-a-repository}{clone
  the repository}. If you are using RStudio, you can setup a new RStudio
  Project with the clone using the `New Project\ldots{}' dialog,
  choosing `Version Control', and following the steps.
\end{enumerate}

Before you begin configuring and adding your project-specific details to
this template. Reproduce this project `as-is' to confirm that it builds
on your local machine.

In RStudio or in R session in a Terminal application, open the console
in the root directory of the project. Then run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{source}\NormalTok{(}\StringTok{"\_pipeline.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It will take some time to complete, when it does the prompt
(\texttt{\textgreater{}}) in the console will return. Then navigate to
and open \texttt{docs/index.html} in a browser.

Once you have confirmed that the project template builds, then you can
begin to configure the template to reflect your project. There a few
files to consider first. These files are places where the title of your
project should appear.

\begin{itemize}
\tightlist
\item
  \texttt{README.md}
\item
  \texttt{\_pipeline.R}
\item
  \texttt{analysis/index.Rmd}
\end{itemize}

After updating these files, build the project again and make sure that
the new changes appear as you would like them. You are now ready to
start your research project!

\hypertarget{activities-3}{%
\section*{Activities}\label{activities-3}}
\addcontentsline{toc}{section}{Activities}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_5.html}{Project
management with Git, GitHub, and RStudio Cloud}\\
\textbf{How}: Read Recipe 5 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To learn how to use Git, GitHub, and RStudio to manage,
store, and publish reproducible research projects.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_5}{Project management
with Git, GitHub, and RStudio Cloud}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 5.\\
\textbf{Why}: To set up a GitHub account, fork and copy a GitHub
repository to RStudio Cloud, and use R, Git, and GitHub to manage,
store, and publish changes to a reproducible research project.
\end{tcolorbox}

\hypertarget{summary-4}{%
\section*{Summary}\label{summary-4}}
\addcontentsline{toc}{section}{Summary}

The aim of this chapter is to provide the key conceptual and practical
points to guide the development of a viable research project. Good
research is purposive, inquisitive, informed, methodological, and
communicable. It is not, however, always a linear process. Exploring
your area(s) of interest and connecting with existing work will help
couch and refine your research. But practical considerations, such as
the existence of viable data, technical skills, and/ or time constrains,
sometimes pose challenges and require a researcher to rethink and/ or
redirect the research in sometimes small and other times more
significant ways. The process of formulating a research question and
developing a viable research plan is key to supporting viable,
successful, and insightful research. To ensure that the effort to derive
insight from data is of most value to the researcher and the research
community, the research should strive to be methodological and
communicable adopting best practices for reproducible research.

This chapter concludes the Orientation section of this coursebook. At
this point the fundamental characteristics of research are in place to
move a project towards implementation. The next section, Preparation,
aims to cover the acquisition, curation, and transformation of data in
preparation for analysis. These are the first steps in putting a
research blueprint into action and by no coincidence the first
components in the Data to Insight Hierarchy. Following the Preparation
section our attention will turn to the implementation of the three
analysis approaches we have covered: inference, prediction, and
exploration. Throughout these next sections we will maintain our aim to
develop methodological and communicable research by connecting our
implementation process to reproducible programming strategies.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/framing-research/framing-research-visual-summary.png}

}

\caption{\label{fig-framing-research-visual-summary-graphic}Framing
research: visual summary}

\end{figure}

\part{Preparation}

At this point we will turn our attention to implementing the specifics
outlined in our research blueprint. This section will group the
components which concern the acquisition, curation, and transformation
of data into a dataset which is prepared to be submitted to analysis. In
each of these three chapters I will outline some of the main
characteristics to consider in each of these research steps and provide
authentic examples of working with R to implement these steps. In
\protect\hyperlink{acquire-data}{Chapter 5} this includes downloads,
working with APIs, and webscraping. In
\protect\hyperlink{curate-data}{Chapter 6} we turn to organize data into
rectangular, or `tidy', format. Depending on the data or dataset
acquired for the research project, the steps necessary to shape our data
into a base dataset will vary, as we will see. In
\protect\hyperlink{transform-data}{Chapter 7} we will work to manipulate
curated datasets to create datasets which are aligned with the research
aim and research question. This often includes normalizing values,
recoding variables, and generating new variables as well as and sourcing
and merging information from other datasets with the dataset to be
submitted for analysis.

\hypertarget{sec-acquire-data}{%
\chapter{Acquire data}\label{sec-acquire-data}}

\begin{quote}
The scariest moment is always just before you start.

--Stephen King
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  What are the most common strategies for acquiring corpus data?
\item
  What programmatic steps can we take to ensure the acquisition process
  is reproducible?
\item
  What is the importance of documenting data?
\end{itemize}

\end{tcolorbox}

There are three main ways to acquire corpus data using R that I will
introduce you to: \textbf{downloads}, \textbf{APIs}, and \textbf{web
scraping}. In this chapter we will start by manual and programmatically
downloading a corpus as it is the most straightforward process for the
novice R programmer and typically incurs the least number of steps.
Along the way I will introduce some key R coding concepts including
control statements and custom functions. Next I will cover using R
packages to interface with APIs, both open-access and
authentication-based. APIs will require us to delve into more detail
about R objects and custom functions. Finally acquiring data from the
web via webscraping is the most idiosyncratic and involves both
knowledge of the web, more sophisticated R skills, and often some clever
hacking skills. I will start with a crash course on the structure of web
documents (HTML) and then scale up to a real-world example. To round out
the chapter we will cover the process of ensuring that our data is
documented in such a way as to provide sufficient information to
understand its key sampling characteristics and the source from which it
was drawn.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Swirl}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/swirl}{Loops and
vectorization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To \ldots.
\end{tcolorbox}

\hypertarget{downloads}{%
\section{Downloads}\label{downloads}}

\hypertarget{manual}{%
\subsection{Manual}\label{manual}}

The first acquisition method I will cover here is inherently
non-reproducible from the standpoint that the programming implementation
cannot acquire the data based solely on running the project code itself.
In other words, it requires manual intervention. Manual downloads are
typical for data resources which are not openly accessible on the public
facing web. These can be resources that require institutional or private
licensing (\href{https://www.ldc.upenn.edu/}{Language Data Consortium},
\href{http://ice-corpora.net/ice/}{International Corpus of English},
\href{https://www.corpusdata.org/}{BYU Corpora}, etc.), require
authorization/ registration (\href{https://archive.mpi.nl/tla/}{The
Language Archive}, \href{https://www.webcorpora.org/}{COW Corpora},
etc.), and/ or are only accessible via resource search interfaces
(\href{https://cesa.arizona.edu/}{Corpus of Spanish in Southern
Arizona}, \href{http://cedel2.learnercorpora.com/}{Corpus Escrito del
Espaol como L2 (CEDEL2)}, etc.).

Let's work with the CEDEL2 corpus (Lozano 2009) which provides a search
interface and open access to the data through the search interface. The
homepage can be seen in Figure~\ref{fig-ad-show-page-cedel2-1}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-cedel2-site.png}

}

\caption{\label{fig-ad-show-page-cedel2-1}CEDEL2 Corpus homepage}

\end{figure}

Following the search/ download link you can find a search interface that
allows the user to select the sub-corpus of interest. I've selected the
subcorpus ``Learners of L2 Spanish'' and specified the L1 as English.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-cedel2-search-download.png}

}

\caption{\label{fig-ad-show-page-cedel2-2}Search and download interface
for the CEDEL2 Corpus}

\end{figure}

The `Download' link now appears for this search criteria. Following this
link will provide the user a form to fill out. This particular resource
allows for access to different formats to download (Texts only, Texts
with metadata, CSV (Excel), CSV (Others)). I will select the `CSV
(Others)' option so that the data is structured for easier processing
downstream when we work to curate the data in our next processing step.
Then I will choose to save the CSV in the \texttt{data/original/}
directory of my project and create a sub-directory called
\texttt{cedel2/}.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ cedel2}
       \ExtensionTok{}\NormalTok{ texts.csv}
\end{Highlighting}
\end{Shaded}

Other resources will inevitably include unique processes to obtaining
the data, but in the end the data should be archived in the research
structure in the \texttt{data/original/} directory and be treated as
`read-only'.

\hypertarget{programmatic}{%
\subsection{Programmatic}\label{programmatic}}

There are many resources that provide corpus data is directly accessible
for which programmatic approaches can be applied. Let's take a look at
how this works starting with the a sample from the Switchboard Corpus, a
corpus of 2,400 telephone conversations by 543 speakers. First we
navigate to the site with a browser and download the file that we are
looking for. In this case I found the Switchboard Corpus on the
\href{http://www.nltk.org/nltk_data/}{NLTK data repository site}. More
often than not this file will be some type of compressed archive file
with an extension such as \texttt{.zip} or \texttt{.tz}, which is the
case here. Archive files make downloading large single files or multiple
files easy by grouping files and directories into one file. In R we can
used the \texttt{download.file()} function from the base R
library\footnote{Remember base R packages are installed by default with
  R and are loaded and accessible by default in each R session.}. There
are a number of \textbf{arguments} that a function may require or
provide optionally. The \texttt{download.file()} function minimally
requires two: \texttt{url} and \texttt{destfile}. That is the file to
download and the location where it is to be saved to disk.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{, }\AttributeTok{destfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As we can see looking at the directory structure for \texttt{data/} the
\texttt{switchboard.zip} file has been downloaded.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

Once an archive file is downloaded, however, the file needs to be
`decompressed' to reveal the file structure. To decompress this file we
use the \texttt{unzip()} function with the arguments \texttt{zipfile}
pointing to the \texttt{.zip} file and \texttt{exdir} specifying the
directory where we want the files to be extracted to.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =} \StringTok{"../data/original/switchboard.zip"}\NormalTok{, }\AttributeTok{exdir =} \StringTok{"../data/original/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The directory structure of \texttt{data/} now should look like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ switchboard}
    \ExtensionTok{}\NormalTok{    README}
    \ExtensionTok{}\NormalTok{    discourse}
    \ExtensionTok{}\NormalTok{    disfluency}
    \ExtensionTok{}\NormalTok{    tagged}
    \ExtensionTok{}\NormalTok{    timed{-}transcript}
    \ExtensionTok{}\NormalTok{    transcript}
    \ExtensionTok{}\NormalTok{ switchboard.zip}
\end{Highlighting}
\end{Shaded}

At this point we have acquired the data programmatically and with this
code as part of our workflow anyone could run this code and reproduce
the same results. The code as it is, however, is not ideally efficient.
Firstly the \texttt{switchboard.zip} file is not strictly needed after
we decompress it and it occupies disk space if we keep it. And second,
each time we run this code the file will be downloaded from the remote
serve leading to unnecessary data transfer and server traffic. Let's
tackle each of these issues in turn.

To avoid writing the \texttt{switchboard.zip} file to disk (long-term)
we can use the \texttt{tempfile()} function to open a temporary holding
space for the file. This space can then be used to store the file, unzip
it, and then the temporary file will be destroyed. We assign the
temporary space to an R object we will name \texttt{temp} with the
\texttt{tempfile()} function. This object can now be used as the value
of the argument \texttt{destfile} in the \texttt{download.file()}
function. Let's also assign the web address to another object
\texttt{url} which we will use as the value of the \texttt{url}
argument.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a temporary file space for our .zip file}
\NormalTok{temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}
\CommentTok{\# Assign our web address to \textasciigrave{}url\textasciigrave{}}
\NormalTok{url }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}
\CommentTok{\# Download .zip file and write to disk}
\FunctionTok{download.file}\NormalTok{(url, temp)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
In the previous code I've used the values stored in the objects
\texttt{url} and \texttt{temp} in the \texttt{download.file()} function
without specifying the argument names --only providing the names of the
objects. R will assume that values of a function map to the ordering of
the arguments. If your values do not map to ordering of the arguments
you are required to specify the argument name and the value. To view the
ordering of objects hit \texttt{TAB} after entering the function name or
consult the function documentation by prefixing the function name with
\texttt{?} and hitting \texttt{ENTER}.
\end{tcolorbox}

At this point our downloaded file is stored temporarily on disk and can
be accessed and decompressed to our target directory using \texttt{temp}
as the value for the argument \texttt{zipfile} from the \texttt{unzip()}
function. I've assigned our target directory path to
\texttt{target\_dir} and used it as the value for the argument
\texttt{exdir} to prepare us for the next tweak on our approach.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assign our target directory to \textasciigrave{}target\_dir\textasciigrave{}}
\NormalTok{target\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/"}
\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir)}
\end{Highlighting}
\end{Shaded}

Our directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ switchboard}
        \ExtensionTok{}\NormalTok{ README}
        \ExtensionTok{}\NormalTok{ discourse}
        \ExtensionTok{}\NormalTok{ disfluency}
        \ExtensionTok{}\NormalTok{ tagged}
        \ExtensionTok{}\NormalTok{ timed{-}transcript}
        \ExtensionTok{}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

The second issue I raised concerns the fact that running this code as
part of our project will repeat the download each time. Since we would
like to be good citizens and avoid unnecessary traffic on the web it
would be nice if our code checked to see if we already have the data on
disk and if it exists, then skip the download, if not then download it.

To achieve this we need to introduce two new functions \texttt{if()} and
\texttt{dir.exists()}. \texttt{dir.exists()} takes a path to a directory
as an argument and returns the logical value, \texttt{TRUE}, if that
directory exists, and \texttt{FALSE} if it does not. \texttt{if()}
evaluates logical statements and processes subsequent code based on the
logical value it is passed as an argument. Let's look at a toy example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{1}
\ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 1 is 1
\end{verbatim}

I assigned \texttt{num} to the value \texttt{1} and created a logical
evaluation \texttt{num\ ==} whose result is passed as the argument to
\texttt{if()}. If the statement returns \texttt{TRUE} then the code
withing the first set of curly braces \texttt{\{...\}} is run. If
\texttt{num\ ==\ 1} is false, like in the code below, the code withing
the braces following the \texttt{else} will be run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num }\OtherTok{\textless{}{-}} \DecValTok{2}
\ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 2 is not 1
\end{verbatim}

The function \texttt{if()} is one of various functions that are called
\textbf{control statements}. Theses functions provide a lot of power to
make dynamic choices as code is run.

Before we get back to our key objective to avoid downloading resources
that we already have on disk, let me introduce another strategy to
making code more powerful and ultimately more efficient and as well as
more legible --the \textbf{custom function}. Custom functions are
functions that the user writes to create a set of procedures that can be
run in similar contexts. I've created a custom function named
\texttt{eval\_num()} below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eval\_num }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(num) \{}
  \ControlFlowTok{if}\NormalTok{(num }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) \{ }
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is 1"}\NormalTok{) }
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{cat}\NormalTok{(num, }\StringTok{"is not 1"}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Let's take a closer look at what's going on here. The function
\texttt{function()} creates a function in which the user decides what
arguments are necessary for the code to perform its task. In this case
the only necessary argument is the object to store a numeric value to be
evaluated. I've called it \texttt{num} because it reflects the name of
the object in our toy example, but there is nothing special about this
name. It's only important that the object names be consistently used.
I've included our previous code (except for the hard-coded assignment of
\texttt{num}) inside the curly braces and assigned the entire code chunk
to \texttt{eval\_num}.

We can now use the function \texttt{eval\_num()} to perform the task of
evaluating whether a value of \texttt{num} is or is not equal to
\texttt{1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 1 is 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 2 is not 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{eval\_num}\NormalTok{(}\AttributeTok{num =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> 3 is not 1
\end{verbatim}

I've put these coding strategies together with our previous code in a
custom function I named \texttt{get\_zip\_data()}. There is a lot going
on here. Take a look first and see if you can follow the logic involved
given what you now know.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_zip\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(url, target\_dir) \{}
  \CommentTok{\# Function: to download and decompress a .zip file to a target directory}
  
  \CommentTok{\# Check to see if the data already exists}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(target\_dir)) \{ }\CommentTok{\# if data does not exist, download/ decompress}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating target data directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create target data directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{    temp }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{() }\CommentTok{\# create a temporary space for the file to be written to}
    \FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ url, }\AttributeTok{destfile =}\NormalTok{ temp) }\CommentTok{\# download the data to the temp file}
    \FunctionTok{unzip}\NormalTok{(}\AttributeTok{zipfile =}\NormalTok{ temp, }\AttributeTok{exdir =}\NormalTok{ target\_dir, }\AttributeTok{junkpaths =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# decompress the temp file in the target directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# if data exists, don\textquotesingle{}t download it again}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

OK. You should have recognized the general steps in this function: the
argument \texttt{url} and \texttt{target\_dir} specify where to get the
data and where to write the decompressed files, the \texttt{if()}
statement evaluates whether the data already exists, if not
(\texttt{!dir.exists(target\_dir)}) then the data is downloaded and
decompressed, if it does exist (\texttt{else}) then it is not
downloaded.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The prefixed \texttt{!} in the logical expression
\texttt{dir.exists(target\_dir)} returns the opposite logical value.
This is needed in this case so when the target directory exists, the
expression will return \texttt{FALSE}, not \texttt{TRUE}, and therefore
not proceed in downloading the resource.
\end{tcolorbox}

There are a couple key tweaks I've added that provide some additional
functionality. For one I've included the function \texttt{dir.create()}
to create the target directory where the data will be written. I've also
added an additional argument to the \texttt{unzip()} function,
\texttt{junkpaths\ =\ TRUE}. Together these additions allow the user to
create an arbitrary directory path where the files, and only the files,
will be extracted to on our disk. This will discard the containing
directory of the \texttt{.zip} file which can be helpful when we want to
add multiple \texttt{.zip} files to the same target directory.

A practical scenario where this applies is when we want to download data
from a corpus that is contained in multiple \texttt{.zip} files but
still maintain these files in a single primary data directory. Take for
example the
\href{http://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa
Barbara Corpus}. This corpus resource includes a series of interviews in
which there is one \texttt{.zip} file, \texttt{SBCorpus.zip} which
contains the
\href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip}{transcribed
interviews} and another \texttt{.zip} file, \texttt{metadata.zip} which
organizes the
\href{http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip}{meta-data}
associated with each speaker. Applying our initial strategy to download
and decompress the data will lead to the following directory structure:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ SBCorpus}
    \ExtensionTok{}\NormalTok{  TRN}
    \ExtensionTok{}\NormalTok{  \_\_MACOSX}
    \ExtensionTok{}\NormalTok{      TRN}
    \ExtensionTok{}\NormalTok{ metadata}
        \ExtensionTok{}\NormalTok{ \_\_MACOSX}
\end{Highlighting}
\end{Shaded}

By applying our new custom function \texttt{get\_zip\_data()} to the
transcriptions and then the meta-data we can better organize the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus transcriptions}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/transcriptions/"}\NormalTok{)}

\CommentTok{\# Download corpus meta{-}data}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/sbc/meta{-}data/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ sbc}
        \ExtensionTok{}\NormalTok{ meta{-}data}
        \ExtensionTok{}\NormalTok{ transcriptions}
\end{Highlighting}
\end{Shaded}

If we add data from other sources we can keep them logical separate and
allow our data collection to scale without creating unnecessary
complexity. Let's add the Switchboard Corpus sample using our
\texttt{get\_zip\_data()} function to see this in action.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download corpus}
\FunctionTok{get\_zip\_data}\NormalTok{(}\AttributeTok{url =} \StringTok{"https://raw.githubusercontent.com/nltk/nltk\_data/gh{-}pages/packages/corpora/switchboard.zip"}\NormalTok{, }\AttributeTok{target\_dir =} \StringTok{"../data/original/scs/"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ sbc}
    \ExtensionTok{}\NormalTok{    meta{-}data}
    \ExtensionTok{}\NormalTok{    transcriptions}
    \ExtensionTok{}\NormalTok{ scs}
        \ExtensionTok{}\NormalTok{ README}
        \ExtensionTok{}\NormalTok{ discourse}
        \ExtensionTok{}\NormalTok{ disfluency}
        \ExtensionTok{}\NormalTok{ tagged}
        \ExtensionTok{}\NormalTok{ timed{-}transcript}
        \ExtensionTok{}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

At this point we have what we need to continue to the next step in our
data analysis project. But before we go, we should do some housekeeping
to document and organize this process to make our work reproducible. We
will take advantage of the \texttt{project-template} directory
structure, seen below.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{}\NormalTok{ README.md}
\ExtensionTok{}\NormalTok{ \_pipeline.R}
\ExtensionTok{}\NormalTok{ analysis}
\ExtensionTok{}\NormalTok{    1\_acquire\_data.Rmd}
\ExtensionTok{}\NormalTok{    2\_curate\_dataset.Rmd}
\ExtensionTok{}\NormalTok{    3\_transform\_dataset.Rmd}
\ExtensionTok{}\NormalTok{    4\_analyze\_dataset.Rmd}
\ExtensionTok{}\NormalTok{    5\_generate\_article.Rmd}
\ExtensionTok{}\NormalTok{    \_session{-}info.Rmd}
\ExtensionTok{}\NormalTok{    \_site.yml}
\ExtensionTok{}\NormalTok{    index.Rmd}
\ExtensionTok{}\NormalTok{    references.bib}
\ExtensionTok{}\NormalTok{ data}
\ExtensionTok{}\NormalTok{    derived}
\ExtensionTok{}\NormalTok{    original}
\ExtensionTok{}\NormalTok{        sbc}
\ExtensionTok{}\NormalTok{        scs}
\ExtensionTok{}\NormalTok{ functions}
\ExtensionTok{}\NormalTok{ output}
    \ExtensionTok{}\NormalTok{ figures}
    \ExtensionTok{}\NormalTok{ results}
\end{Highlighting}
\end{Shaded}

First it is good practice to separate custom functions from our
processing scripts. We can create a file in our \texttt{functions/}
directory named \texttt{acquire\_functions.R} and add our custom
function \texttt{get\_zip\_data()} there.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
Note that that the \texttt{acquire\_functions.R} file is an R script,
not an Rmarkdown document. Therefore code chunks that are used in
\texttt{.Rmd} files are not used, only the R code itself.
\end{tcolorbox}

We then use the \texttt{source()} function to read that function into
our current script to make it available to use as needed. It is good
practice to source your functions in the SETUP section of your script.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load custom functions for this project}
\FunctionTok{source}\NormalTok{(}\AttributeTok{file =} \StringTok{"../functions/acquire\_functions.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this section, to sum up, we've covered how to access, download, and
organize data contained in .zip files; the most common format for
language data found on repositories and individual sites. This included
an introduction to a few key R programming concepts and strategies
including using functions, writing custom functions, and controlling
program flow with control statements. Our approach was to gather data
while also keeping in mind the reproducibility of the code. To this end
I introduced programming strategies for avoiding unnecessary web traffic
(downloads), scalable directory creation, and data documentation.

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The custom function \texttt{get\_zip\_data()} works with \texttt{.zip}
files. There are many other compressed file formats (e.g.~\texttt{.gz},
\texttt{.tar}, \texttt{.tgz}), however. In the R package \texttt{tadr}
that accompanies this coursebook, a modified version of the
\texttt{get\_zip\_data()} function, \texttt{get\_compressed\_data()},
extends the same logic to deal with a wider range of compressed file
formats, including \texttt{.zip} files.

Explore this function's documentation
(\texttt{?tadr::get\_compressed\_data()}) and/ or view the code
(\texttt{tadr::get\_compressed\_data}) to better understand this
function.
\end{tcolorbox}

\hypertarget{apis}{%
\section{APIs}\label{apis}}

A convenient alternative method for acquiring data in R is through
package interfaces to web services. These interfaces are built using R
code to make connections with resources on the web through
\textbf{Application Programming Interfaces} (APIs). Websites such as
Project Gutenberg, Twitter, Facebook, and many others provide APIs to
allow access to their data under certain conditions, some more limiting
for data collection than others. Programmers (like you!) in the R
community take up the task of wrapping calls to an API with R code to
make accessing that data from R possible. For example,
\href{https://CRAN.R-project.org/package=gutenbergr}{gutenbergr}
provides access to Project Gutenberg,
\href{https://CRAN.R-project.org/package=rtweet}{rtweet} to Twitter, and
\href{https://CRAN.R-project.org/package=Rfacebook}{Rfacebook} to
Facebook.\footnote{See Section @ref(sources) for a list of some other
  API packages.}

\hypertarget{open-access}{%
\subsection{Open access}\label{open-access}}

Using R package interfaces, however, often requires some more knowledge
about R objects and functions. Let's take a look at how to access data
from Project Gutenberg through the \texttt{gutenbergr} package. Along
the way we will touch upon various functions and concepts that are key
to working with the R data types vectors and data frames including
filtering and writing tabular data to disk in plain-text format.

To get started let's install and/ or load the \texttt{gutenbergr}
package. If a package is not part of the R base library, we cannot
assume that the user will have the package in their library. The
standard approach for installing and then loading a package is by using
the \texttt{install.packages()} function and then calling
\texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"gutenbergr"}\NormalTok{) }\CommentTok{\# install \textasciigrave{}gutenbergr\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(gutenbergr) }\CommentTok{\# load the \textasciigrave{}gutenbergr\textasciigrave{} package}
\end{Highlighting}
\end{Shaded}

This approach works just fine, but luck has it that there is an R
package for installing and loading packages! The
\href{https://CRAN.R-project.org/package=pacman}{pacman} package
includes a set of functions for managing packages. A very useful one is
\texttt{p\_load()} which will look for a package on a system, load it if
it is found, and install and then load it if it is not found. This helps
potentially avoid using unnecessary bandwidth to install packages that
may already exist on a user's system. But, to use \texttt{pacman} we
need to include the code to install and load it with the functions
\texttt{install.packages()} and \texttt{library()}. I've included some
code that will mimic the behavior of \texttt{p\_load()} for installing
\texttt{pacman} itself, but as you can see it is not elegant, luckily
it's only used once as we add it to the SETUP section of our master
file, \texttt{\_pipeline.R}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load \textasciigrave{}pacman\textasciigrave{}. If not installed, install then load.}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)) \{}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"pacman"}\NormalTok{)}
  \FunctionTok{library}\NormalTok{(}\StringTok{"pacman"}\NormalTok{, }\AttributeTok{character.only =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now that we have \texttt{pacman} installed and loaded into our R
session, let's use the \texttt{p\_load()} function to make sure to
install/ load the two packages we will need for the upcoming tasks. If
you are following along with the \texttt{project\_template}, add this
code within the SETUP section of the \texttt{1\_acquire\_data.Rmd} file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Script{-}specific options or packages}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
Note that the arguments \texttt{tidyverse} and \texttt{gutenbergr} are
comma-separated but not quoted when using \texttt{p\_load()}. When using
\texttt{install.packages()} to install, package names need to be quoted
(character strings). \texttt{library()} can take quotes or no quotes,
but only one package at a time.
\end{tcolorbox}

Project Gutenberg provides access to thousands of texts in the public
domain. The \texttt{gutenbergr} package contains a set of tables, or
\textbf{data frames} in R speak, that index the meta-data for these
texts broken down by text (\texttt{gutenberg\_metadata}), author
(\texttt{gutenberg\_authors}), and subject
(\texttt{gutenberg\_subjects}). I'll use the \texttt{glimpse()} function
loaded in the
\href{https://CRAN.R-project.org/package=tidyverse}{tidyverse} package
\footnote{\texttt{tidyverse} is not a typical package. It is a set of
  packages: \texttt{ggplot2}, \texttt{dplyr}, \texttt{tidyr},
  \texttt{readr}, \texttt{purrr}, and \texttt{tibble}. These packages
  are all installed/ loaded with \texttt{tidyverse} and form the
  backbone for the type of work you will typically do in most analyses.}
to summarize the structure of these data frames.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_metadata) }\CommentTok{\# summarize text meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 51,997
#> Columns: 8
#> $ gutenberg_id        <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ~
#> $ title               <chr> NA, "The Declaration of Independence of the United~
#> $ author              <chr> NA, "Jefferson, Thomas", "United States", "Kennedy~
#> $ gutenberg_author_id <int> NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, 7, 7, 7,~
#> $ language            <chr> "en", "en", "en", "en", "en", "en", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> NA, "United States Law/American Revolutionary War/~
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_authors) }\CommentTok{\# summarize authors meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 16,236
#> Columns: 7
#> $ gutenberg_author_id <int> 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 18, 20, 2~
#> $ author              <chr> "United States", "Lincoln, Abraham", "Henry, Patri~
#> $ alias               <chr> NA, NA, NA, NA, "Dodgson, Charles Lutwidge", NA, "~
#> $ birthdate           <int> NA, 1809, 1736, NA, 1832, NA, 1819, 1860, 1805, 17~
#> $ deathdate           <int> NA, 1865, 1799, NA, 1898, NA, 1891, 1937, 1844, 18~
#> $ wikipedia           <chr> NA, "http://en.wikipedia.org/wiki/Abraham_Lincoln"~
#> $ aliases             <chr> NA, "United States President (1861-1865)/Lincoln, ~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gutenberg\_subjects) }\CommentTok{\# summarize subjects meta{-}data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 140,173
#> Columns: 3
#> $ gutenberg_id <int> 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, ~
#> $ subject_type <chr> "lcc", "lcsh", "lcsh", "lcc", "lcc", "lcsh", "lcsh", "lcc~
#> $ subject      <chr> "E201", "United States. Declaration of Independence", "Un~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The \texttt{gutenberg\_metadata}, \texttt{gutenberg\_authors}, and
\texttt{gutenberg\_subjects} are periodically updated. To check to see
when each data frame was last updated run:

\texttt{attr(gutenberg\_metadata,\ "date\_updated")}
\end{tcolorbox}

To download the text itself we use the \texttt{gutenberg\_download()}
function which takes one required argument, \texttt{gutenberg\_id}. The
\texttt{gutenberg\_download()} function is what is known as
`vectorized', that is, it can take a single value or multiple values for
the argument \texttt{gutenberg\_id}. Vectorization refers to the process
of applying a function to each of the elements stored in a
\textbf{vector} --a primary object type in R. A vector is a grouping of
values of one of various types including character (\texttt{chr}),
integer (\texttt{int}), double (\texttt{dbl}), and logical
(\texttt{lgl}) and a data frame is a grouping of vectors. The
\texttt{gutenberg\_download()} function takes an integer vector which
can be manually added or selected from the \texttt{gutenberg\_metadata}
or \texttt{gutenberg\_subjects} data frames using the \texttt{\$}
operator (e.g.~\texttt{gutenberg\_metadata\$gutenberg\_id}).

Let's first add them manually here as a toy example by generating a
vector of integers from 1 to 5 assigned to the variable name
\texttt{ids}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ids }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5} \CommentTok{\# integer vector of values 1 to 5}
\NormalTok{ids}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] 1 2 3 4 5
\end{verbatim}

To download the works from Project Gutenberg corresponding to the
\texttt{gutenberg\_id}s 1 to 5, we pass the \texttt{ids} object to the
\texttt{gutenberg\_download()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids) }\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5}
\FunctionTok{glimpse}\NormalTok{(works\_sample) }\CommentTok{\# summarize \textasciigrave{}works\textasciigrave{} dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,959
#> Columns: 2
#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
#> $ text         <chr> "December, 1971  [Etext #1]", "", "", "The Project Gutenb~
\end{verbatim}

Two attributes are returned: \texttt{gutenberg\_id} and \texttt{text}.
The \texttt{text} column contains values for each line of text
(delimited by a carriage return) for each of the 5 works we downloaded.
There are many more attributes available from the Project Gutenberg API
that can be accessed by passing a character vector of the attribute
names to the argument \texttt{meta\_fields}. The column names of the
\texttt{gutenberg\_metadata} data frame contains the available
attributes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(gutenberg\_metadata) }\CommentTok{\# print the column names of the \textasciigrave{}gutenberg\_metadata\textasciigrave{} data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "gutenberg_id"        "title"               "author"             
#> [4] "gutenberg_author_id" "language"            "gutenberg_bookshelf"
#> [7] "rights"              "has_text"
\end{verbatim}

Let's augment our previous download with the title and author of each of
the works. To create a character vector we use the \texttt{c()}
function, then, quote and delimit the individual elements of the vector
with a comma.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# download works with \textasciigrave{}gutenberg\_id\textasciigrave{} 1{-}5 including \textasciigrave{}title\textasciigrave{} and \textasciigrave{}author\textasciigrave{} as attributes}
\NormalTok{works\_sample }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids, }
                            \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"title"}\NormalTok{,}
                                            \StringTok{"author"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(works\_sample) }\CommentTok{\# summarize dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 2,959
#> Columns: 4
#> $ gutenberg_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
#> $ text         <chr> "December, 1971  [Etext #1]", "", "", "The Project Gutenb~
#> $ title        <chr> "The Declaration of Independence of the United States of ~
#> $ author       <chr> "Jefferson, Thomas", "Jefferson, Thomas", "Jefferson, Tho~
\end{verbatim}

Now, in a more practical scenario we would like to select the values of
\texttt{gutenberg\_id} by some principled query such as works from a
specific author, language, or subject. To do this we first query either
the \texttt{gutenberg\_metadata} data frame or the
\texttt{gutenberg\_subjects} data frame. Let's say we want to download a
random sample of 10 works from English Literature (Library of Congress
Classification, ``PR''). Using the \texttt{dplyr::filter()} function
(\texttt{dplyr} is part of the \texttt{tidyverse} package set) we first
extract all the Gutenberg ids from \texttt{gutenberg\_subjects} where
\texttt{subject\_type\ ==\ "lcc"} and \texttt{subject\ ==\ "PR"}
assigning the result to \texttt{ids}.\footnote{See
  \href{https://www.loc.gov/catdir/cpso/lcco/}{Library of Congress
  Classification} documentation for a complete list of subject codes.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter for only English literature}
\NormalTok{ids }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==} \StringTok{"PR"}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 7,100
#> Columns: 3
#> $ gutenberg_id <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58, 60, 8~
#> $ subject_type <chr> "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "lcc", "~
#> $ subject      <chr> "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR", "PR~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The operators \texttt{=} and \texttt{==} are not equivalents.
\texttt{==} is used for logical evaluation and \texttt{=} is an
alternate notation for variable assignment (\texttt{\textless{}-}).
\end{tcolorbox}

The \texttt{gutenberg\_subjects} data frame does not contain information
as to whether a \texttt{gutenberg\_id} is associated with a plain-text
version. To limit our query to only those English Literature works with
text, we filter the \texttt{gutenberg\_metadata} data frame by the ids
we have selected in \texttt{ids} and the attribute \texttt{has\_text} in
the \texttt{gutenberg\_metadata} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for only those works that have text}
\NormalTok{ids\_has\_text }\OtherTok{\textless{}{-}} 
  \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{         gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
\NormalTok{         has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{glimpse}\NormalTok{(ids\_has\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 6,724
#> Columns: 8
#> $ gutenberg_id        <int> 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 58~
#> $ title               <chr> "Alice's Adventures in Wonderland", "Through the L~
#> $ author              <chr> "Carroll, Lewis", "Carroll, Lewis", "Carroll, Lewi~
#> $ gutenberg_author_id <int> 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 37, 17, 4~
#> $ language            <chr> "en", "en", "en", "en", "en", "en", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> "Children's Literature", "Children's Literature/Be~
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
A couple R programming notes on the code phrase
\texttt{gutenberg\_id\ \%in\%\ ids\$gutenberg\_id}. First, the
\texttt{\$} symbol in \texttt{ids\$gutenberg\_id} is the programmatic
way to target a particular column in an R data frame. In this example we
select the \texttt{ids} data frame and the column
\texttt{gutenberg\_id}, which is a integer vector. The
\texttt{gutenberg\_id} variable that precedes the \texttt{\%in\%}
operator does not need an explicit reference to a data frame because the
primary argument of the \texttt{filter()} function is this data frame
(\texttt{gutenberg\_metadata}). Second, the \texttt{\%in\%} operator
logically evaluates whether the vector elements in
\texttt{gutenberg\_metadata\$gutenberg\_ids} are also found in the
vector \texttt{ids\$gutenberg\_id} returning \texttt{TRUE} and
\texttt{FALSE} accordingly. This effectively filters those ids which are
not in both vectors.
\end{tcolorbox}

As we can see the number of works with text is fewer than the number of
works listed, 7100 versus 6724. Now we can safely do our random
selection of 10 works, with the function \texttt{slice\_sample()} and be
confident that the ids we select will contain text when we take the next
step by downloading the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{ids\_sample }\OtherTok{\textless{}{-}} \FunctionTok{slice\_sample}\NormalTok{(ids\_has\_text, }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# sample 10 works}
\FunctionTok{glimpse}\NormalTok{(ids\_sample) }\CommentTok{\# summarize the dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 10
#> Columns: 8
#> $ gutenberg_id        <int> 10564, 10784, 9316, 1540, 24450, 13821, 7595, 3818~
#> $ title               <chr> "Fairy Gold\nShip's Company, Part 4.", "Sentence D~
#> $ author              <chr> "Jacobs, W. W. (William Wymark)", "Jacobs, W. W. (~
#> $ gutenberg_author_id <int> 1865, 1865, 2364, 65, 999, 2685, 761, 1317, 3564, ~
#> $ language            <chr> "en", "en", "en", "en", "en", "en", "en", "en", "e~
#> $ gutenberg_bookshelf <chr> NA, NA, NA, NA, "Adventure", "Fantasy", NA, NA, NA~
#> $ rights              <chr> "Public domain in the USA.", "Public domain in the~
#> $ has_text            <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\OtherTok{\textless{}{-}} \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                               \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
\FunctionTok{glimpse}\NormalTok{(works\_pr) }\CommentTok{\# summarize the dataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 47,513
#> Columns: 4
#> $ gutenberg_id <int> 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 154~
#> $ text         <chr> "cover ", "", "", "", "", "THE TEMPEST", "", "by William ~
#> $ author       <chr> "Shakespeare, William", "Shakespeare, William", "Shakespe~
#> $ title        <chr> "The Tempest", "The Tempest", "The Tempest", "The Tempest~
\end{verbatim}

At this point we have data and could move on to processing this dataset
in preparation for analysis. However, we are aiming for a reproducible
workflow and this code does not conform to our principle of modularity:
each subsequent step in our analysis will depend on running this code
first. Furthermore, running this code as it is creates issues with
bandwidth, as in our previous examples from direct downloads. To address
modularity we will write the dataset to disk in \textbf{plain-text
format}. In this way each subsequent step in our analysis can access the
dataset locally. To address bandwidth concerns, we will devise a method
for checking to see if the dataset is already downloaded and skip the
download, if possible, to avoid accessing the Project Gutenberg server
unnecessarily.

To write our data frame to disk we will export it into a standard
plain-text format for two-dimensional datasets: a CSV file
(comma-separated value). The CSV structure for this dataset will look
like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{works\_pr }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{head}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{format\_csv}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{cat}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> gutenberg_id,text,author,title
#> 1540,cover ,"Shakespeare, William",The Tempest
#> 1540,,"Shakespeare, William",The Tempest
#> 1540,,"Shakespeare, William",The Tempest
#> 1540,,"Shakespeare, William",The Tempest
#> 1540,,"Shakespeare, William",The Tempest
#> 1540,THE TEMPEST,"Shakespeare, William",The Tempest
\end{verbatim}

The first line contains the names of the columns and subsequent lines
the observations. Data points that contain commas themselves
(e.g.~``Shaw, Bernard'') are quoted to avoid misinterpreting these
commas a deliminators in our data. To write this dataset to disk we will
use the \texttt{reader::write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(works\_pr, }\AttributeTok{file =} \StringTok{"../data/original/gutenberg\_works\_pr.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To avoid downloading dataset that already resides on disk, let's
implement a similar strategy to the one used for direct downloads
(\texttt{get\_zip\_data()}). I've incorporated the code for sampling and
downloading data for a particular subject from Project Gutenberg with a
control statement to check if the dataset file already exists into a
function I named \texttt{get\_gutenberg\_subject()}. Take a look at this
function below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_gutenberg\_subject }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(subject, target\_file, }\AttributeTok{sample\_size =} \DecValTok{10}\NormalTok{) \{}
  \CommentTok{\# Function: to download texts from Project Gutenberg with }
  \CommentTok{\# a specific LCC subject and write the data to disk.}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, gutenbergr) }\CommentTok{\# install/load necessary packages}
  
  \CommentTok{\# Check to see if the data already exists}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{ }\CommentTok{\# if data does not exist, download and write}
\NormalTok{    target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file) }\CommentTok{\# generate target directory for the .csv file}
    \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create target data directory}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data... }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
    \CommentTok{\# Select all records with a particular LCC subject}
\NormalTok{    ids }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_subjects, }
\NormalTok{             subject\_type }\SpecialCharTok{==} \StringTok{"lcc"}\NormalTok{, subject }\SpecialCharTok{==}\NormalTok{ subject) }\CommentTok{\# select subject}
    \CommentTok{\# Select only those records with plain text available}
    \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# make the sampling reproducible}
\NormalTok{    ids\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{filter}\NormalTok{(gutenberg\_metadata, }
\NormalTok{             gutenberg\_id }\SpecialCharTok{\%in\%}\NormalTok{ ids}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }\CommentTok{\# select ids in both data frames }
\NormalTok{             has\_text }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# select those ids that have text}
      \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sample\_size) }\CommentTok{\# sample N works }
    \CommentTok{\# Download sample with associated \textasciigrave{}author\textasciigrave{} and \textasciigrave{}title\textasciigrave{} metadata}
\NormalTok{    works\_sample }\OtherTok{\textless{}{-}} 
      \FunctionTok{gutenberg\_download}\NormalTok{(}\AttributeTok{gutenberg\_id =}\NormalTok{ ids\_sample}\SpecialCharTok{$}\NormalTok{gutenberg\_id, }
                         \AttributeTok{meta\_fields =} \FunctionTok{c}\NormalTok{(}\StringTok{"author"}\NormalTok{, }\StringTok{"title"}\NormalTok{))}
    \CommentTok{\# Write the dataset to disk in .csv format}
    \FunctionTok{write\_csv}\NormalTok{(works\_sample, }\AttributeTok{file =}\NormalTok{ target\_file)}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data downloaded! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{ }\CommentTok{\# if data exists, don\textquotesingle{}t download it again}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already exists }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# print status message}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Adding this function to our function script
\texttt{functions/acquire\_functions.R}, we can now source this function
in our \texttt{analysis/1\_acquire\_data.Rmd} script to download
multiple subjects and store them in on disk in their own file.

Let's download American Literature now (LCC code ``PQ'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Download Project Gutenberg text for subject \textquotesingle{}PQ\textquotesingle{} (American Literature)}
\CommentTok{\# and then write this dataset to disk in .csv format}
\FunctionTok{get\_gutenberg\_subject}\NormalTok{(}\AttributeTok{subject =} \StringTok{"PQ"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/gutenberg/works\_pq.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Applying this function to both the English and American Literature
datasets, our data directory structure now looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ gutenberg}
    \ExtensionTok{}\NormalTok{    works\_pq.csv}
    \ExtensionTok{}\NormalTok{    works\_pr.csv}
    \ExtensionTok{}\NormalTok{ sbc}
    \ExtensionTok{}\NormalTok{    meta{-}data}
    \ExtensionTok{}\NormalTok{    transcriptions}
    \ExtensionTok{}\NormalTok{ scs}
        \ExtensionTok{}\NormalTok{ README}
        \ExtensionTok{}\NormalTok{ discourse}
        \ExtensionTok{}\NormalTok{ disfluency}
        \ExtensionTok{}\NormalTok{ documentation}
        \ExtensionTok{}\NormalTok{ tagged}
        \ExtensionTok{}\NormalTok{ timed{-}transcript}
        \ExtensionTok{}\NormalTok{ transcript}
\end{Highlighting}
\end{Shaded}

\hypertarget{authentication}{%
\subsection{Authentication}\label{authentication}}

Some APIs and the R interfaces that provide access to them require
authentication. This may either be through an interactive process that
is mediated between R and the web service and/ or by visiting the
developer website of the particular API. In either case, there is an
extra step that is necessary to make the connect to the API to access
the data.

Let's take a look at the popular micro-blogging platform Twitter. The
rtweet package (Kearney, Revilla Sancho, and Wickham 2022) provides
access to tweets in various ways. To get started install and/or load the
rtweet package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rtweet) }\CommentTok{\# install/load rtweet package}
\end{Highlighting}
\end{Shaded}

Now before a researcher can access data from Twitter with rtweet,
\href{https://docs.ropensci.org/rtweet/articles/auth.html}{an
authentication token must be setup and made accessible}. After following
the steps for setting up an authentication token and saving it, that
token can be accessed with the \texttt{auth\_as()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{auth\_as}\NormalTok{(twitter\_auth) }\CommentTok{\# load the saved \textasciigrave{}twitter\_auth\textasciigrave{} token}
\end{Highlighting}
\end{Shaded}

Now that we the R session is authenticated, we can explore a popular
method for querying the Twitter API which searchs tweets
(\texttt{search\_tweets}) posted in the recent past (6-9 days).

Let's look at a typical query using the \texttt{search\_tweets()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx }\OtherTok{\textless{}{-}} 
  \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =} \StringTok{"latinx"}\NormalTok{, }\CommentTok{\# query term}
                \AttributeTok{n =} \DecValTok{100}\NormalTok{, }\CommentTok{\# number of tweets desired}
                \AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\CommentTok{\# a mix of \textasciigrave{}recent\textasciigrave{} and \textasciigrave{}popular\textasciigrave{} tweets}
                \AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not include RTs}
\end{Highlighting}
\end{Shaded}

Looking at the arguments in this function, we see I've specified the
query term to be `latinx'. This is a single word query but if the query
included multiple words, the spaces between would be interpreted as the
logical \texttt{AND} (only match tweets with all the individual terms).
If one would like to include multi-word expressions, the expressions
should be enclosed by single quotes
(i.e.~\texttt{q\ =\ "\textquotesingle{}spanish\ speakers\textquotesingle{}\ AND\ latinx"}).
Another approach would be to include the logical \texttt{OR} (match
tweets with either of the terms). Multi-word expressions can be included
as in the previous case. Of note, hashtags are acceptable terms, so
\texttt{q\ =\ "\#latinx"} would match tweets with this hashtag.

The number of results has been set at `100', but this is the default, so
I could have left it out. But you can increase the number of desired
tweets. There are rate limits which cap the number of tweets you can
access in a given 15-minute time period.

Another argument of importance is the \texttt{type} argument. This
argument has three possible attributes \texttt{popular},
\texttt{recent}, and \texttt{mixed}. When the \texttt{popular} attribute
he Twitter API will tend to return fewer tweets than specified by
\texttt{n}. With \texttt{recent} or \texttt{mixed} you will most likely
get the \texttt{n} you specified (note that \texttt{mixed} is a mix of
\texttt{popular} and \texttt{recent}).

A final argument to note is the \texttt{include\_rts} whose attribute is
logical. If \texttt{FALSE} no retweets will be included in the results.
This is often what a language researcher will want.

Now, once the \texttt{search\_tweets} query has been run, there a a
large number of variables that are included in the resulting data frame.
Here's an overview of the names of the variables and the vector types
for each variable.

\hypertarget{tbl-ad-rtweet-variables-table}{}
\begin{table}
\caption{\label{tbl-ad-rtweet-variables-table}Variables and variable types returned from Twitter API via rtweet's
search\_tweets() function. }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
created\_at & character\\
id & double\\
id\_str & character\\
full\_text & character\\
truncated & logical\\
\addlinespace
display\_text\_range & double\\
entities & list\\
metadata & list\\
source & character\\
in\_reply\_to\_status\_id & double\\
\addlinespace
in\_reply\_to\_status\_id\_str & character\\
in\_reply\_to\_user\_id & double\\
in\_reply\_to\_user\_id\_str & character\\
in\_reply\_to\_screen\_name & character\\
geo & logical\\
\addlinespace
coordinates & list\\
place & list\\
contributors & logical\\
is\_quote\_status & logical\\
retweet\_count & integer\\
\addlinespace
favorite\_count & integer\\
favorited & logical\\
retweeted & logical\\
lang & character\\
possibly\_sensitive & logical\\
\addlinespace
quoted\_status\_id & double\\
quoted\_status\_id\_str & character\\
quoted\_status & list\\
text & character\\
favorited\_by & logical\\
\addlinespace
display\_text\_width & logical\\
retweeted\_status & logical\\
quoted\_status\_permalink & logical\\
query & logical\\
possibly\_sensitive\_appealable & logical\\
\bottomrule
\end{tabular}
\end{table}

The
\href{https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets}{Twitter
API documentation for the standard Search Tweets call}, which is what
\texttt{search\_tweets()} interfaces with has quite a few variables (35
to be exact). For many purposes it is not necessary to keep all the
variables. Furthermore, since we will want to write a plain-text file to
disk as part of our project, we will need to either convert or eliminate
any of the variables that are marked as type \texttt{list}. The most
common variable to convert is the \texttt{coordinates} variable, as it
will contain the geolocation codes for those Twitter users' tweets
captured in the query that have geolocation enabled on their device. It
is of note, however, that using \texttt{search\_tweets()} without
specifying that only tweets with geocodes should be captured
(\texttt{geocode\ =}) will tend to return very few, if any, tweets with
geolocation information as the majority of Twitter users do not have
geolocation enabled.

Let's assume that we want to keep all the variables that are not of type
\texttt{list}. One option is to use \texttt{select()} and name each
variable we want to keep. On the other hand we can use a combination of
\texttt{select()} and negated \texttt{!where()} to select all the
variables that are not lists (\texttt{is\_list}). Let's do the later
approach.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rt\_latinx\_subset }\OtherTok{\textless{}{-}} 
\NormalTok{  rt\_latinx }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# dataset}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# select all variables that are NOT lists}

\NormalTok{rt\_latinx\_subset }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# subsetted dataset}
  \FunctionTok{glimpse}\NormalTok{() }\CommentTok{\# overview}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 100
#> Columns: 30
#> $ created_at                    <chr> "Sun Sep 26 17:38:06 +0000 2021", "Sun S~
#> $ id                            <dbl> 1.44e+18, 1.44e+18, 1.44e+18, 1.44e+18, ~
#> $ id_str                        <chr> "1442181701967302659", "1442196629801488~
#> $ full_text                     <chr> "If we call it Latinx Mass they can't ca~
#> $ truncated                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ display_text_range            <dbl> 57, 177, 166, 23, 261, 153, 202, 211, 57~
#> $ source                        <chr> "<a href=\"https://mobile.twitter.com\" ~
#> $ in_reply_to_status_id         <dbl> NA, NA, NA, 1.44e+18, NA, NA, NA, NA, 1.~
#> $ in_reply_to_status_id_str     <chr> NA, NA, NA, "1437436224042635269", NA, N~
#> $ in_reply_to_user_id           <dbl> NA, NA, NA, 4.26e+08, NA, NA, NA, NA, 2.~
#> $ in_reply_to_user_id_str       <chr> NA, NA, NA, "426159377", NA, NA, NA, NA,~
#> $ in_reply_to_screen_name       <chr> NA, NA, NA, "MorganStanley", NA, NA, NA,~
#> $ geo                           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ contributors                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ is_quote_status               <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,~
#> $ retweet_count                 <int> 351, 124, 62, 0, 0, 0, 0, 0, 0, 0, 0, 1,~
#> $ favorite_count                <int> 3902, 898, 280, 0, 0, 0, 0, 0, 0, 7, 0, ~
#> $ favorited                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ retweeted                     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE~
#> $ lang                          <chr> "en", "en", "es", "en", "en", "en", "en"~
#> $ possibly_sensitive            <lgl> NA, FALSE, FALSE, FALSE, FALSE, FALSE, F~
#> $ quoted_status_id              <dbl> NA, NA, NA, NA, 1.44e+18, NA, NA, NA, NA~
#> $ quoted_status_id_str          <chr> NA, NA, NA, NA, "1442475408058830856", N~
#> $ text                          <chr> "If we call it Latinx Mass they can't ca~
#> $ favorited_by                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ display_text_width            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ retweeted_status              <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ quoted_status_permalink       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ query                         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
#> $ possibly_sensitive_appealable <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
\end{verbatim}

Now we have the 30 variables which can be written to disk as a
plain-text file. Let's go ahead a do this, but wrap it in a function
that does all the work we've just laid out in one function. In addition
we will check to see if the same query has been run, and skip running
the query if the dataset is on disk.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_search\_tweets }\OtherTok{\textless{}{-}} 
  \ControlFlowTok{function}\NormalTok{(query, path, }\AttributeTok{n =} \DecValTok{100}\NormalTok{, }\AttributeTok{type =} \StringTok{"mixed"}\NormalTok{, }\AttributeTok{include\_rts =} \ConstantTok{FALSE}\NormalTok{) \{}
    \CommentTok{\# Function}
    \CommentTok{\# Conduct a Twitter search query and write the results to a csv file}
    
    \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(path)) \{ }\CommentTok{\# check to see if the file already exists}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File does not exist }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
      \FunctionTok{library}\NormalTok{(rtweet) }\CommentTok{\# to use Twitter API}
      \FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# to manipulate data}
      
      \FunctionTok{auth\_get}\NormalTok{() }\CommentTok{\# get authentication token}
      
\NormalTok{      results }\OtherTok{\textless{}{-}} \CommentTok{\# query results}
        \FunctionTok{search\_tweets}\NormalTok{(}\AttributeTok{q =}\NormalTok{ query, }\CommentTok{\# query term}
                      \AttributeTok{n =}\NormalTok{ n, }\CommentTok{\# number of tweets desired (default 100)}
                      \AttributeTok{type =}\NormalTok{ type, }\CommentTok{\# type of query}
                      \AttributeTok{include\_rts =}\NormalTok{ include\_rts) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# to include RTs}
        \FunctionTok{select}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{where}\NormalTok{(is\_list))  }\CommentTok{\# remove list variables}
      
      \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{dir.exists}\NormalTok{(}\FunctionTok{dirname}\NormalTok{(path))) \{ }\CommentTok{\# isolate directory and check if exists}
        \FunctionTok{cat}\NormalTok{(}\StringTok{"Creating directory }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
        
        \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =} \FunctionTok{dirname}\NormalTok{(path), }\CommentTok{\# isolate and create directory (remove file name)}
                   \AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# create embedded directories if necessary}
                   \AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not report warnings}
\NormalTok{      \}}
      
      \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ results, }\AttributeTok{file =}\NormalTok{ path) }\CommentTok{\# write results to csv file }
      \FunctionTok{cat}\NormalTok{(}\StringTok{"Twitter search results written to disk }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
      
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \FunctionTok{cat}\NormalTok{(}\StringTok{"File already exists! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{) }\CommentTok{\# message}
\NormalTok{    \}}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

Let's run this function with the same query as above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_search\_tweets}\NormalTok{(}\AttributeTok{query =} \StringTok{"latinx"}\NormalTok{, }\AttributeTok{path =} \StringTok{"../data/original/twitter/rt\_latinx.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And the appropriate directory structure and file have been written to
disk.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/twitter/}
\ExtensionTok{}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

In sum, this subsection provided an overview to acquiring data from web
service APIs through R packages. We took at closer look at the
\texttt{gutenbergr} package which provides programmatic access to works
available on Projec t Gutenberg and the \texttt{rtweet} package which
provides authenticated access to Twitter. Working with package
interfaces requires more knowledge of R including loading/ installing
packages, working with vectors and data frames, and exporting data from
an R session. We touched on these programming concepts and also outlined
a method to create a reproducible workflow.

\hypertarget{web-scraping}{%
\section{Web scraping}\label{web-scraping}}

There are many resources available through manual and direct downloads
from repositories and individual sites and R package interfaces to web
resources with APIs, but these resources are relatively limited to the
amount of public-facing textual data recorded on the web. In the case
that you want to acquire data from webpages, R can be used to access the
web programmatically through a process known as web scraping. The
complexity of web scrapes can vary but in general it requires more
advanced knowledge of R as well as the structure of the language of the
web: HTML (Hypertext Markup Language).

\hypertarget{a-toy-example}{%
\subsection{A toy example}\label{a-toy-example}}

HTML is a cousin of XML (eXtensible Markup Language) and as such
organizes web documents in a hierarchical format that is read by your
browser as you navigate the web. Take for example the toy webpage I
created as a demonstration in Figure~\ref{fig-ad-example-webpage}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/example-webpage.png}

}

\caption{\label{fig-ad-example-webpage}Example web page.}

\end{figure}

The file accessed by my browser to render this webpage is
\texttt{test.html} and in plain-text format looks like this:

\begin{verbatim}

<html>
  <head>
    <title>My website</title>
  </head>
  <body>
    <div class="intro">
      <p>Welcome!</p>
      <p>This is my first website. </p>
    </div>
    <table>
      <tr>
        <td>Contact me:</td>
        <td>
          <a href="mailto:francojc@wfu.edu">francojc@wfu.edu</a>
        </td>
      </tr>
    </table>
    <div class="conc">
      <p>Good-bye!</p>
    </div>
  </body>
</html>
\end{verbatim}

Each element in this file is delineated by an opening and closing tag,
\texttt{\textless{}head\textgreater{}\textless{}/head\textgreater{}}.
Tags are nested within other tags to create the structural hierarchy.
Tags can take class and id labels to distinguish them from other tags
and often contain other attributes that dictate how the tag is to behave
when rendered visually by a browser. For example, there are two
\texttt{\textless{}div\textgreater{}} tags in our toy example: one has
the label \texttt{class\ =\ "intro"} and the other
\texttt{class\ =\ "conc"}. \texttt{\textless{}div\textgreater{}} tags
are often used to separate sections of a webpage that may require
special visual formatting. The \texttt{\textless{}a\textgreater{}} tag,
on the other hand, creates a web link. As part of this tag's function,
it requires the attribute \texttt{href=} and a web protocol --in this
case it is a link to an email address \texttt{mailto:francojc@wfu.edu}.
More often than not, however, the \texttt{href=} contains a URL (Uniform
Resource Locator). A working example might look like this:
\texttt{\textless{}a\ href="https://francojc.github.io/"\textgreater{}My\ homepage\textless{}/a\textgreater{}}.

The aim of a web scrape is to download the HTML file, parse the document
structure, and extract the elements containing the relevant information
we wish to capture. Let's attempt to extract some information from our
toy example. To do this we will need the
\href{https://CRAN.R-project.org/package=rvest}{rvest}(Wickham 2022)
package. First, install/load the package, then, read and parse the HTML
from the character vector named \texttt{web\_file} assigning the result
to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(rvest) }\CommentTok{\# install/ load \textasciigrave{}rvest\textasciigrave{}}

\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(web\_file) }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_document}
#> <html>
#> [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
#> [2] <body>\n    <div class="intro">\n      <p>Welcome!</p>\n      <p>This is  ...
\end{verbatim}

\texttt{read\_html()} parses the raw HTML into an object of class
\texttt{xml\_document}. The summary output above shows that tags the
HTML structure have been parsed into `elements'. The tag elements can be
accessed by using the \texttt{html\_elements()} function by specifying
the tag to isolate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (2)}
#> [1] <div class="intro">\n      <p>Welcome!</p>\n      <p>This is my first web ...
#> [2] <div class="conc">\n      <p>Good-bye!</p>\n    </div>
\end{verbatim}

Notice that \texttt{html\_elements("div")} has returned both
\texttt{div} tags. To isolate one of tags by its class, we add the class
name to the tag separating it with a \texttt{.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (1)}
#> [1] <div class="intro">\n      <p>Welcome!</p>\n      <p>This is my first web ...
\end{verbatim}

Great. Now say we want to drill down and isolate the subordinate
\texttt{\textless{}p\textgreater{}} nodes. We can add \texttt{p} to our
node filter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {xml_nodeset (2)}
#> [1] <p>Welcome!</p>
#> [2] <p>This is my first website. </p>
\end{verbatim}

To extract the text contained within a node we use the
\texttt{html\_text()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Welcome!"                   "This is my first website. "
\end{verbatim}

The result is a character vector with two elements corresponding to the
text contained in each \texttt{\textless{}p\textgreater{}} tag. If you
were paying close attention you might have noticed that the second
element in our vector includes extra whitespace after the period. To
trim leading and trailing whitespace from text we can add the
\texttt{trim\ =\ TRUE} argument to \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"div.intro p"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{(}\AttributeTok{trim =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Welcome!"                  "This is my first website."
\end{verbatim}

From here we would then work to organize the text into a format we want
to store it in and write the results to disk. Let's leave writing data
to disk for later in the chapter. For now keep our focus on working with
\texttt{rvest} to acquire data from html documents working with a more
practical example.

\hypertarget{a-practical-example}{%
\subsection{A practical example}\label{a-practical-example}}

With some basic understanding of HTML and how to use the \texttt{rvest}
package, let's turn to a realistic example. Say we want to acquire
lyrics from the online music website and database
\href{https://www.last.fm/}{last.fm}. The first step in any web scrape
is to investigate the site and page(s) we want to scrape to ascertain if
there any licensing restrictions. Many, but not all websites, will
include a plain text file
\href{https://www.cloudflare.com/learning/bots/what-is-robots.txt/}{\texttt{robots.txt}}
at the root of the main URL. This file is declares which webpages a
`robot' (including web scraping scripts) can and cannot access. We can
use the \texttt{robotstxt} package to find out which URLs are accessible
\footnote{It is important to check the paths of sub-domains as some
  website allow access in some areas and not in others}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(robotstxt) }\CommentTok{\# load/ install \textasciigrave{}robotstxt\textasciigrave{}}

\FunctionTok{paths\_allowed}\NormalTok{(}\AttributeTok{paths =} \StringTok{"https://www.last.fm/"}\NormalTok{) }\CommentTok{\# check permissions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] TRUE
\end{verbatim}

The next step includes identifying the URL we want to target and
exploring the structure of the HTML document. Take the following webpage
I have identified, seen in
Figure~\ref{fig-ad-example-lyrics-page-lastfm}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-lastfm-webpage-lyrics.png}

}

\caption{\label{fig-ad-example-lyrics-page-lastfm}Lyrics page from
last.fm}

\end{figure}

As in our toy example, first we want to feed the HTML web address to the
\texttt{read\_html()} function to parse the tags into elements. We will
then assign the result to \texttt{html}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read and parse html as an xml object}
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Radiohead/\_/Karma+Police/+lyrics"}
\NormalTok{html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(lyrics\_url) }\CommentTok{\# read raw html and parse to xml}
\NormalTok{html}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_document}
#> <html lang="en" class="
#>         no-js
#>         playbar-masthead-release-shim
#>         youtube-provider-not-ready
#>     ">
#> [1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
#> [2] <body>\n<div id="initial-tealium-data" data-require="tracking/tealium-uta ...
\end{verbatim}

At this point we have captured and parsed the raw HTML assigning it to
the object named \texttt{html}. The next step is to identify the html
elements that contain the information we want to extract from the page.
To do this it is helpful to use a browser to inspect specific elements
of the webpage. Your browser will be equipped with a command that you
can enable by hovering your mouse over the element of the page you want
to target and using a right click to select ``Inspect'' (Chrome) or
``Inspect Element'' (Safari, Brave). This will split your browser window
vertical or horizontally showing you the raw HTML underlying the
webpage.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-lastfm-artist-inspect.png}

}

\caption{\label{fig-ad-inspect-element-artist-lastfm}Using the ``Inspect
Element'' command to explore raw html.}

\end{figure}

From Figure~\ref{fig-ad-inspect-element-artist-lastfm} we see that the
element we want to target is contained within an
\texttt{\textless{}a\textgreater{}\textless{}/a\textgreater{}} tag. Now
this tag is common and we don't want to extract every \texttt{a} so we
use the class \texttt{header-new-crumb} to specify we only want the
artist name. Using the convention described in our toy example, we can
isolate the artist of the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> {html_node}
#> <a class="header-new-crumb" itemprop="url" href="/music/Radiohead">
#> [1] <span itemprop="name">Radiohead</span>
\end{verbatim}

We can then extract the text with \texttt{html\_text()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{artist }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{artist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Radiohead"
\end{verbatim}

Let's extract the song title in the same way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{song}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Karma Police"
\end{verbatim}

Now if we inspect the HTML of the lyrics page, we will notice that the
lyrics are contained in
\texttt{\textless{}p\textgreater{}\textless{}/p\textgreater{}} tags with
the class \texttt{lyrics-paragraph}.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-lastfm-lyrics-inspect.png}

}

\caption{\label{fig-ad-inspect-element-lyrics-lastfm}Using the ``Inspect
Element'' command to explore raw html.}

\end{figure}

Since there are multiple elements that we want to extract, we will need
to use the \texttt{html\_elements()} function instead of the
\texttt{html\_element()} which only targets one element.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics }\OtherTok{\textless{}{-}} 
\NormalTok{  html }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{html\_text}\NormalTok{()}
\NormalTok{lyrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> [1] "Karma policeArrest this manHe talks in mathsHe buzzes like a fridgeHe's like a detuned radio"      
#> [2] "Karma policeArrest this girlHer Hitler hairdoIs making me feel illAnd we have crashed her party"   
#> [3] "This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us"        
#> [4] "Karma policeI've given all I canIt's not enoughI've given all I canBut we're still on the payroll" 
#> [5] "This is what you'll getThis is what you'll getThis is what you'll getWhen you mess with us"        
#> [6] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"
#> [7] "For a minute thereI lost myself, I lost myselfPhew, for a minute thereI lost myself, I lost myself"
\end{verbatim}

At this point, we have isolated and extracted the artist, song, and
lyrics from the webpage. Each of these elements are stored in character
vectors in our R session. To complete our task we need to write this
data to disk as plain text. With an eye towards a tidy dataset, an ideal
format to store the data is in a CSV file where each column corresponds
to one of the elements from our scrape and each row an observation. A
CSV file is a tabular format and so before we can write the data to disk
let's coerce the data that we have into tabular format. We will use the
\texttt{tibble()} function here to streamline our data frame creation.
\footnote{\texttt{tibble} objects are \texttt{data.frame} objects with
  some added extra bells and whistles that we won't get into here.}
Feeding each of the vectors \texttt{artist}, \texttt{song}, and
\texttt{lyrics} as arguments to \texttt{tibble()} creates the tabular
format we are looking for.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tibble}\NormalTok{(artist, song, lyrics) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 7
#> Columns: 3
#> $ artist <chr> "Radiohead", "Radiohead", "Radiohead", "Radiohead", "Radiohead"~
#> $ song   <chr> "Karma Police", "Karma Police", "Karma Police", "Karma Police",~
#> $ lyrics <chr> "Karma policeArrest this manHe talks in mathsHe buzzes like a f~
\end{verbatim}

Notice that there are seven rows in this data frame, one corresponding
to each paragraph in \texttt{lyrics}. R has a bias towards working with
vectors of the same length. As such each of the other vectors
(\texttt{artist}, and \texttt{song}) are replicated, or recycled, until
they are the same length as the longest vector \texttt{lyrics}, which a
length of seven.

For good documentation let's add our object \texttt{lyrics\_url} to the
data frame, which contains the actual web link to this page, and assign
the result to \texttt{song\_lyrics}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}
\end{Highlighting}
\end{Shaded}

The final step is to write this data to disk. To do this we will use the
\texttt{write\_csv()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{x =}\NormalTok{ song\_lyrics, }\AttributeTok{path =} \StringTok{"../data/original/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{scaling-up}{%
\subsection{Scaling up}\label{scaling-up}}

At this point you may be think, `Great, I can download data from a
single page, but what about downloading multiple pages?' Good question.
That's really where the strength of a programming approach takes hold.
Extracting information from multiple pages is not fundamentally
different than working with a single page. However, it does require more
sophisticated understanding of the web and R coding strategies, in
particular \textbf{iteration}.

Before we get to iteration, let's first create a couple functions to
make it possible to efficiently reuse the code we have developed so far:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the \texttt{get\_lyrics} function wraps the code for scraping a single
  lyrics webpage from last.fm.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
  \CommentTok{\# Function: Scrape last.fm lyrics page for: artist, song, }
  \CommentTok{\# and lyrics from a provided content link. }
  \CommentTok{\# Return as a tibble/data.frame}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping song lyrics from:"}\NormalTok{, lyrics\_url, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest) }\CommentTok{\# install/ load package(s)}
  
\NormalTok{  url }\OtherTok{\textless{}{-}} \FunctionTok{url}\NormalTok{(lyrics\_url, }\StringTok{"rb"}\NormalTok{) }\CommentTok{\# open url connection }
\NormalTok{  html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(url) }\CommentTok{\# read and parse html as an xml object}
  \FunctionTok{close}\NormalTok{(url) }\CommentTok{\# close url connection}
  
\NormalTok{  artist }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"a.header{-}new{-}crumb"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_text}\NormalTok{()}
  
\NormalTok{  song }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_element}\NormalTok{(}\StringTok{"h1.header{-}new{-}title"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_text}\NormalTok{()}
  
\NormalTok{  lyrics }\OtherTok{\textless{}{-}} 
\NormalTok{    html }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"p.lyrics{-}paragraph"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{html\_text}\NormalTok{()}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"...one moment "}\NormalTok{)}
  
  \FunctionTok{Sys.sleep}\NormalTok{(}\DecValTok{1}\NormalTok{) }\CommentTok{\# sleep for 1 second to reduce server load}
  
\NormalTok{  song\_lyrics }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(artist, song, lyrics, lyrics\_url)}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"... done! }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(song\_lyrics)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The \texttt{get\_lyrics} function includes all of the code developed
previously, but also includes: (1) output messages (\texttt{cat()}), (2)
a processing pause (\texttt{Sys.sleep()}), and (3) code to manage
opening and closing web connections (\texttt{url()} and
\texttt{close()}).

Points (1) and (2) will be useful when we iterate over this function to
provide status messages and to reduce server load when processing
multiple webpages from a web domain. (3) will be necessary to manage
webpages that are non-existent. As we will see, we will generate url
link to multiple song lyrics some of which will not be valid. To avoid
errors that will stop the processing these steps have been incorporated
here.
\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  the \texttt{write\_content} writes the webscraped data to our local
  machine, including functionality to create the necessary directory
  structure of the target file path we choose.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{write\_content }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(content, target\_file) \{}
  \CommentTok{\# Function: Write the tibble content to disk. Create the directory if}
  \CommentTok{\# it does not already exist.}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse) }\CommentTok{\# install/ load packages}
  
\NormalTok{  target\_dir }\OtherTok{\textless{}{-}} \FunctionTok{dirname}\NormalTok{(target\_file) }\CommentTok{\# identify target file directory structure}
  \FunctionTok{dir.create}\NormalTok{(}\AttributeTok{path =}\NormalTok{ target\_dir, }\AttributeTok{recursive =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# create directory}
  \FunctionTok{write\_csv}\NormalTok{(content, target\_file) }\CommentTok{\# write csv file to target location}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Content written to disk!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With just these two functions, we can take a lyrics URL from last.fm and
scrape and write the data to disk like this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lyrics\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.last.fm/music/Pixies/\_/Where+Is+My+Mind\%3F/+lyrics"}

\NormalTok{lyrics\_url }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/lyrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/lastfm/}
\ExtensionTok{}\NormalTok{ lyrics.csv}
\end{Highlighting}
\end{Shaded}

Now we could manually search and copy URLs and run this function
pipeline. This would be fine if we had just a few particular URLs that
we wanted to scrape. But if we want to, say, scrape a set of lyrics
grouped by genre. We would probably want a more programmatic approach.
The good news is we can leverage our understanding of webscraping to
scrape last.fm to harvest the information needed to create and store
links to songs by genre. We can then pass these links to a pipeline,
similar to the previous one, to scrape lyrics for many songs and store
the results in files grouped by genre.

Last.fm provides a genres page where some of the top genres are listed
and can be further explored.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-lastfm-genres.png}

}

\caption{\label{fig-ad-genre-page-lastfm}Genre page on last.fm}

\end{figure}

Diving into a a particular genre, `rock' for example, you will get a
listing of the top tracks in that genre.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./figures/acquire-data/ad-lastfm-genre-tracks-list.png}

}

\caption{\label{fig-ad-genre-tracks-list-lastfm}Tracks by genre list
page on last.fm}

\end{figure}

If we inspect the HTML elements for the track names in
Figure~\ref{fig-ad-genre-tracks-list-lastfm}, we can see that a relative
URL is found for the track. In this case, I have `Smells Like Teen
Spirit' by Nirvana highlighted in the inspector. If we follow this link
to the track page and then to the lyrics for the track, you will notice
that the relative URL on the track listings page has all the unique
information. Only the web domain \texttt{https://www.last.fm} and the
post-pended \texttt{/+lyrics} is missing.

So with this we can put together a function which gets the track listing
for a last.fm genre, scrapes the relative URLs for each of the tracks,
and creates a full absolute URL to the lyrics page.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_genre\_lyrics\_urls }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre) \{}
  \CommentTok{\# Function: Scrapes a given last.fm genre title for top tracks in}
  \CommentTok{\# that genre and then creates links to the lyrics pages for these tracks}
  
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Scraping top songs from:"}\NormalTok{, last\_fm\_genre, }\StringTok{"genre: }\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
\NormalTok{  pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(tidyverse, rvest) }\CommentTok{\# install/ load packages}
  
  \CommentTok{\# create web url for the genre listing page}
\NormalTok{  genre\_listing\_url }\OtherTok{\textless{}{-}} 
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm/tag/"}\NormalTok{, last\_fm\_genre, }\StringTok{"/tracks"}\NormalTok{) }
  
\NormalTok{  genre\_lyrics\_urls }\OtherTok{\textless{}{-}} 
    \FunctionTok{read\_html}\NormalTok{(genre\_listing\_url) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# read raw html and parse to xml}
    \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"td.chartlist{-}name a"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# isolate the track elements}
    \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"href"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# extract the href attribute}
    \FunctionTok{paste0}\NormalTok{(}\StringTok{"https://www.last.fm"}\NormalTok{, ., }\StringTok{"/+lyrics"}\NormalTok{) }\CommentTok{\# join the domain, relative artist path, and the post{-}pended /+lyrics to create an absolute URL}
  
  \FunctionTok{return}\NormalTok{(genre\_lyrics\_urls)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

With this function, all we need is to identify the verbatim way last.fm
lists the genres. For Rock, it is \texttt{rock} but for Hip Hop, it is
\texttt{hip+hop}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# get urls for top hip hop tracks}
  \FunctionTok{head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\CommentTok{\# only display 10 tracks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Scraping top songs from: hip+hop genre:
\end{verbatim}

\begin{verbatim}
#>  [1] "https://www.last.fm/music/Juzhin/_/Charlie+Conscience+(feat.+MMAIO)/+lyrics"
#>  [2] "https://www.last.fm/music/Juzhin/_/Railways/+lyrics"                        
#>  [3] "https://www.last.fm/music/Juzhin/_/Coming+Down/+lyrics"                     
#>  [4] "https://www.last.fm/music/Juzhin/_/Tupona/+lyrics"                          
#>  [5] "https://www.last.fm/music/Juzhin/_/Sakhalin/+lyrics"                        
#>  [6] "https://www.last.fm/music/Juzhin/_/3+Simple+Minutes/+lyrics"                
#>  [7] "https://www.last.fm/music/Juzhin/_/Lost+Sense/+lyrics"                      
#>  [8] "https://www.last.fm/music/Juzhin/_/Wonderful/+lyrics"                       
#>  [9] "https://www.last.fm/music/Gina+Moryson/_/Vanilla+Smoothy+(Live)/+lyrics"    
#> [10] "https://www.last.fm/music/Juzhin/_/Flunk-Down+(Juzhin+Remix)/+lyrics"
\end{verbatim}

So now we have a method to scrape URLs by genre and list them in a
vector. Our approach, then, could be to pass these lyrics URLs to our
existing pipeline which downloads the lyrics (\texttt{get\_lyrics()})
and then writes them to disk (\texttt{write\_content()}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will not run}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{get\_lyrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# scrape lyrics url}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This approach, however, has a couple of problems. (1) our
\texttt{get\_lyrics()} function only takes one URL at a time, but the
result of \texttt{get\_genre\_lyrics\_urls()} will produce many URLs. We
will be able to solve this with iteration using the \href{}{purrr}
package, specifically the \texttt{map()} function which will iteratively
map each URL output from \texttt{get\_genre\_lyrics\_urls()} to
\texttt{get\_lyrics()} in turn. (2) the output from our iterative
application of \texttt{get\_lyrics()} will produce a tibble for each
URL, which then sets up a problem with writing the tibbles to disk with
the \texttt{write\_content()} function. To avoid this we will want to
combine the tibbles into one single tibble and then send it to be
written to disk. The \texttt{bind\_rows()} function will do just this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but with occasional errors}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This preceding pipeline conceptually will work. However, on my testing,
it turns out that some of the URLs that are generated in the
\texttt{get\_genre\_lyrics\_urls()} do not exist on the site. That is,
the song is listed but no lyrics have been added to the song site. This
will mean that when the URL is sent to the \texttt{get\_lyrics()}
function, there will be an error when attempting to download and parse
the page with \texttt{read\_html()} which will halt the entire process.
To avoid this error, we can wrap the \texttt{get\_lyrics()} function in
a function designed to attempt to download and parse the URL
(\texttt{tryCatch()}), but if there is an error, it will skip it and
move on to the next URL without stopping the processing. This approach
is reflected in the \texttt{get\_lyrics\_catch()} function below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Wrap the \textasciigrave{}get\_lyrics()\textasciigrave{} function with \textasciigrave{}tryCatch()\textasciigrave{} to }
\CommentTok{\# skip URLs that have no lyrics}

\NormalTok{get\_lyrics\_catch }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(lyrics\_url) \{}
  \FunctionTok{tryCatch}\NormalTok{(}\FunctionTok{get\_lyrics}\NormalTok{(lyrics\_url), }
           \AttributeTok{error =} \ControlFlowTok{function}\NormalTok{(e) }\FunctionTok{return}\NormalTok{(}\ConstantTok{NULL}\NormalTok{)) }\CommentTok{\# no, URL, return(NULL)/ skip}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Updating the pipeline with the \texttt{get\_lyrics\_catch()} function
would look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: will run, but we can do better}
\FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(}\StringTok{"hip+hop"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# get lyrics urls for specific genre}
  \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# scrape lyrics url}
  \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# combine tibbles into one}
  \FunctionTok{write\_content}\NormalTok{(}\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{) }\CommentTok{\# write to disk}
\end{Highlighting}
\end{Shaded}

This will work, but as we have discussed before one of this goals we
have we acquiring data for a reproducible research project is to make
sure that we are developing efficient code that will not burden site's
server we are scraping from. In this case, we would like to check to see
if the data is already downloaded. If not, then the script should run.
If so, then the script does not run. Of course this is a perfect use of
a conditional statement. To make this a single function we can call,
I've wrapped the functions we created for getting lyric URLs from
last.fm, scraping the URLs, and writing the results to disk in the
\texttt{download\_lastfm\_lyrics()} function below. I also added a line
to add a \texttt{last\_fm\_genre} column to the combined tibble to store
the name of the genre we scraped
(i.e.~\texttt{mutate(genre\ =\ last\_fm\_genre)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{download\_lastfm\_lyrics }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(last\_fm\_genre, target\_file) \{}
  \CommentTok{\# Function: get last.fm lyric urls by genre and write them to disk}
  
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file.exists}\NormalTok{(target\_file)) \{}
    
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Downloading data.}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    
    \FunctionTok{get\_genre\_lyrics\_urls}\NormalTok{(last\_fm\_genre) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{map}\NormalTok{(get\_lyrics\_catch) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{genre =}\NormalTok{ last\_fm\_genre) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{write\_content}\NormalTok{(target\_file)}
    
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{cat}\NormalTok{(}\StringTok{"Data already downloaded!}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we can call this function on any genre on the last.fm site and
download the top 50 song lyrics for that genre (provided they all have
lyrics pages).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scrape lyrics for \textquotesingle{}pop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"pop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/pop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}rock\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"rock"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/rock.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}hip hop\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"hip+hop"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/hip\_hop.csv"}\NormalTok{)}

\CommentTok{\# Scrape lyrics for \textquotesingle{}metal\textquotesingle{}}
\FunctionTok{download\_lastfm\_lyrics}\NormalTok{(}\AttributeTok{last\_fm\_genre =} \StringTok{"metal"}\NormalTok{, }\AttributeTok{target\_file =} \StringTok{"../data/original/lastfm/metal.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we can see that our web scrape data is organized in a similar
fashion to the other data we acquired in this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ cedel2/}
    \ExtensionTok{}\NormalTok{    texts.csv}
    \ExtensionTok{}\NormalTok{ gutenberg/}
    \ExtensionTok{}\NormalTok{    works\_pq.csv}
    \ExtensionTok{}\NormalTok{    works\_pr.csv}
    \ExtensionTok{}\NormalTok{ lastfm/}
    \ExtensionTok{}\NormalTok{    country.csv}
    \ExtensionTok{}\NormalTok{    hip\_hop.csv}
    \ExtensionTok{}\NormalTok{    lyrics.csv}
    \ExtensionTok{}\NormalTok{    metal.csv}
    \ExtensionTok{}\NormalTok{    pop.csv}
    \ExtensionTok{}\NormalTok{    rock.csv}
    \ExtensionTok{}\NormalTok{ sbc/}
    \ExtensionTok{}\NormalTok{    meta{-}data/}
    \ExtensionTok{}\NormalTok{    transcriptions/}
    \ExtensionTok{}\NormalTok{ scs/}
    \ExtensionTok{}\NormalTok{    README}
    \ExtensionTok{}\NormalTok{    discourse}
    \ExtensionTok{}\NormalTok{    disfluency}
    \ExtensionTok{}\NormalTok{    documentation/}
    \ExtensionTok{}\NormalTok{    tagged}
    \ExtensionTok{}\NormalTok{    timed{-}transcript}
    \ExtensionTok{}\NormalTok{    transcript}
    \ExtensionTok{}\NormalTok{ twitter/}
        \ExtensionTok{}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

Again, it is important to add these custom functions to our
\texttt{acquire\_functions.R} script in the \texttt{functions/}
directory so we can access them in our scripts more efficiently and make
our analysis steps more succinct and legible.

In this section we covered scraping language data from the web. The
rvest package provides a host of functions for downloading and parsing
HTML. We first looked at a toy example to get a basic understanding of
how HTML works and then moved to applying this knowledge to a practical
example. To maintain a reproducible workflow, the code developed in this
example was grouped into task-oriented functions which were in turn
joined and wrapped into a function that provided convenient access to
our workflow and avoided unnecessary downloads (in the case the data
already exists on disk).

Here we have built on previously introduced R coding concepts and
demonstrated various others. Web scraping often requires more knowledge
of and familiarity with R as well as other web technologies. Rest
assured, however, practice will increase confidence in your abilities. I
encourage you to practice on your own with other websites. You will
encounter problems. Consult the R documentation in RStudio or online and
lean on the R community on the web at sites such as
\href{https://stackoverflow.com/}{Stack Overflow} \emph{inter alia}.

\hypertarget{documentation-1}{%
\section{Documentation}\label{documentation-1}}

As part of the data acquisition process it is important include
documentation that describes the data resource(s) that will serve as the
base for a research project. For all resources the data should include
as much information possible that outlines the sampling frame of the
data (del 2020). For a corpus sample acquired from a repository will
often include documentation which will outline the sampling frame --this
most likely will be the very information which leads a researcher to
select this resource for the project at hand. It is important to include
this documentation (HTML or PDF file) or reference to the documentation
(article citation or link\footnote{Note that web links can change and it
  is often best to safeguard the documentation by downloading the HTML
  documentation page instead of linking}) within the reproducible
project's directory structure.

In other cases where the data acquisition process is formulated and
conducted by the researcher for the specific aims of the research
(i.e.~API and web scraping approaches), the researcher should make an
effort to document those aspects which are key for the study, but that
may also be of interest to other researchers for similar research
questions. This will may include language characteristics such as
modality, register, genre, etc., speaker/ writer characteristics such as
demographics, time period(s), context of the linguistic communication,
etc. and process characteristics such as the source of the data, the
process of acquisition, date of acquisition, etc. However, it is
important to recognize that each language sample and the resource from
which it is drawn is unique. As a general rule of thumb, a researcher
should document the resource as if this were a resource \emph{they} were
to encounter for the first time. To archive this information, it is
standard practice to include a \texttt{README} file in the relevant
directory where the data is stored.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ cedel2/}
    \ExtensionTok{}\NormalTok{  documentation/}
    \ExtensionTok{}\NormalTok{  texts.csv}
    \ExtensionTok{}\NormalTok{ gutenberg/}
    \ExtensionTok{}\NormalTok{  README.md}
    \ExtensionTok{}\NormalTok{  works\_pq.csv}
    \ExtensionTok{}\NormalTok{  works\_pr.csv}
    \ExtensionTok{}\NormalTok{ lastfm/}
    \ExtensionTok{}\NormalTok{  README.md}
    \ExtensionTok{}\NormalTok{  country.csv}
    \ExtensionTok{}\NormalTok{  hip\_hop.csv}
    \ExtensionTok{}\NormalTok{  lyrics.csv}
    \ExtensionTok{}\NormalTok{  metal.csv}
    \ExtensionTok{}\NormalTok{  pop.csv}
    \ExtensionTok{}\NormalTok{  rock.csv}
    \ExtensionTok{}\NormalTok{ sbc/}
    \ExtensionTok{}\NormalTok{  meta{-}data/}
    \ExtensionTok{}\NormalTok{  transcriptions/}
    \ExtensionTok{}\NormalTok{ scs/}
    \ExtensionTok{}\NormalTok{  README}
    \ExtensionTok{}\NormalTok{  discourse}
    \ExtensionTok{}\NormalTok{  disfluency}
    \ExtensionTok{}\NormalTok{  documentation/}
    \ExtensionTok{}\NormalTok{  tagged}
    \ExtensionTok{}\NormalTok{  timed{-}transcript}
    \ExtensionTok{}\NormalTok{  transcript}
    \ExtensionTok{}\NormalTok{ twitter/}
        \ExtensionTok{}\NormalTok{ README.md}
        \ExtensionTok{}\NormalTok{ rt\_latinx.csv}
\end{Highlighting}
\end{Shaded}

For both existing corpora and data samples acquired by the researcher it
is also important to signal if there are conditions and/ or licensing
restrictions that one should heed when using and potentially sharing the
data. In some cases existing corpus data come with restrictions on data
sharing. These can be quite restrictive and ultimately require that the
corpus data not be included in publically available reproducible project
or data can only be shared in a derived format. If this the case, it is
important to document the steps to legally acquire the data so that a
researcher can acquire their own license and take full advantage of your
reproducible project.

In the case of data from APIs or web scraping, there too may be
stipulations on sharing data. A growing number of data sources apply one
of \href{https://creativecommons.org/about/cclicenses/}{the available
Creative Common Licenses}. Check the source of your data for more
information and if you are a member of a research institution you will
likely have a
\href{https://zsr.wfu.edu/digital-scholarship/copyright/}{specialist} on
\href{https://www.copyright.gov/fair-use/more-info.html}{Copyright and
Fair Use}.

\hypertarget{activities-4}{%
\section*{Activities}\label{activities-4}}
\addcontentsline{toc}{section}{Activities}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Recipe}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_6.html}{Control
statements, custom functions, and iteration}\\
\textbf{How}: Read Recipe 6 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To increase your ability to produce effective, concise,
and reproducible code. The three main areas we will cover are working
with control statements, writing custom functions, and leveraging
iteration. These programming strategies are often useful for acquiring
data but, as we will see, they are powerful concepts that can be used
throughout a reproducible research project.
\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-tip-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Lab}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm]
\textbf{What}: \href{https://github.com/lin380/lab_6}{Control
statements, custom functions, and iteration}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 6.\\
\textbf{Why}: To gain experience working with coding strategies such as
control statements, custom functions, and iteration, practice working
with direct downloads and API interfaces to acquire data, and implement
organizational strategies for organizing data in reproducible fashion.
\end{tcolorbox}

\hypertarget{summary-5}{%
\section*{Summary}\label{summary-5}}
\addcontentsline{toc}{section}{Summary}

In this chapter we have covered a lot of ground. On the surface we have
discussed three methods for acquiring corpus data for use in text
analysis. In the process we have delved into various aspects of the R
programming language. Some key concepts include writing custom functions
and working with those function in an iterative manner. We have also
considered topics that are more general in nature and concern
interacting with data found on the internet.

Each of these methods should be approached in a way that is transparent
to the researcher and to would-be collaborators and the general research
community. For this reason the documentation of the steps taken to
acquire data are key both in the code and in human-facing documentation.

At this point you have both a bird's eye view of the data available on
the web and strategies on how to access a great majority of it. It is
now time to turn to the next step in our data analysis project: data
curation. In the next posts I will cover how to wrangle your raw data
into a tidy dataset. This will include working with and incorporating
meta-data as well as augmenting a dataset with linguistic annotations.

\hypertarget{sec-curate-datasets}{%
\chapter{Curate data(sets)}\label{sec-curate-datasets}}

\begin{quote}
The hardest bit of information to extract is the first piece.

--Robert Ferrigno
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-note-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Keys}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-note-color!10!white, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  what are some of the formats that data can take?
\item
  what R programming strategies are used to read these formats into
  tabular, tidy dataset structures?
\item
  what is the importance of maintaining modularity between data and data
  processing in a reproducible research project?
\end{itemize}

\end{tcolorbox}

In this chapter we will now look at the next step in a text analysis
project: data curation. That is, the process of converting the original
data we acquire to a tidy dataset. As Acquired data can come in a wide
variety of formats that depend largely on the richness of the metadata
that is included, but also can reflect individual preferences. In this
chapter we will consider three general types of formats: (1)
unstructured data, (2) structured data, and (3) semi-structured data.
Regardless of the file type and the structure of the data, it will be
necessary to consider how to curate a dataset that such that the
structure reflects the basic the unit of analysis that we wish to
investigate (see
\protect\hyperlink{sec-framing-research.htmlux5cux23research-question}{Chapter
4, section 4.2}. The resulting dataset will be the base from which we
will work to further transform the dataset such that it aligns with the
analysis method(s) that we will implement. And as in previous
implementation steps, we will discuss the important role of
documentation.

\hypertarget{unstructured}{%
\section{Unstructured}\label{unstructured}}

The bulk of text that is available in the wild is of the unstructured
variety. Unstructured data is data that has not been organized to make
the information contained within explicit. Explicit information that is
included with data is called metadata. Metadata can be linguistic or
non-linguistic in nature. So for unstructured data there is little to no
metadata directly associated with the data. This information needs to be
added or derived for the purposes of the research, either through manual
inspection or (semi-)automatic processes. For now, however, our job is
just to get the unstructured data into a structured format with a
minimal set of metadata that we can derive from the resource.

As an example of an unstructured source of text data, let's take a look
at the \href{https://www.statmt.org/europarl/}{Europarle Parallel
Corpus}, as introduced in
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''}. This data contains parallel texts (source and translated
documents) from the European Parliamentary proceedings for some 21
European languages. Here we will focus in on the translation from
Spanish to English (Spanish-English).

\hypertarget{orientation}{%
\subsection{Orientation}\label{orientation}}

With the data downloaded into the \texttt{data/original/europarle/}
directory we see that there are two files. One corresponding to the
source language (Spanish) and one for the target language (English).

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/original/europarle/}
\ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.en}
\ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

Looking at the first 10 lines of the first file, we can see that this is
running text.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"data/understanding{-}data/formats\_europarle{-}en\_sample.txt"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{cat}\NormalTok{(}\AttributeTok{fill =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Resumption of the session
> I declare resumed the session of the European Parliament adjourned on Friday
17 December 1999, and I would like once again to wish you a happy new year in
the hope that you enjoyed a pleasant festive period.
> Although, as you will have seen, the dreaded 'millennium bug' failed to
materialise, still the people in a number of countries suffered a series of
natural disasters that truly were dreadful.
> You have requested a debate on this subject in the course of the next few
days, during this part-session.
> In the meantime, I should like to observe a minute' s silence, as a number of
Members have requested, on behalf of all the victims concerned, particularly
those of the terrible storms, in the various countries of the European Union.
> Please rise, then, for this minute' s silence.
> (The House rose and observed a minute' s silence)
> Madam President, on a point of order.
> You will be aware from the press and television that there have been a number
of bomb explosions and killings in Sri Lanka.
> One of the people assassinated very recently in Sri Lanka was Mr Kumar
Ponnambalam, who had visited the European Parliament just a few months ago.
\end{verbatim}

The only meta information that we can surmise from these files is the
fact that we know one is the source language and one is the target
language and that each sentence is aligned (parallel) with the lines in
the other file.

So with what we have we'd like to create a data frame that has the seen
in Table~\ref{tbl-cd-unstructured-europarle-structure-example}.

\hypertarget{tbl-cd-unstructured-europarle-structure-example}{}
\begin{table}
\caption{\label{tbl-cd-unstructured-europarle-structure-example}Idealized structure for the Europarle Corpus dataset. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
type & sentence\_id & sentence\\
\midrule
Source & 1 & ...sentence from source language\\
Target & 1 & ...sentence from target language\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{tidy-the-data}{%
\subsection{Tidy the data}\label{tidy-the-data}}

To create this dataset structure lets's read the files with the
\texttt{readtext()} function from readtext package and assign them to a
meaningful variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the Europarle files}
\NormalTok{europarle\_en }\OtherTok{\textless{}{-}}  \CommentTok{\# English target text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.en"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}

\NormalTok{europarle\_es }\OtherTok{\textless{}{-}} \CommentTok{\# Spanish source text}
\NormalTok{  readtext}\SpecialCharTok{::}\FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarle/europarl{-}v7.es{-}en.es"}\NormalTok{, }\CommentTok{\# path to the data}
                     \AttributeTok{verbosity =} \DecValTok{0}\NormalTok{) }\CommentTok{\# don\textquotesingle{}t show warnings}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
The \texttt{readtext()} function can read many different types of file
formats, from structured to unstructured. However, it depends in large
part on the extension of the file to recognize what algorithm to use
when reading a file. In this particular case the Europarle files do not
have a typical extension (they have \texttt{.en} and \texttt{.es}). The
\texttt{readtext()} function will treat them as plain text
(\texttt{.txt}), but it will throw a warning message. To suppress the
warning message you can add the \texttt{verbosity\ =\ 0} argument.
\end{tcolorbox}

Now there are a couple things to note about thbe \texttt{europarle\_en}
and \texttt{europarle\_es} objects. If we inspect their structure, we
will find that the dimensions of the data frame that is created is one
row by two columns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(europarle\_en) }\CommentTok{\# inspect the structure of the object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Classes 'readtext' and 'data.frame': 1 obs. of 2 variables:
#> $ doc_id: chr "europarl-v7.es-en.en"
#> $ text : chr "Resumption of the session\nI declare resumed the session of
the European Parliament adjourned on Friday 17 Dece"| __truncated__
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
Note that the \texttt{str()} function from base R is similar to
\texttt{glimpse()}. However, \texttt{glimpse()} will attempt to show you
as much data as possible. In this case since our column \texttt{text} is
a very long character vector it will take a long time to render. I've
chosen the \texttt{str()} function as it will automatically truncate the
data.
\end{tcolorbox}

The columns are \texttt{doc\_id} and \texttt{text}. \texttt{doc\_id} is
created by readtext to index each file that is read in. The
\texttt{text} column is where the text appears. The fact that we only
have one row means that all the text in the entire file is contained in
one cell! We will want to break this cell up into rows for each
sentence, but for now let's work with getting the columns to line up
with our idealized dataset structure.

First let's change the type of data frame that we are working with to a
tibble. This will make sure we don't accidentally print hundreds of
lines to our R Markdown output and/ or the R Console. Then we will
rename the \texttt{doc\_id} column to \texttt{type} and change the value
of that column to ``Target'' (for English) and ``Source'' (for Spanish).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_target }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_en }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Target"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Target\textquotesingle{}}

\NormalTok{europarle\_source }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle\_es }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# readtext data frame}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# convert to tibble}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{type =}\NormalTok{ doc\_id) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# rename doc\_id to type}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"Source"}\NormalTok{) }\CommentTok{\# change type value to \textquotesingle{}Source\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

We have two objects now, one corresponding to the `Source' and the other
the `Target' parallel texts. Let's now join these two datasets, one on
top of the other --that is, by rows. We wil use the
\texttt{bind\_rows()} function for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle }\OtherTok{\textless{}{-}} 
  \FunctionTok{bind\_rows}\NormalTok{(europarle\_target, europarle\_source)}

\FunctionTok{str}\NormalTok{(europarle) }\CommentTok{\# inspect the structure of the object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> tibble [2 x 2] (S3: tbl_df/tbl/data.frame)
#>  $ type: chr [1:2] "Target" "Source"
#>  $ text: chr [1:2] "Resumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece"| __truncated__ "Reanudacin del perodo de sesiones\nDeclaro reanudado el perodo de sesiones del Parlamento Europeo, interrump"| __truncated__
\end{verbatim}

The \texttt{europarle} dataset now has 2 columns, as before, and 2 rows
--each corresponding to the distinct language types (Source/ Target).

Remember our goal is to create a dataset structure with three columns
\texttt{type}, \texttt{sentence\_id}, and \texttt{sentence}. At the
moment we have \texttt{type} and \texttt{text} --where \texttt{text} has
all of the sentences in for each type in a cell. So we are going to want
to break up the \texttt{text} column into sentences, group the sentences
that are created by \texttt{type}, and then number these sentences so
that they are aligned between the distinct types.

To break up the text into sentences we are going to turn to the tidytext
package. This package has a extremely useful function
\texttt{unnest\_tokens()} which provides an effective way to break text
into various units (see \texttt{?tidytext::unnest\_tokens} for a full
list of token types). Since I know from looking at the raw text that
each sentence is on its own line, the best strategy to break the text
into sentence units is to find a way to break each line into a new row
in our dataset. To do this we need to use the \texttt{token\ =\ "regex"}
(for Regular Expression) and use the
\texttt{pattern\ =\ "\textbackslash{}\textbackslash{}n"} which tells R
to look for carriage returns to use as the breaking criterion.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{europarle\_sentences }\OtherTok{\textless{}{-}} 
\NormalTok{  europarle }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  tidytext}\SpecialCharTok{::}\FunctionTok{unnest\_tokens}\NormalTok{(}\AttributeTok{output =}\NormalTok{ sentence, }\CommentTok{\# new column}
                          \AttributeTok{input =}\NormalTok{ text, }\CommentTok{\# column to find text}
                          \AttributeTok{token =} \StringTok{"regex"}\NormalTok{, }\CommentTok{\# use a regular expression to break up the text}
                          \AttributeTok{pattern =} \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{n"}\NormalTok{, }\CommentTok{\# break text by carriage returns (returns after lines)}
                          \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# do not lowercase the text}

\FunctionTok{glimpse}\NormalTok{(europarle\_sentences) }\CommentTok{\# preview the structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> Rows: 3,926,375
#> Columns: 2
#> $ type     <chr> "Target", "Target", "Target", "Target", "Target", "Target", "~
#> $ sentence <chr> "Resumption of the session", "I declare resumed the session o~
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, opacitybacktitle=0.6, breakable, colframe=quarto-callout-warning-color-frame, arc=.35mm, left=2mm, leftrule=.75mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Tip}, opacityback=0, colback=white, toptitle=1mm, rightrule=.15mm, titlerule=0mm, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, colbacktitle=quarto-callout-warning-color!10!white, toprule=.15mm]
Regular Expressions are a powerful pattern matching syntax. They are
used extensively in text manipulation and we will see them again and
again. A good website to practice Regular Expressions is
\href{https://regex101.com/}{RegEx101}. You can also install the
regexplain package in R to get access to a useful
\href{https://rstudio.github.io/rstudioaddins/}{RStudio Addin}.
\end{tcolorbox}

Our new \texttt{europarle\_sentences} object is a data frame with almost
4 million rows! The final step to get to our envisioned dataset
structure is to add the \texttt{sentence\_id} column which will be
calculated by grouping the data by \texttt{type} and then assigning a
row number to each of the sentences in each group.

\hypertarget{transform-datasets-chapter}{%
\chapter{Transform datasets}\label{transform-datasets-chapter}}

\ldots{}

\part{Analysis}

\ldots{}

\hypertarget{inference-chapter}{%
\chapter{Inference}\label{inference-chapter}}

\ldots{}

\hypertarget{prediction-chapter}{%
\chapter{Prediction}\label{prediction-chapter}}

\ldots{}

\hypertarget{exploration-chapter}{%
\chapter{Exploration}\label{exploration-chapter}}

\ldots{}

\part{Communication}

\ldots{}

\hypertarget{reporting-chapter}{%
\chapter{Reporting}\label{reporting-chapter}}

\ldots{}

\hypertarget{collaboration-chapter}{%
\chapter{Collaboration}\label{collaboration-chapter}}

\ldots{}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Ackoff1989}{}}%
Ackoff, Russell L. 1989. {``From Data to Wisdom.''} \emph{Journal of
Applied Systems Analysis} 16 (1): 3--9.

\leavevmode\vadjust pre{\hypertarget{ref-Adel2020}{}}%
del, Annelie. 2020. {``Corpus Compilation.''} In \emph{A Practical
Handbook of Corpus Linguistics}, edited by Magali Paquot and Stefan Th.
Gries, 3--24. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Almeida2011a}{}}%
Almeida, Tiago A, Jos'e Mara G'omez Hildago, and Akebo Yamakami. 2011.
{``Contributions to the Study of SMS Spam Filtering: New Collection and
Results.''} In \emph{Proceedings of the 2011 ACM Symposium on Document
Engineering (DOCENG'11)}, 4. Mountain View, CA.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2004}{}}%
Baayen, R. Harald. 2004. {``Statistics in Psycholinguistics: A Critique
of Some Current Gold Standards.''} \emph{Mental Lexicon Working Papers}
1 (1): 1--47.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2011}{}}%
---------. 2011. {``Corpus Linguistics and Naive Discriminative
Learning.''} \emph{Revista Brasileira de
Lingu\textbackslash'\textbackslash istica Aplicada} 11 (2): 295--328.

\leavevmode\vadjust pre{\hypertarget{ref-Bao2019}{}}%
Bao, Wang, Ning Lianju, and Kong Yue. 2019. {``Integration of
Unsupervised and Supervised Machine Learning Algorithms for Credit Risk
Assessment.''} \emph{Expert Systems with Applications} 128 (August):
301--15. \url{https://doi.org/10.1016/j.eswa.2019.02.033}.

\leavevmode\vadjust pre{\hypertarget{ref-R-quanteda.corpora}{}}%
Benoit, Kenneth. 2020. \emph{Quanteda.corpora: A Collection of Corpora
for Quanteda}. \url{http://github.com/quanteda/quanteda.corpora}.

\leavevmode\vadjust pre{\hypertarget{ref-Brown2005}{}}%
Brown, Keith. 2005. \emph{Encyclopedia of Language and Linguistics}.
Vol. 1. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Buckheit1995}{}}%
Buckheit, Jonathan B., and David L. Donoho. 1995. {``Wavelab and
Reproducible Research.''} In \emph{Wavelets and Statistics}, 55--81.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Bychkovska2017}{}}%
Bychkovska, Tetyana, and Joseph J. Lee. 2017. {``At the Same Time:
Lexical Bundles in L1 and L2 University Student Argumentative
Writing.''} \emph{Journal of English for Academic Purposes} 30
(November): 38--52. \url{https://doi.org/10.1016/j.jeap.2017.10.008}.

\leavevmode\vadjust pre{\hypertarget{ref-Carmi2020}{}}%
Carmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.
2020. {``Data Citizenship: Rethinking Data Literacy in the Age of
Disinformation, Misinformation, and Malinformation.''} \emph{Internet
Policy Review} 9 (2).

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2020}{}}%
Chambers, John M. 2020. {``S, r, and Data Science.''} \emph{Proceedings
of the ACM on Programming Languages} 4 (HOPL): 1--17.
\url{https://doi.org/10.1145/3386334}.

\leavevmode\vadjust pre{\hypertarget{ref-Chan2014}{}}%
Chan, Sin-wai. 2014. \emph{Routledge Encyclopedia of Translation
Technology}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-Conway2012}{}}%
Conway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul
Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.
2012. {``Does Complex or Simple Rhetoric Win Elections? An Integrative
Complexity Analysis of u.s. Presidential Campaigns.''} \emph{Political
Psychology} 33 (5): 599--618.
\url{https://doi.org/10.1111/j.1467-9221.2012.00910.x}.

\leavevmode\vadjust pre{\hypertarget{ref-Cross2006}{}}%
Cross, Nigel. 2006. {``Design as a Discipline.''} \emph{Designerly Ways
of Knowing}, 95--103.

\leavevmode\vadjust pre{\hypertarget{ref-DataNeverSleeps08-2021}{}}%
{``Data Never Sleeps 7.0 Infographic.''} 2019.
https://www.domo.com/learn/infographic/data-never-sleeps-7.

\leavevmode\vadjust pre{\hypertarget{ref-Deshors2016}{}}%
Deshors, Sandra C, and Stefan Th. Gries. 2016. {``Profiling Verb
Complementation Constructions Across New Englishes.''}
\emph{International Journal of Corpus Linguistics.} 21 (2): 192--218.

\leavevmode\vadjust pre{\hypertarget{ref-Desjardins2019}{}}%
Desjardins, Jeff. 2019. {``How Much Data Is Generated Each Day?''}
\emph{Visual Capitalist}.

\leavevmode\vadjust pre{\hypertarget{ref-Donoho2017}{}}%
Donoho, David. 2017. {``50 Years of Data Science.''} \emph{Journal of
Computational and Graphical Statistics} 26 (4): 745--66.
\url{https://doi.org/10.1080/10618600.2017.1384734}.

\leavevmode\vadjust pre{\hypertarget{ref-Dubnjakovic2010}{}}%
Dubnjakovic, Ana, and Patrick Tomlin. 2010. \emph{A Practical Guide to
Electronic Resources in the Humanities}. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Eisenstein2012}{}}%
Eisenstein, Jacob, Brendan O'Connor, Noah A Smith, and Eric P Xing.
2012. {``Mapping the Geographical Diffusion of New Words.''}
\emph{Computation and Language}, 1--13.
\url{https://doi.org/10.1371/journal.pone.0113114}.

\leavevmode\vadjust pre{\hypertarget{ref-Gandrud2015}{}}%
Gandrud, Christopher. 2015.
\emph{\href{https://www.ncbi.nlm.nih.gov/pubmed/17811671}{Reproducible
Research with r and r Studio}}. Second edition. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-Gentleman2007}{}}%
Gentleman, Robert, and Duncan Temple Lang. 2007. {``Statistical Analyses
and Reproducible Research.''} \emph{Journal of Computational and
Graphical Statistics} 16 (1): 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-Gilquin:2009}{}}%
Gilquin, Gatanelle, and Stefan Th Gries. 2009. {``Corpora and
Experimental Methods: A State-of-the-Art Review.''} \emph{Corpus
Linguistics and Linguistic Theory} 5 (1): 1--26.
\url{https://doi.org/10.1515/CLLT.2009.001}.

\leavevmode\vadjust pre{\hypertarget{ref-Gomez-Uribe2015}{}}%
Gomez-Uribe, Carlos A., and Neil Hunt. 2015. {``The Netflix Recommender
System: Algorithms, Business Value, and Innovation.''} \emph{ACM
Transactions on Management Information Systems (TMIS)} 6 (4): 1--19.

\leavevmode\vadjust pre{\hypertarget{ref-Gries2013a}{}}%
Gries, Stefan Th. 2013. \emph{Statistics for Linguistics with r. A
Practical Introduction}. 2nd revise.

\leavevmode\vadjust pre{\hypertarget{ref-Grieve2018}{}}%
Grieve, Jack, Andrea Nini, and Diansheng Guo. 2018. {``Mapping Lexical
Innovation on American Social Media.''} \emph{Journal of English
Linguistics} 46 (4): 293--319.

\leavevmode\vadjust pre{\hypertarget{ref-Head2015}{}}%
Head, Megan L., Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D.
Jennions. 2015. {``The Extent and Consequences of p-Hacking in
Science.''} \emph{PLOS Biology} 13 (3): e1002106.
\url{https://doi.org/10.1371/journal.pbio.1002106}.

\leavevmode\vadjust pre{\hypertarget{ref-Ignatow2017}{}}%
Ignatow, Gabe, and Rada Mihalcea. 2017. \emph{An Introduction to Text
Mining: Research Design, Data Collection, and Analysis}. Sage
Publications.

\leavevmode\vadjust pre{\hypertarget{ref-Jaeger:2007a}{}}%
Jaeger, T Florian, and Neal Snider. 2007. {``Implicit Learning and
Syntactic Persistence: Surprisal and Cumulativity.''} \emph{University
of Rochester Working Papers in the Language Sciences} 3 (1).

\leavevmode\vadjust pre{\hypertarget{ref-Jurafsky2020}{}}%
Jurafsky, Daniel, and James H. Martin. 2020. \emph{Speech and Language
Processing}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rtweet}{}}%
Kearney, Michael W., Llus Revilla Sancho, and Hadley Wickham. 2022.
\emph{Rtweet: Collecting Twitter Data}.
\url{https://CRAN.R-project.org/package=rtweet}.

\leavevmode\vadjust pre{\hypertarget{ref-Kerr1998}{}}%
Kerr, Norbert L. 1998. {``HARKing: Hypothesizing After the Results Are
Known.''} \emph{Personality and Social Psychology Review} 2 (3):
196--217.

\leavevmode\vadjust pre{\hypertarget{ref-Kloumann2012}{}}%
Kloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. {``Positivity
of the English Language.''} \emph{PloS One}.

\leavevmode\vadjust pre{\hypertarget{ref-Kowsari2019}{}}%
Kowsari, Kamran, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana
Mendu, Laura E. Barnes, and Donald E. Brown. 2019. {``Text
Classification Algorithms: A Survey.''} \emph{Information} 10 (4): 150.
\url{https://doi.org/10.3390/info10040150}.

\leavevmode\vadjust pre{\hypertarget{ref-Kucera1967}{}}%
Kucera, H, and W N Francis. 1967. \emph{Computational Analysis of
Present Day American English}. Brown University Press Providence.

\leavevmode\vadjust pre{\hypertarget{ref-Lantz2013}{}}%
Lantz, Brett. 2013. \emph{Machine Learning with r}. Birmingham: Packt
Publishing.

\leavevmode\vadjust pre{\hypertarget{ref-Lewis2004}{}}%
Lewis, Michael. 2004. \emph{Moneyball: The Art of Winning an Unfair
Game}. WW Norton \& Company.

\leavevmode\vadjust pre{\hypertarget{ref-Lozano2009}{}}%
Lozano, Crist'obal. 2009. {``CEDEL2: Corpus Escrito Del Espaol L2.''}
\emph{Applied Linguistics Now: Understanding Language and Mind/La
Lingstica Aplicada Hoy: Comprendiendo El Lenguaje y La Mente. Almera:
Universidad de Almera}, 197--212.

\leavevmode\vadjust pre{\hypertarget{ref-Marwick2018}{}}%
Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. {``Packaging
Data Analytical Work Reproducibly Using r (and Friends).''} \emph{The
American Statistician} 72 (1): 80--88.

\leavevmode\vadjust pre{\hypertarget{ref-Millikan1923}{}}%
Millikan, Robert A. 1923. \emph{The Electron and the Light-Quant from
the Experimental Point of View}. \emph{Nobel Prize Acceptance Speech}.

\leavevmode\vadjust pre{\hypertarget{ref-Mosteller1963}{}}%
Mosteller, Frederick, and David L Wallace. 1963.
{``\href{https://www.ncbi.nlm.nih.gov/pubmed/3427}{Inference in an
Authorship Problem}.''} \emph{Journal of the American Statistical
Association} 58 (302): 275--309.

\leavevmode\vadjust pre{\hypertarget{ref-Olohan2008}{}}%
Olohan, Maeve. 2008. {``Leave It Out! Using a Comparable Corpus to
Investigate Aspects of Explicitation in Translation.''} \emph{Cadernos
de Traduo}, 153--69.

\leavevmode\vadjust pre{\hypertarget{ref-Paquot2020a}{}}%
Paquot, Magali, and Stefan Th. Gries, eds. 2020. \emph{A Practical
Handbook of Corpus Linguistics}. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-roediger:2000}{}}%
Roediger, H. L. L, and K. B. B McDermott. 2000. {``Distortions of
Memory.''} \emph{The Oxford Handbook of Memory}, 149--62.

\leavevmode\vadjust pre{\hypertarget{ref-Saxena2020}{}}%
Saxena, Shweta, and Manasi Gyanchandani. 2020. {``Machine Learning
Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:
A Narrative Review.''} \emph{Journal of Medical Imaging and Radiation
Sciences} 51 (1): 182--93.

\leavevmode\vadjust pre{\hypertarget{ref-Talarico2003}{}}%
Talarico, Jennifer M., and David C. Rubin. 2003. {``Confidence, Not
Consistency, Characterizes Flashbulb Memories.''} \emph{Psychological
Science} 14 (5): 455--61. \url{https://doi.org/10.1111/1467-9280.02453}.

\leavevmode\vadjust pre{\hypertarget{ref-Voigt2017}{}}%
Voigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.
Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan
Jurafsky, and Jennifer L. Eberhardt. 2017. {``Language from Police Body
Camera Footage Shows Racial Disparities in Officer Respect.''}
\emph{Proceedings of the National Academy of Sciences} 114 (25):
6521--26.

\leavevmode\vadjust pre{\hypertarget{ref-Wickham2014a}{}}%
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (10). \url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rvest}{}}%
---------. 2022. \emph{Rvest: Easily Harvest (Scrape) Web Pages}.
\url{https://CRAN.R-project.org/package=rvest}.

\leavevmode\vadjust pre{\hypertarget{ref-Wulff2007}{}}%
Wulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. {``Brutal Brits
and Persuasive Americans.''} \emph{Aspects of Meaning}.

\end{CSLReferences}

\appendix
\addcontentsline{toc}{part}{Appendices}

\hypertarget{data-appendix}{%
\chapter{Data}\label{data-appendix}}

\ldots{}



\printindex

\end{document}
