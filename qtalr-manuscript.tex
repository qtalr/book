% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreport}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
  \setmonofont[Scale=0.8]{PT Mono}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{2}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{255,255,255}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}

% create index
\usepackage{makeidx}
\makeindex

% Wrap 
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

% wrap code blocks
\usepackage{fvextra}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}

% typography
\usepackage{microtype}

% https://github.com/hadley/r-pkgs/blob/main/latex/preamble.tex 
% No widow lines
\widowpenalty10000 % avoid leaving lines behind
\clubpenalty10000  % avoid creating orphans
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{File}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\@ifundefined{codebgcolor}{\definecolor{codebgcolor}{HTML}{f9f9f9}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\hypersetup{
  pdftitle={An Introduction to Quantitative Text Analysis for Linguistics},
  pdfauthor={Jerid Francom},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{An Introduction to Quantitative Text Analysis for Linguistics}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Reproducible Research using R}
\author{Jerid Francom}
\date{September 19, 2023}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, frame hidden, colback={codebgcolor}, borderline west={3pt}{0pt}{shadecolor}, sharp corners, boxrule=0pt, enhanced]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}

\markboth{Welcome}{Welcome}

The goal of this textbook is to provide readers with foundational
knowledge and practical skills in quantitative text analysis using the R
programming language.

By the end of this textbook, readers will be able to identify, interpret
and evaluate data analysis procedures and results to support research
questions within language science. Additionally, readers will gain
experience in designing and implementing research projects that involve
processing and analyzing textual data employing modern programming
strategies. This textbook aims to instill a strong sense of reproducible
research practices, which are critical for promoting transparency,
verification, and sharing of research findings.

This textbook is geared towards advanced undergraduates, graduate
students, and researchers looking to expand their methodological
toolbox. It assumes no prior knowledge of programming or quantitative
methods and prioritizes practical application and intuitive
understanding over technical details.

\textbf{About the author}

Dr.~Jerid Francom is Associate Professor of Spanish and Linguistics at
Wake Forest University. His research focuses on the use of language
corpora from a variety of sources (news, social media, and other
internet sources) to better understand the linguistic and cultural
similarities and differences between language varieties for both
scholarly and pedagogical projects. He has published on topics including
the development, annotation, and evaluation of linguistic corpora and
analyzed corpora through corpus, psycholinguistic, and computational
methodologies. He also has experience working with and teaching
statistical programming with R.

\hypertarget{license}{%
\section*{License}\label{license}}
\addcontentsline{toc}{section}{License}

\markright{License}

This work by \href{https://francojc.github.io/}{Jerid C. Francom} is
licensed under a Creative Commons
Attribution-NonCommercial-NoDerivatives 4.0 International License.

\hypertarget{credits}{%
\section*{Credits}\label{credits}}
\addcontentsline{toc}{section}{Credits}

\markright{Credits}

\href{https://fontawesome.com/}{Font Awesome Icons} are SIL OFL 1.1
Licensed

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

The development of this book has benefited from the generous feedback
from the following people: Andrea Bowling, Caroline Brady, Declan
Golsen, Asya Little, Claudia Valdez, Laura Aull, Jack Nelson, \emph{(add
your name here!)}. As always, any errors or omissions are my own.

\hypertarget{build-information}{%
\section*{Build information}\label{build-information}}
\addcontentsline{toc}{section}{Build information}

\markright{Build information}

This textbook was built with the \texttt{quarto} package
(\protect\hyperlink{ref-R-quarto}{Allaire 2022}) and the
\texttt{bookdown} package (\protect\hyperlink{ref-R-bookdown}{Xie
2023a}) for R. The source code for this book is available on
\href{https://github.com/qtalr/book}{GitHub}.

This version of the textbook was built with R version 4.3.1 (2023-06-16)
on macOS Ventura 13.5.2 with the following packages:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1264}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1149}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.7586}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
version
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
source
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
dplyr & 1.1.3 & CRAN (R 4.3.1) \\
ggplot2 & 3.4.3 & CRAN (R 4.3.1) \\
here & 1.0.1 & CRAN (R 4.3.0) \\
knitr & 1.44 & CRAN (R 4.3.1) \\
qtalrkit & 0.0.3.300 & Github
(qtalr/qtalrkit@37814b5085b0e62a95532fbdb3a782d28b44f00a) \\
readr & 2.1.4 & CRAN (R 4.3.0) \\
reprex & 2.0.2 & CRAN (R 4.3.0) \\
rmarkdown & 2.24 & CRAN (R 4.3.1) \\
rstudioapi & 0.15.0 & CRAN (R 4.3.1) \\
scales & 1.2.1 & CRAN (R 4.3.0) \\
stringr & 1.5.0 & CRAN (R 4.3.0) \\
tibble & 3.2.1 & CRAN (R 4.3.0) \\
tidyr & 1.3.0 & CRAN (R 4.3.0) \\
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{sec-preface}{%
\chapter*{Preface}\label{sec-preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
The journey of a thousand miles begins with one step.

--- Lao Tzu
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Comprehend the book's rationale, learning goals, and pedagogical
  approach.
\item
  Navigate and engage with the book's structure and content effectively.
\item
  Interpret conventions used in the book reliably.
\item
  Set up the computing environment and utilize textbook and support
  resources for an optimal learning experience.
\end{itemize}

\end{tcolorbox}

The purpose of this chapter is to present the rationale behind this
textbook, outline the key learning objectives, describe the pedagogical
approach, and identify the intended audience. Additionally, this chapter
will provide readers with a guide to the book's structure and the scope
of its content, as well as instructions for the instructor and a summary
of supporting resources available. Finally, this chapter will provide
readers with information on setting up their computing environment and
where to seek support.

\hypertarget{sec-p-rationale}{%
\section*{Rationale}\label{sec-p-rationale}}
\addcontentsline{toc}{section}{Rationale}

\markright{Rationale}

Data science, an interdisciplinary field that combines knownledge and
skills from statistics, computer science, and domain-specific expertise
to extract meaningful insight from structured and unstructured data, has
emerged as an exciting and rapidly growing field in recent years, driven
in large part by the increase in computing power available to the
average individual and the abundance of electronic data now available
through the internet. These advances have become an integral part of the
modern scientific landscape, with data-driven insights now being used to
inform decision-making in a wide variety of academic fields, including
linguistics and language-related disciplines.

This textbook seeks to meet this growing demand by providing an
introduction to the fundamental concepts and practical programming
skills from data science applied to the task of quantitative text
analysis. It is intended primarily for undergraduate students, but may
also be useful for graduates and researchers seeking to expand their
methodological toolbox. The textbook takes a pedagogical approach which
assumes no prior experience with statistics or programming, making it an
accessible resource for novices beginning their exploration of
quantitative text analysis methods.

\hypertarget{sec-p-aims}{%
\section*{Aims}\label{sec-p-aims}}
\addcontentsline{toc}{section}{Aims}

\markright{Aims}

The overarching goal of this textbook is to provide readers with
foundational knowledge and practical skills to conduct and evaluate
quantitative text analysis using the R programming language and other
open source tools and technologies. The specific aims are to develop the
reader's proficiency in three main areas:

\begin{itemize}
\tightlist
\item
  \textbf{Data literacy:} Identify, interpret and evaluate data analysis
  procedures and results
\end{itemize}

\begin{quote}
Throughout this textbook we will explore topics which will help you
understand how data analysis methods derive insight from data. In this
process you will be encouraged to critically evaluate connections across
linguistic and language-related disciplines using data analysis
knowledge and skills. Data literacy is an invaluable skillset for
academics and professionals but also is an indispensable aptitude for in
the 21st century citizens to navigate and actively participate in the
`Information Age' in which we live
(\protect\hyperlink{ref-Carmi2020}{Carmi et al. 2020}).
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Research skills:} Design, implement, and communicate
  quantitative research
\end{itemize}

\begin{quote}
This aim does not differ significantly, in spirit, from common learning
outcomes in a research methods course. However, working with text will
incur a series of key steps in the selection, collection, and
preparation of the data that are unique to text analysis projects. In
addition, I will stress the importance of research documentation and
creating reproducible research as an integral part of modern scientific
inquiry (\protect\hyperlink{ref-Buckheit1995}{Buckheit and Donoho
1995}).
\end{quote}

\begin{itemize}
\tightlist
\item
  \textbf{Programming skills:} Apply programmatic strategies to develop
  and collaborate on reproducible research projects
\end{itemize}

\begin{quote}
Modern data analysis, and by extension, text analysis is conducted using
programming. There are various key reasons for this: a programming
approach (1) affords researchers unlimited research freedom --if you can
envision it, you can program it, (2) underlies well-documented and
reproducible research (\protect\hyperlink{ref-Gandrud2015}{Gandrud
2015}), and (3) invites researchers to engage more intimately with the
data and the methods for analysis.
\end{quote}

These aims are important for linguistics students because they provide a
foundation for concepts and in the skills required to succeed in the
rapidly evolving landscape of 21st-century research. These abilities
enable researchers to evaluate and conduct high-quality empirical
investigation across linguistic fields on a wide variety of topics.
Moreover, these skills go beyond linguistics research; they are widely
applicable across many disciplines where quantitative data analysis and
programming are becoming increasingly important. Thus, this textbook
provides students with a comprehensive introduction to quantitative text
analysis that is relevant to linguistics research and that equips them
with valuable skills for their future careers.

\hypertarget{sec-p-approach}{%
\section*{Approach}\label{sec-p-approach}}
\addcontentsline{toc}{section}{Approach}

\markright{Approach}

The approach taken in this textbook is designed to accomodate
linguistics students and researchers with little to no prior experience
with programming or quantitative methods. With this in mind the
objective is connect conceptual understanding with practical
application. Real-world data and research tasks relevant to linguistics
are used thoughtout the book to provide context and to motivate the
learning process\footnote{Research data and questions are primarily
  based on English for wide accessibility as it is the \emph{de facto}
  language of academics and research. However, the methods and
  techniques presented in this textbook are applicable to many other
  languages.}. Furthermore, as an introduction to the field, the
textbook focuses on the most common and fundamental methods and
techniques for quantitative text analysis and prioritizes breadth over
depth and intuitive understanding over technical explanations. On the
programming side, the Tidyverse approach to programming in R will be
adopted. This approach provides a consistent syntax across different
packages and is known for its legibility, making it easier for readers
to understand and write code. Together, these strategies form an
approach that is intended to provide readers with an accessible resource
to gain a foothold in the field and to equip them with the knowledge and
skills to apply quantitative text analysis in their own research.

\hypertarget{sec-p-structure}{%
\section*{Structure}\label{sec-p-structure}}
\addcontentsline{toc}{section}{Structure}

\markright{Structure}

The aims and approach described above is reflected in the overall
structure of the book and each chapter.

\hypertarget{sec-p-structure-book}{%
\subsection*{Book level}\label{sec-p-structure-book}}
\addcontentsline{toc}{subsection}{Book level}

At the book level, there are five interdependent parts:

Part I ``Orientation'' provides the necessary background knowledge to
situate quantitative text analysis in the wider context of data analysis
and linguistic research and to provide a clearer picture of what text
analysis entails and its range of applications.

The subsequent parts are directly aligned with the data analysis
process. The building blocks of this process are reflected in `Data to
Insight Hierarchy (DIKI)' visualized in
Figure~\ref{fig-diki-hierarchy}\footnote{Adapted from Ackoff
  (\protect\hyperlink{ref-Ackoff1989}{1989}) and Rowley
  (\protect\hyperlink{ref-Rowley2007}{2007}).}.

\begin{figure}[H]

{\centering \includegraphics[width=3.26in,height=\textheight]{figures/p-diki.drawio.png}

}

\caption{\label{fig-diki-hierarchy}Data to Insight Hierarchy (DIKI)}

\end{figure}

The DIKI Hierarchy highlights the stages and intermediate steps required
to derive insight from data. Part II ``Foundations'' provides a
conceptual introduction to the DIKI Hierarchy and establishes
foundational knowledge about data, information, knowledge, and insight
which is fundamental to developing a viable research plan.

Parts III ``Preparation'' and IV ``Analysis'' focus on the
implementation process. Part III covers the steps involved in preparing
data for analysis, including data acquisition, curation, and
transformation. Part IV covers the steps involved in conducting
analysis, including exploratory, predictive, and inferential data
analysis.

The final part, Part V ``Communication'', covers the final stage of the
data analysis process, which is to communicate the results of the
analysis. This includes the structure and content of research reports as
well as the process of publishing, sharing, and collaborating on
research.

\hypertarget{sec-p-structure-chapter}{%
\subsection*{Chapter level}\label{sec-p-structure-chapter}}
\addcontentsline{toc}{subsection}{Chapter level}

At the chapter level, both conceptual and programming skills are
developed in stages\footnote{These stages attempt to capture the general
  progression of learning reflected in Bloom's Taxonomy (see Krathwohl
  (\protect\hyperlink{ref-Krathwohl2002}{2002}) for a description and
  revision).}. The chapter-level structure is consistent across chapters
and can be seen in Table~\ref{tbl-structure-approach}.

\hypertarget{tbl-structure-approach}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}@{}}
\caption{\label{tbl-structure-approach}The general structure of a
chapter including: the component, its purpose, where to find the
resource, and the target learning stage.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Outcomes & Identify the learning objectives for the chapter & Textbook &
Introduction \\
Overview & Provide a brief introduction to the chapter topic & Textbook
& Introduction \\
Coding Lessons & Teach programming techniques with hands-on interactive
exercises & GitHub & Skills \\
Content & Combine conceptual discussions and programming skills,
incorporating thought-provoking questions, relevant studies, and
advanced topic references & Textbook & Knowledge \\
Recipes & Offer step-by-step programming examples related to the chapter
& Resources website & Comprehension \\
Labs & Allow readers to apply chapter-specific concepts and techniques &
GitHub & Application \\
Summary & Review the key concepts and skills covered in the chapter &
Textbook & Review \\
Questions & Assess and expand the reader's knowledge and abilities &
Textbook & Assessment \\
\end{longtable}

Each chapter will begin with a list of key learning outcomes followed by
a brief introduction to the chapter's content. The goal is to orient the
reader to the chapter. Next there will be a prompt to complete the
interactive coding lesson(s) to introduce reader's to key programming
concepts related to the chapter though hands-on experience and then the
main content of the chapter will follow. The content will be a
combination of conceptual discussions and programming skills,
incorporating thought-provoking questions (`Consider this'), relevant
studies (`Case study'), and advanced topic references (`Dive deeper').
Together these components form the skills and knowledge phase. The next
phase is the application phase. This phase will include step-by-step
programming demonstrations related to the chapter (Recipes) and lab
exercises that allow readers to apply their knowledge and skills
chapter-related tasks. Finally the chapters conclude with a summary of
the key concepts and skills covered in the chapter and a set of
questions to assess and expand the reader's knowledge and abilities.

\hypertarget{sec-p-resources}{%
\section*{Resources}\label{sec-p-resources}}
\addcontentsline{toc}{section}{Resources}

\markright{Resources}

There are three main resources available to support the aims and
approach of this textbook. Firstly, the textbook itself provides prose
discussion, figures/ tables, R code, case studies, and thought and
practical exercises. Secondly, there is a companion R package called
\texttt{qtalrkit} (\protect\hyperlink{ref-R-qtalrkit}{Francom 2023}),
which includes functions for accessing data and datasets, as well as
various useful functions developed specifically for this textbook. In
addition, there is a comprehensive website
\href{https://qtalr.github.io/qtalrkit/}{Quantitative Text Analysis for
Linguistics Resources}(qtalr website) that includes programming
tutorials and demonstrations to enhance the reader's recognition of how
programming strategies are implemented. Finally, a
\href{https://github.com/qtalr}{GitHub repository} is provided which
contains both a set of interactive R programming lessons (Swirl) and lab
exercises designed to guide the reader through practical hands-on
programming applications. The companion \texttt{qtalrkit} package and
the GitHub repository are both under active development and will be
updated regularly to ensure that supplementary materials remain relevant
to the content of the text\footnote{Errata for the textbook is found on
  the qtalr website.}.

\hypertarget{sec-p-getting-started}{%
\section*{Getting started}\label{sec-p-getting-started}}
\addcontentsline{toc}{section}{Getting started}

\markright{Getting started}

Before jumping in to this and subsequent chapter's textbook activities,
it is important to prepare your computing environment and understand how
to take advantage of the resources available, both those directly and
indirectly associated with the textbook.

\hypertarget{sec-p-r-ides}{%
\subsection*{R and IDEs}\label{sec-p-r-ides}}
\addcontentsline{toc}{subsection}{R and IDEs}

Programming is the backbone for modern quantitative research. Among the
many programming languages available, R is a popular open-source
language and software environment for statistical computing. R is
popular with statisticians and has been adopted as the \emph{de facto}
language by many other fields in natural and social sciences, including
linguistics. It is freely downloadable from
\href{https://www.r-project.org/}{The R Project for Statistical
Programming} website and is available for
\href{https://cloud.r-project.org/}{macOS, Linux, and Windows} operating
systems.

Successfully installing R is rarely the last step in setting up your
R-enabled computing environment. The majority of R users also install an
\textbf{integrated development environment} (IDE). An IDE, such as
\href{https://posit.co/products/open-source/rstudio/}{RStudio} or
\href{https://code.visualstudio.com/}{Visual Studio Code}, provide a
\textbf{graphical user interface} (GUI) for working with R. In effect,
IDEs provide a dashboard for working with R and are designed to make it
easier to write and execute R code. IDEs also provide a number of other
useful features such as syntax highlighting, code completion, and
debugging. IDEs are not required to work with R but they are
\emph{highly} recommended.

Choosing to install R and an IDE on your personal computer, which is
know as your \textbf{local environment}, is not the only option to work
with R. You can also choose to work with R in the cloud, a
\textbf{remote environment}. There are a number of cloud-based options
for working with R, including
\href{https://www.rstudio.com/products/cloud/}{RStudio Cloud} and
\href{https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/r-developers-guide}{Microsoft
Azure}. These options provide a pre-configured R environment that you
can access from any computer with an internet connection. The advantage
of working in the cloud is that you do not need to install R or an IDE
on your local computer. The disadvantage is that you will need to be
connected to the internet to work with R and the free tiers for these
services are limited.

If you are new to R, you may want to consider working in the cloud to
get started. If you plan to continue to work with R in the future, you
will most likely want to install R and an IDE on your local computer or
explore using a \textbf{virtual environment}. Virtual environments, such
as \href{https://www.docker.com/}{Docker}, provide a way to use a
pre-configured computing environment or create your own that you can
share with others. Virtual environments are a good option if you want to
ensure that everyone in your research group is working with the same
computing environment. Pre-configured virtual environments exist for R
through the \href{https://rocker-project.org/}{Rocker project} and can
be used locally or in the cloud.

There are trade-offs in terms of cost, convenience, and flexibility when
choosing to work with R in a local, remote, or virtual environment. The
choice is yours and you can always change your mind later. The important
thing is to get started and begin learning R. Furthermore, any of the
approaches described here will be compatible with this textbook.

For more information and instructions on setting up an R environment
consult the following guides.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Guides}

\begin{itemize}
\tightlist
\item
  \href{https://qtalr.github.io/qtalrkit/articles/guide-0.html}{Installing
  R}
\item
  \href{https://qtalr.github.io/qtalrkit/articles/guide-1.html}{Choosing
  and setting up an IDE}
\item
  \href{https://qtalr.github.io/qtalrkit/articles/guide-2.html}{Working
  with R in remote and virtual environments}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-p-r-packages}{%
\subsection*{R packages}\label{sec-p-r-packages}}
\addcontentsline{toc}{subsection}{R packages}

As you progress in your R programming experience, you'll find yourself
leveraging code from other R users, which is typically provided as
packages. Packages are sets of functions and/or datasets that are freely
accessible for download, designed to perform a specific set of
interrelated tasks. They enhance the capabilities of R. Official R
packages can be found in repositories like
\href{https://cran.r-project.org/}{CRAN} (Comprehensive R Archive
Network), while other packages can be obtained from code-sharing
platforms such as \href{https://github.com/}{GitHub}.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

The Comprehensive R Archive Network (CRAN) includes groupings of popular
packages related to a given applied programming task called
\href{https://cran.r-project.org/web/views/}{Task Views}. Explore the
available CRAN Task Views listings. Note the variety of areas (tasks)
that are covered in this listing. Now explore in more detail one of the
following task views which are directly related to topics covered in
this textbook noting the associated packages and their descriptions: (1)
Cluster, (2) MachineLearning, (3) NaturalLanguageProcessing, or (4)
ReproducibleResearch.

\end{tcolorbox}

You will download a number of packages at different stages of this
textbook, but there is a set of packages that will be key to have from
the get go. Once you have access to a working R/ RStudio environment,
you can proceed to install the following packages.

Install the following packages from CRAN.

\begin{itemize}
\tightlist
\item
  \texttt{tidyverse} (\protect\hyperlink{ref-R-tidyverse}{Wickham 2023})
\item
  \texttt{rmarkdown} (\protect\hyperlink{ref-R-rmarkdown}{Allaire et al.
  2023})
\item
  \texttt{quarto} (\protect\hyperlink{ref-R-quarto}{Allaire 2022})
\item
  \texttt{tinytex} (\protect\hyperlink{ref-R-tinytex}{Xie 2023b})
\item
  \texttt{devtools} (\protect\hyperlink{ref-R-devtools}{Wickham et al.
  2022})
\item
  \texttt{usethis} (\protect\hyperlink{ref-R-usethis}{Wickham, Bryan, et
  al. 2023})
\item
  \texttt{swirl} (\protect\hyperlink{ref-R-swirl}{Kross et al. 2020})
\end{itemize}

You can do this by running the following code in an R console:

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# install key packages from CRAN}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"quarto"}\NormalTok{, }\StringTok{"tinytex"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{, }\StringTok{"usethis"}\NormalTok{, }\StringTok{"swirl"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

For instructions on how to install the \texttt{qtalrkit} package from
GitHub and download and use the interactive R programming lessons for
this textbook, see the following guides.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Guides}

\begin{itemize}
\tightlist
\item
  \href{https://qtalr.github.io/qtalrkit/articles/qtalrkit.html}{Getting
  started}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-p-git-github}{%
\subsection*{Git and GitHub}\label{sec-p-git-github}}
\addcontentsline{toc}{subsection}{Git and GitHub}

\href{https://github.com/}{GitHub} is a code sharing website. Modern
computing is highly collaborative and GitHub is a very popular platform
for sharing and collaborating on coding projects. The
\href{https://github.com/stars/francojc/lists/labs}{lab exercises for
this textbook} are shared on GitHub. To access and complete these
exercises you will need to
\href{https://github.com/signup?ref_cta=Sign+up\&ref_loc=header+logged+out\&ref_page=\%2F\&source=header-home}{sign
up for a (free) GitHub account} and then set up the version control
software \texttt{git} on your computing environment. \texttt{git} is the
conduit to interfacing GitHub and for many \texttt{git} will already be
installed on your computer (or cloud computing environment).

For more information and instructions on setting up version control
consult the following guide.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Guides}

\begin{itemize}
\tightlist
\item
  \href{https://qtalr.github.io/qtalrkit/articles/guide-3.html}{Setting
  up Git and GitHub}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-p-getting-help}{%
\subsection*{Getting help}\label{sec-p-getting-help}}
\addcontentsline{toc}{subsection}{Getting help}

The technologies employed in this approach to text analysis will include
a somewhat steep learning curve. And in all honesty, the learning never
stops! Both seasoned programmers and beginners alike need assistance.
Fortunately there is a very large community of programmers who have
developed many official support resources and who actively contribute to
official and unofficial discussion forums. Together these resources
provide many avenues for overcoming challenges.

In Table~\ref{tbl-support-resources}, I provide a list of steps for
seeking help with R.

\hypertarget{tbl-support-resources}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6500}}@{}}
\caption{\label{tbl-support-resources}Recommended order for seeking help
with R.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Resource
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Official R Documentation & Access the official documentation by
running \texttt{help(package\ =\ "package\_name")} in an R console. Use
the \texttt{?} operator followed by the package or function name. Check
out available Vignettes by running
\texttt{browseVignettes("package\_name")}. \\
2 & Web Search & Look for package documentation and vignettes on the
web. A popular site for this is R-Universe. \\
3 & RStudio IDE Help Toolbar & If you're using RStudio IDE, use the
``Help'' toolbar menu. It provides links to help resources, guides, and
manuals. \\
4 & Online Discussion Forums & Sites like Stack Overflow and RStudio
Community are great platforms where the programming community asks and
answers questions related to real-world issues. \\
5 & Post Questions with Reprex & When posting a question, especially
those involving coding issues or errors, provide enough background and
include a reproducible example (reprex) - a minimal piece of code that
demonstrates your issue. This helps others understand and answer your
question effectively. \\
\end{longtable}

The first place to look for help with R is the official documentation of
the R package you are using. You can access this documentation by
running \texttt{help(package\ =\ "package\_name")} in an R console or
using the \texttt{?} operator and then the package or function name.
Many R packages often include ``Vignettes'' (long-form documentation and
demonstrations). These can be accessed either by running
\texttt{browseVignettes()} in an R console with the package name in
quotes (e.g.~\texttt{browseVignettes("tidyverse")}).

You can also search the web for package documentation and vignettes. A
popular site for this purpose is
\href{https://r-universe.dev/search/}{R-Universe}.

If you are using the RStudio IDE, the easiest and most convenient place
to get help with either R or RStudio is through the RStudio ``Help''
toolbar menu. There you will find links to help resources, guides, and
manuals.

There are a number of very popular discussion forum websites where the
programming community asks and answers questions to real-world issues.
These sites often have subsections dedicated to particular programming
languages or software. The most popular of these sites is
\href{https://stackoverflow.com/}{Stack Overflow}. There are also
R-specific discussion forums such as
\href{https://community.rstudio.com/}{RStudio Community}.

If you post a question on one of these communities ensure that if your
question involves some coding issue or error that you provide enough
background such that the community will be able to help you. This is
often referred to as a \textbf{reproducible example} or ``reprex''. A
reprex is a minimal piece of code that demonstrates the issue you are
having. It is a very useful tool for both asking and answering
questions.

For information on how to create a reprex consult the following guide.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Guides}

\begin{itemize}
\tightlist
\item
  \href{https://qtalr.github.io/qtalrkit/articles/guide-4.html}{Creating
  reproducible examples}
\end{itemize}

\end{tcolorbox}

The take-home message here is that you are not alone. There are many
people world-wide that are learning to program and/ or contribute to the
learning of others. The more you engage with these resources and
communities the more successful your learning will be. As soon as you
are able, pay it forward. Posting questions and offering answers helps
the community and engages and refines your skills --a win-win.

\hypertarget{sec-p-conventions}{%
\section*{Conventions}\label{sec-p-conventions}}
\addcontentsline{toc}{section}{Conventions}

\markright{Conventions}

To facilitate the learning process, this textbook will employ a number
of conventions. These conventions are intended to help the reader
navigate the text and to signal the reader's attention to important
concepts and information.

\hypertarget{sec-p-prose}{%
\subsection*{Prose}\label{sec-p-prose}}
\addcontentsline{toc}{subsection}{Prose}

The following typographic conventions are used throughout the text:

\begin{itemize}
\tightlist
\item
  \emph{Italics}

  \begin{itemize}
  \tightlist
  \item
    Filenames, file extensions, directory paths, and URLs.
  \end{itemize}
\item
  \texttt{Fixed-width}

  \begin{itemize}
  \tightlist
  \item
    Package names, function names, variable names, and in-line code
    including expressions and operators.
  \end{itemize}
\item
  \textbf{Bold}

  \begin{itemize}
  \tightlist
  \item
    Key concepts when first introduced.
  \end{itemize}
\item
  \href{https://qtalr.github.io/qtalrkit/}{Linked text}

  \begin{itemize}
  \tightlist
  \item
    Links to internal and external resources, footnotes, and citations
    including references to R packages when first introduced.
  \end{itemize}
\end{itemize}

\hypertarget{sec-p-code-blocks}{%
\subsection*{Code blocks}\label{sec-p-code-blocks}}
\addcontentsline{toc}{subsection}{Code blocks}

More lengthy code will be presented in code blocks, as seen in
Example~\ref{exm-code-block}.

\begin{example}[]\protect\hypertarget{exm-code-block}{}\label{exm-code-block}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A function that takes a name and returns a greeting}
\NormalTok{greet }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(name) \{ }\CommentTok{\# function definition}
  \FunctionTok{paste}\NormalTok{(}\StringTok{"Hello"}\NormalTok{, name) }\CommentTok{\# print greeting}
\NormalTok{\} }\CommentTok{\# end function definition}

\FunctionTok{greet}\NormalTok{(}\AttributeTok{name =} \StringTok{"Jerid"}\NormalTok{) }\CommentTok{\# apply function to a name}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "Hello Jerid"
\end{verbatim}

\end{example}

There are a couple of things to note about the code in
Example~\ref{exm-code-block}. First, it shows the code that is run in R
as well as the ouput that is returned. The code will appear in a box and
the output will appear below the box. Both code and output will appear
in fixed-width font. Output which is text will be prefixed with
\texttt{\textgreater{}}. Second, the \texttt{\#} symbol is used to
signal a \textbf{code comment}, a human-facing description. Everything
right of a \texttt{\#} is not run as code. In this textbook you will see
code comments above code on a separate line and to the right of code on
the same line. It is good practice to comment your code to enhance
readability and to help others understand what your code is doing.

All figures, tables, and images in this textbook are generated by code
blocks but only code for those elements that are relevant for discussion
will be shown. However, if you wish to see the code for any element in
this textbook, you can visit the GitHub repository
\url{https://qtalr.github.io/book/}.

When a reference to a file and its contents is made, it will appear as
in File~\ref{lst-r}.

\begin{codelisting}

\caption{\texttt{example.R}: Example R script}

\hypertarget{lst-r}{%
\label{lst-r}}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}

\CommentTok{\# Add 1 and 1}
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\hypertarget{sec-p-callouts}{%
\subsection*{Callouts}\label{sec-p-callouts}}
\addcontentsline{toc}{subsection}{Callouts}

Callouts are used to signal the reader's attention to content, activity,
and other important sections. The following callouts are used in this
textbook:

\textbf{Content}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

Learning outcomes for the chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Points for you to consider and questions to explore appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-alt} Case study}

Case studies for applying conceptual knowledge and coding skills covered
in the chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

Links to additional resources for diving deeper into the topic appear
here.

\end{tcolorbox}

\textbf{Activities}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

Links to swirl lessons for practicing coding skills for the chapter
appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

Links to demonstration programming tasks on the qtalr site for the
chapter appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

Links to lab exercises for applying conceptual knowledge and coding
skills on the qtalr GitHub repository for the chapter appear here.

\end{tcolorbox}

\textbf{Other}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

Tips for using R and related tools appear here.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

Warnings for using R and related tools appear here.

\end{tcolorbox}

\hypertarget{sec-p-instructor}{%
\section*{To the instructor}\label{sec-p-instructor}}
\addcontentsline{toc}{section}{To the instructor}

\markright{To the instructor}

Depending on the experience level and expectations of your readers, you
may want to consider adopting one of the following course designs for
using this textbook.

\hypertarget{sec-p-basic-intro}{%
\subsection*{Basic Introduction}\label{sec-p-basic-intro}}
\addcontentsline{toc}{subsection}{Basic Introduction}

\begin{itemize}
\tightlist
\item
  Cover chapters 1-5 in sequence to give your readers a foundational
  understanding of quantitative text analysis.
\item
  Culminate the course with a research proposal assignment that requires
  them to identify an interesting linguistic problem, propose ways of
  solving it using the methods covered in class, and identify potential
  data sources.
\item
  If your readers have little to no experience with R, you may want to
  consider using the RStudio Cloud platform to host the course. This
  will provide them with a pre-installed R environment and allow them to
  focus on learning the material rather than troubleshooting.
\end{itemize}

\hypertarget{sec-p-intermediate-intro}{%
\subsection*{Intermediate Introduction}\label{sec-p-intermediate-intro}}
\addcontentsline{toc}{subsection}{Intermediate Introduction}

\begin{itemize}
\tightlist
\item
  Cover chapters 1, 5-10 in sequence to give your readers a deeper
  understanding of quantitative text analysis methods. Explore
  additional case studies or dataset examples throughout the course if
  you wish to supplement your lectures.
\item
  Culminate the course with a research project assignment that allows
  your readers to apply what they've learned to linguistic content of
  their choice.
\item
  You may consider using the RStudio Cloud platform to host the course,
  but ensure that your readers have access to R and RStudio on their own
  computers as well.
\end{itemize}

\hypertarget{sec-p-advanced-intro}{%
\subsection*{Advanced Introduction}\label{sec-p-advanced-intro}}
\addcontentsline{toc}{subsection}{Advanced Introduction}

\begin{itemize}
\tightlist
\item
  Cover all 12 chapters to give your readers a thorough understanding of
  quantitative text analysis concepts and techniques. Devote more time
  chapters 5-10 providing demonstrations of how to approach different
  problems and evaluating alternative approaches.
\item
  Culminate the course with a collaborative research project that
  requires your readers to work in groups to conduct a comprehensive
  analysis of a given dataset.
\item
  Ensure that your readers install R and RStudio on their own computers
  as they will need full control over their coding environment.
\end{itemize}

For all course designs, it is strongly recommend that you evaluate the
readers' success in understanding the material by providing a
combination of quizzes, lab assignments, programming exercises, and
written reports. Additionally, encourage your readers to ask
questions\footnote{If you are using this textbook in a course, consider
  using a CMS (\emph{e.g.} Canvas, Blackboard, etc.) or the web-based
  social annotation tool \href{https://hypothes.is/}{Hypothes.is} to
  facilitate reader questions and discussion.}, collaborate with peers,
and seek help from the ample resources available online when they
encounter scope-limited programming problems.

For more information on how to use this textbook in your course, visit
the
\href{https://qtalr.github.io/qtalrkit/articles/instructor-guide.html}{Instructor
Guide} on the compansion website.

\hypertarget{sec-p-activities}{%
\section*{Activities}\label{sec-p-activities}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

At this point you should have a working R environment with the core
packages including \texttt{qtalrkit} installed. You should also have
verified that you have a working Git environment and that you have a
GitHub account. If you have not completed these tasks, return to the
guides listed above in ``Getting started'' of this Preface and complete
them before proceeding.

The following activities are designed to help you become familiar with
the tools and resources that you will be using throughout this textbook.
These and subsequent activities are designed to be completed in the
order that they are presented in this textbook.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Intro to Swirl}\\
\textbf{How}: In the R console load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize you with navigating, selecting, and
completing swirl lessons.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-0.html}{Literate
Programming: writing with code}\\
\textbf{How}: Read Recipe 0 and participate in collaborative discussion
with peers.\\
\textbf{Why}: To introduce the concept of Literate Programming and how
to create literate documents using R and Quarto.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-0}{Literate
programming I}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 0.\\
\textbf{Why}: To put literate programming techniques covered in Recipe 0
into practice. Specifically, you will create and edit a Quarto document
and render a report in PDF format.

\end{tcolorbox}

\hypertarget{sec-p-summary}{%
\section*{Summary}\label{sec-p-summary}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In the Preface, we lay the groundwork by introducing the textbook's
underlying principles, learning goals, teaching methods, and target
audience. The chapter also offers advice on how to navigate the book's
layout, comprehend its subject matter, and make use of supplementary
materials. Crucial insights from this section involve grasping the
book's objectives and aims, which center around instructing readers on
quantitative text analysis for linguistics using R while emphasizing
reproducible research. This chapter assists readers in setting up a
working R development environment ensuring they can effectively engage
with the material. Moreover, the Preface provides guidance on how to get
help with R and other related software tools and deciphering conventions
in the text. With this foundation, you're now prepared to delve into the
captivating realm of text analysis in the subsequent chapter, titled
``Text Analysis in Context.''

\hypertarget{sec-p-questions}{%
\section*{Questions}\label{sec-p-questions}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Revise/ add questions.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  How is the textbook designed to be accessible for both novice and
  seasoned practitioners in the area of quantitative text analysis?
\item
  What is the purpose of the textbook and what are the three areas it
  aims to scaffold?
\item
  What are the main components of each chapter, and how are they
  structured to support learning outcomes?
\item
  How does the structure of the textbook and associated resources work
  to support learning and proficiency in areas?
\item
  What is the role of programmatic approaches in quantitative text
  analysis?
\item
  What is the relationship between R and an IDE (e.g.~RStudio, VS Code)?
\item
  What is the relationship between R and a version control system
  (e.g.~Git)?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Technical exercises}

\begin{itemize}
\tightlist
\item
  Install the latest version of R by following the instructions for your
  operating system. \url{https://cran.r-project.org/}
\item
  Install RStudio Desktop
  \url{https://www.rstudio.com/products/rstudio/download/}
\item
  Verify a Git installation or install Git (Windows:
  \url{https://git-scm.com/downloads}). Git a version control system
  that allows you to track changes to files and collaborate with others
  through GitHub.
\item
  Create a free GitHub account at \url{https://github.com/join}.
\item
  Install the \texttt{tidyverse} package in R by running
  \texttt{install.packages("tidyverse")} in the R Console pane.
\item
  Install the \texttt{swirl} package by running
  \texttt{install.packages("swirl")} in the R Console pane.
\item
  Open RStudio and create a new project for this textbook. This will
  help you keep your code and files organized.
\end{itemize}

\end{tcolorbox}

\part{Orientation}

In this section the aims are to: 1) provide an overview of quantitative
research and their applications, by both highlighting visible
applications and notable research in various fields, 2) consider how
quantitative research contributes to language research, and 3) layout
the main types of research and situate quantitative text analysis inside
these.

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Update the overview of Part I ``Orientation'' to
  reflect the new structure of the chapter.
\end{itemize}

\hypertarget{sec-text-analysis-in-context}{%
\chapter{Text analysis in context}\label{sec-text-analysis-in-context}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
Everything about science is changing because of the impact of
information technology and the data deluge.

--- Jim Gray
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Understand the role and goals of data analysis both within and outside
  of academia.
\item
  Describe the various approaches to quantitative language research.
\item
  Identify the applications of text analysis in different contexts.
\end{itemize}

\end{tcolorbox}

In this chapter I will aim to introduce the topic of text analysis and
provide the context needed to understand how text analysis fits in a
larger universe of science and the ever-ubiquitous methods of data
science, with attention to how linguistics and language-related studies
employ data analysis down to the particular area of text analysis.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Variables and
vectors, Workspace}\\
\textbf{How}: In an R console load \texttt{swirl}, run \texttt{swirl()},
and follow prompts to select the lesson.\\
\textbf{Why}: To explore some key building blocks of the R programming
language and to examine your local workspace in R and understand the
relationship between your R workspace and the file system of your
computing environment.

\end{tcolorbox}

\hypertarget{text-making-sense-of-a-complex-world}{%
\section{Making sense of a complex
world}\label{text-making-sense-of-a-complex-world}}

\hypertarget{heuristic-understanding}{%
\subsection{Heuristic Understanding}\label{heuristic-understanding}}

The world around us is full of actions and interactions so numerous that
it is difficult to really comprehend. As each individual sees and
experiences this world, we gain knowledge and build up heuristic
understanding about how it works and how we can interact with it. This
happens regardless of your educational background. As humans we are
built for this. Our minds process countless sensory inputs. They
underlie skills and abilities that we take for granted like being able
to predict what will happen if you see someone about to knock a wine
glass off a table and onto a concrete floor. You've never seen this
object before and this is the first time you've been to this winery, but
somehow and from somewhere you `instinctively' make an effort to warn
the would-be-glass-breaker before it is too late. You most likely have
not stopped to consider where this predictive knowledge comes from, or
if you have, you may have just chalked it up to `common sense'. As
common as it may be, it is an incredible display of the brain's capacity
to monitor your environment, relate the events and observations that
take place, and store that information all the time not making a big
fuss to tell your conscious mind what it's up to.

So wait, this is a textbook on text analysis, right? So what does all
this have to do with that? Well, there are two points to make that are
relevant for framing our journey: (1) the world is constantly churning
out data in real-time at a scale that is daunting and (2) for all the
power of the brain that works so efficiently behind the scene making
sense of the world, we are one individual living one life that has a
limited view of the world at large. Let me expand on these two points a
little more.

First let's be clear. There is no way for anyone to experience all
things at all times. But even extremely reduced slices of reality are
still vastly outside of our experiential capacity, at least in
real-time. One can make the point that since the inception of the
internet an individual's ability to experience larger slices of the
world has increased. But could you imagine reading, watching, and
listening to every file that is currently accessible on the web? Or has
been? (See the \href{https://web.archive.org/}{Wayback Machine}.) Scale
this down even further; let's take Wikipedia, the world's largest
encyclopedia. Can you imagine reading every wiki entry? As large as a
resource such as Wikipedia is \footnote{As of 22 July 2021, there are
  6,341,359 articles in the
  \href{https://en.wikipedia.org/wiki/English_Wikipedia}{English
  Wikipedia} containing over 3.9 billion words occupying around 19
  gigabytes of information.}, it is still a small fragment of the
written language that is produced on the web, just the web \footnote{For
  reference, \href{https://commoncrawl.org/big-picture/}{Common Crawl}
  has millions of gigabytes collected since 2008.}. Consider that for a
moment.

To my second framing point, which is actually two points in one. I
underscored the efficiency of our brain's capacity to make sense of the
world. That efficiency comes from some clever evolutionary twists that
lead our brain to take in the world but it makes some shortcuts that
compress the raw experience into heuristic understanding. What that
means is that the brain is not a supercomputer. It does not store every
experience in raw form, we do not have access to the records of our
experience like we would imagine a computer would have access to the
records logged in a database. Where our brains do excel is in making
associations and predictions that help us (most of the time) navigate
the complex world we inhabit. This point is key --our brains are doing
some amazing work, but that work can give us the impression that we
understand the world in more detail that we actually do. Let's do a
little thought experiment. Close your eyes and think about the last time
you saw your best friend. What were they wearing? Can you remember the
colors? If your like me, or any other human, you probably will have a
pretty confident feeling that you know the answers to these questions
and there is a chance you a right. But it has been demonstrated in
numerous experiments on human memory that our confidence does not
correlate with accuracy (\protect\hyperlink{ref-Talarico2003}{Talarico
and Rubin 2003}; \protect\hyperlink{ref-Roediger2000}{Roediger and
McDermott 2000}). You've experienced an event, but there is no real
reason that we should bet our lives on what we experienced. It's a
little bit scary, for sure, but the magic is that it works `good enough'
for practical purposes.

So here's the deal: as humans we are (1) clearly unable to experience
large swaths of experience by the simple fact that we are individuals
living individual lives and (2) the experiences we do live are not
recorded in memory with perfect precision and therefore we cannot
`trust' our intuitions, at least not in an absolute sense.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

How might your own experiences and biases influence your understanding
of the world? What are some ways that you can mitigate these biases? Is
ever possible to be completely objective? How might biases influence the
way you approach text analysis?

\end{tcolorbox}

\hypertarget{science-to-advance-understanding}{%
\subsection{Science to advance
understanding}\label{science-to-advance-understanding}}

What does that mean for our human curiosity about the world around us
and our ability to reliably make sense of it? In short it means that we
need to approach understanding our world with the tools of science.
Science starts with a question, identifies and collects data, careful
selected slices of the complex world, submits this data to analysis
through clearly defined and reproducible procedures, and reports the
results for others to evaluate. This process is repeated, modifying, and
manipulating the procedures, asking new questions and positing new
explanations, all in an effort to make inroads to bring the complex into
tangible view.

In essence what science does is attempt to subvert our inherent
limitations in understanding by drawing on carefully and purposefully
collected slices of observable experience and letting the analysis of
these observations speak, even if it goes against our intuitions (those
powerful but sometime spurious heuristics that our brains use to make
sense of the world).

\hypertarget{data-analysis}{%
\section{Data analysis}\label{data-analysis}}

\hypertarget{emergence-of-data-science}{%
\subsection{Emergence of data science}\label{emergence-of-data-science}}

At this point I've sketched an outline strengths and limitations of
humans' ability to make sense of the world and why science is used to
address these limitations. This science I've described is the one you
are familiar with and it has been an indispensable tool to make sense of
the world. If you are like me, this description of science may be
associated with visions of white coats, labs, and petri dishes. While
science's foundation still stands strong in the 21st century, a series
of intellectual and technological events mid-20th century set in motion
changes that have changed aspects about how science is done, not why it
is done. We could call this Science 2.0, but let's use the more
popularized term \index{data science}\textbf{data science}. The
recognized beginnings of data science are attributed to work in the
``Statistics and Data Analysis Research'' department at Bell Labs during
the 1960s. Although primarily conceptual and theoretic at the time, a
framework for quantitative data analysis took shape that would
anticipate what would come: sizable datasets which would ``{[}\ldots{]}
require advanced statistical and computational techniques {[}\ldots{]}
and the software to implement them.''
(\protect\hyperlink{ref-Chambers2020}{Chambers 2020}) This framework
emphasized both the inference-based research of traditional science, but
also embraced exploratory research and recognized the need to address
practical considerations that would arise when working with and deriving
insight from an abundance of machine-readable data.

Fast-forward to the 21st century a world in which machine-readable data
is truly in abundance. With increased computing power, the emergence of
the world wide web, and wide adoption of mobile devices electronic
communication skyrocketed around the globe. To put this in perspective,
in 2019 it was estimated that every minute 511 thousand tweets were
posted, 18.1 million text messages were sent, and 188 million emails
were sent (\protect\hyperlink{ref-DataNeverSleeps08-2021}{{``Data Never
Sleeps 7.0 Infographic''} 2019}). The data flood has not been limited to
language, there are more sensors and recording devices than ever before
which capture evermore swaths of the world we live in
(\protect\hyperlink{ref-Desjardins2019}{Desjardins 2019}). Where
increased computing power gave rise to the influx of data, it is also
one of the primary methods for gathering, preparing, transforming,
analyzing, and communicating insight derived from this data
(\protect\hyperlink{ref-Donoho2017}{Donoho 2017}). The vision laid out
in the 1960s at Bell Labs had come to fruition.

\hypertarget{computing-skills-statistical-knowledge-and-domain-knowledge}{%
\subsection{Computing skills, statistical knowledge, and domain
knowledge}\label{computing-skills-statistical-knowledge-and-domain-knowledge}}

Data science is not predicated on data alone. Turning data into insight
takes \textbf{computing skills} (i.e.~programming), \textbf{statistical
knowledge}, and \textbf{domain expertise}. This triad has been popularly
represented as a Venn diagram such as in
Figure~\ref{fig-intro-data-science-venn}.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/ta-ds-venn.drawio.png}

}

\caption{\label{fig-intro-data-science-venn}Data Science Venn Diagram
adapted from
\href{http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram}{Drew
Conway}.}

\end{figure}

The \textbf{computing skills} component of data science is the ability
to write code to perform the data analysis process. This is the primary
approach for working with data at scale. The \textbf{statistical
knowledge} component of data science is the ability to apply statistical
methods to data to derive insight. \textbf{Domain expertise} provides
researchers insight at key junctures in the development of a research
project and aid researchers in evaluating results.

This triad of skills in combination with reproducible research practices
is the foundational toolbelt of data science
(\protect\hyperlink{ref-Hicks2019}{Hicks and Peng 2019}).
\textbf{Reproducible research}\index{reproducible research} entails the
use of computational tools to automate the process of data analysis.
This automation is achieved by writing code that can be executed to
replicate the data analysis. This code can then be shared through code
sharing repositories, such as GitHub, where it can be viewed,
downloaded, and executed by others. This adds transparency to the
process and allows others to build on previous work. This is in contrast
to traditional approaches where data analysis is performed
(semi-)manually, results are reported in a static document such as a
report or journal article, and the data analysis process is not shared.
This approach is not reproducible because the data analysis process is
not transparent and cannot be replicated. This is problematic because it
is difficult to evaluate the results and build on previous work.
Reproducible research practices are a key component of data science and
are emphasized throughout this book.

\hypertarget{applications-of-data-science}{%
\subsection{Applications of data
science}\label{applications-of-data-science}}

Equipped with the data science toolbelt, the interest in deriving
insight from the available data is now almost ubiquitous. The science of
data has now reached deep into all aspects of life where making sense of
the world is sought. Predicting whether a loan applicant will get a loan
(\protect\hyperlink{ref-Bao2019}{Bao, Lianju, and Yue 2019}), whether a
lump is cancerous (\protect\hyperlink{ref-Saxena2020}{Saxena and
Gyanchandani 2020}), what films to recommend based on your previous
viewing history (\protect\hyperlink{ref-Gomez-Uribe2015}{Gomez-Uribe and
Hunt 2015}), what players a sports team should sign
(\protect\hyperlink{ref-Lewis2004}{Lewis 2004}) all now incorporate a
common set of data analysis tools.

The data science toolbelt also underlies well-known public-facing
language applications. From the language-capable chat applications,
plagiarism detection software, machine translation algorithms, and
search engines, tangible results of quantitative approaches to language
are becoming standard fixtures in our lives, as seen in
Figure~\ref{fig-intro-language-applications}.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures/ta-lang-venn.drawio.png}

}

\caption{\label{fig-intro-language-applications}Well-known public-facing
language applications}

\end{figure}

The spread of quantitative data analysis too has taken root in academia.
Even in areas that on first blush don't appear readily approachable in a
quantitative manner, such as fields in the social sciences and
humanities, data science is making important and sometimes disciplinary
changes to the way that academic research is conducted. This textbook
focuses in on a domain that cuts across many of these fields; namely
language. At this point let's turn to quantitative approaches to
language analysis as we work closer to contextualizing text analysis.

\hypertarget{language-analysis}{%
\section{Language analysis}\label{language-analysis}}

Language is a defining characteristic of our species. Since antiquity,
language has attracted interest across disciplines and schools of
thought. In the early 20th century, the development of the rigorous
approach to study of language as a field in its own right took root
(\protect\hyperlink{ref-Campbell2001}{Campbell 2001}), yet a plurality
of theoretical views and methodological approaches remained.
Contemporary linguistics bares this complex history and is far from
theoretically and methodologically unified.

Either based on the tenets of theoretical frameworks and/or the objects
of study of particular fields, approaches to language research vary. On
the one hand some language research commonly applies qualitative
assessment of language structure and/ or use. \textbf{Qualitative
approaches} describe and account for characteristics, or ``qualities'',
that can be observed, but not measured (\emph{e.g.} introspective
methods, ethnographic methods, \emph{etc.})

On the other hand other language research programs employ quantitative
research methods either out of necessity given the object of study
(phonetics, psycholinguistics, \emph{etc.}) or based on theoretical
principles (Cognitive Linguistics, Connectionism, \emph{etc.}).
\textbf{Quantitative approaches} involve measurements of properties of
language that can be observed and measured (\emph{e.g.} frequency of
use, reaction time, \emph{etc.}).

These latter research areas and theoretical paradigms employ methods
that share much of the common data analysis toolbox described in the
previous section. In effect, this establishes a common methodological
language between other language research fields but also with research
outside of linguistics.

However, there is never a one-size-fits all approach to anything --much
less data analysis. And even in quantitative language analysis there is
a key methodological distinction that has downstream effects in terms of
procedure but also in terms of interpretation. The key distinction that
we need to make at this point, which will provide context for our
introduction to quantitative text analysis, comes down to the approach
to collecting language data and the nature of that data. This
distinction is between \textbf{experimental
data}\index{experimental data} and \textbf{observational
data}\index{observational data}.

Experimental approaches start with a intentionally designed hypothesis
and lay out a research methodology with appropriate instruments and a
plan to collect data that shows promise for shedding light on the
validity of the hypothesis. Experimental approaches are conducted under
controlled contexts, usually a lab environment, in which participants
are recruited to perform a language related task with stimuli that have
been carefully curated by researchers to elicit some aspect of language
behavior of interest. Experimental approaches to language research are
heavily influenced by procedures adapted from psychology. This link is
logical as language is a central area of study in cognitive psychology.
This approach looks much like the white-coat science that we made
reference to earlier but, as in most quantitative research, has now
taken advantage of the data analysis toolbelt to collect and organize
much larger quantities of data and conduct statistically more robust
analysis procedures and communicate findings more efficiently.

Observational approaches are a bit more of a mixed bag in terms of the
rationale for the study; they may either start with a testable
hypothesis or in other cases may start with a more open-ended research
question to explore. But a more fundamental distinction between the two
is drawn in the amount of control the researcher has on contexts and
conditions in which the language behavior data to be collected is
produced. Observational approaches seek out records of language behavior
that is produced by language speakers for communicative purposes in
natural(istic) contexts. This may take place in labs (language
development, language disorders, \emph{etc.}), but more often than not,
language is collected from sources where speakers are performing
language as part of their daily lives --whether that be posting on
social media, speaking on the telephone, making political speeches,
writing class essays, reporting the latest news for a newspaper, or
crafting the next novel destined to be a New York Times best-seller.
What is more, data collected from the `wild' varies more in structure
relative to data collected in experimental approaches and requires a
number of steps to prepare the data to sync up with the data analysis
toolbelt.

I liken this distinction between experimental and observational data
collection to the difference between farming and foraging. Experimental
approaches are like farming; the groundwork for a research plan is
designed, much as a field is prepared for seeding, then the researcher
performs as series of tasks to produce data, just as a farmer waters and
cares for the crops, the results of the process bear fruit, data in our
case, and this data is harvested. Observational approaches are like
foraging; the researcher scans the available environmental landscape for
viable sources of data from all the naturally existing sources, these
sources are assessed as to their usefulness and value to address the
research question, the most viable is selected, and then the data is
collected.

The data acquired from both of these approaches have their trade-offs,
just as farming and foraging. Experimental approaches directly elicit
language behavior in highly controlled conditions. This directness and
level of control has the benefit of allowing researchers to precisely
track how particular experimental conditions effect language behavior.
As these conditions are an explicit part of the design and therefore the
resulting language behavior can be more precisely attributed to the
experimental manipulation. The primary shortcoming of experimental
approaches is that there is a level of artificialness to this directness
and control. Whether it is the language materials used in the task, the
task itself, or the fact that the procedure takes place under
supervision the language behavior elicited can diverge quite
significantly from language behavior performed in natural communicative
settings. Observational approaches show complementary strengths and
shortcomings.

Whereas experimental approaches may diverge from natural language use,
observational approaches strive to identify and collected language
behavior data in natural, uncontrolled, and unmonitored contexts, as
seen in Figure~\ref{fig-data-collection-methods}. In this way
observational approaches do not have to question to what extent the
language behavior data is or is not performed as a natural communicative
act. On the flipside, the contexts in which natural language
communication take place are complex relative to experimental contexts.
Language collected from natural contexts are nested within the complex
workings of a complex world and as such inevitably include a host of
factors and conditions which can prove challenging to disentangle from
the language phenomenon of interest but must be addressed in order to
draw reliable associations and conclusions.

\begin{figure}[H]

{\centering \includegraphics[width=4.04in,height=\textheight]{figures/ta-data-collection-methods.drawio.png}

}

\caption{\label{fig-data-collection-methods}Trade-offs between
experimental and observational data collection methods.}

\end{figure}

The upshot, then, is twofold: (1) data collection methods matter for
research design and interpretation and (2) there is no single best
approach to data collection, each have their strengths and shortcomings.
In the ideal, a robust science of language will include insight from
both experimental and observational approaches
(\protect\hyperlink{ref-Gilquin2009}{Gilquin and Gries 2009}). And
evermore there is greater appreciation for the complementary nature of
experimental and observational approaches and a growing body of research
which highlights this recognition.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-alt} Case study}

Manning (\protect\hyperlink{ref-Manning2003}{2003}) discusses the use of
probabilistic models in syntax to account for the variability in
language usage and the presence of both hard and soft constraints in
grammar. The paper touches on the statistical methods in text analysis,
the importance of distinguishing between external and internal language,
and the limitations of Generative Grammar. Overall, the paper suggests
that usage-based and formal syntax can learn from each other to better
understand language variation and change.

\end{tcolorbox}

Given their particular trade-offs observational data is often used as an
exploratory starting point to help build insight and form predictions
that can then be submitted to experimental conditions. In this way,
studies based on observational data serve as an exploratory tool to
gather a better and more externally valid view of language use which can
then serve to make prediction that can be explored with more precision
in an experimental paradigm. However, this is not always the case;
observational data is also often used in hypothesis-testing contexts as
well. And furthermore, some in some language-related fields, a
hypothesis-testing is not the approach for deriving knowledge and
insight.

\hypertarget{text-analysis}{%
\section{Text analysis}\label{text-analysis}}

In a nutshell, \textbf{text analysis}\index{Text analysis} is the
process of leveraging the data science toolbelt to derive insight from
textual data collected through observational methods. In the next
subsections, I will unpack this definition and discuss the primary
components that make up text analysis including research appoaches and
technical implementation, as well as practical applications.

\hypertarget{approaches}{%
\subsection{Approaches}\label{approaches}}

Text analysis is a multifacited research methodology. It can be used use
facilitate the qualitative exploration of smaller, human-digestable
textual information, but is more often employed quantitatively to bring
to the surface patterns and relationships in large samples of textual
data that would be otherwise difficult, if not impossible, to identify
manually.

Text being text, there are a series of \textbf{data prepration} steps
that must be taken to ready the data for analysis. In addition to
collecting the data, the data must be organized, cleaned, and
transformed into a format that is amenable to statistical analysis.

The statistical and evaluative approach employed in the analysis is
dependent on the aim of the research. For research aimed at exploring
and uncovering patterns and relationships in a data-driven manner,
\textbf{Exploratory Data Analysis} (EDA) is employed. EDA combines
descriptive statistics, visualizations, and statistical learning methods
in an iterative and interactive way to provide the researcher the
ability to identify patterns and relationships and to evaluate whether
and why they are meaningful.

\textbf{Predictive Data Analysis} (PDA), applied in research for outcome
prediction, is a supervised machine learning task. It uses feature sets
to predict an outcome variable. Its primary evaluation metric is the
prediction accuracy on new data. However, for many text analysis tasks,
human interpretation is crucial to provide context and assess the
significance of the results.

Research aimed at explaining relationships between variables and the
population from which the sample was drawn will adopt an
\textbf{Inferential Data Analysis} (IDA) approach. IDA is a
theory-driven process that employs statistical models for hypothesis
testing. The extent to which the results can be confidently generalized
to the population is the primary evaluation metric.

As we see, text analysis can be used for a variety of purposes; from
data-driven exploration and discovery to hypothesis testing and
generalization.

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

To ensure that the results of text analysis projects are replicable and
transparent, programming strategies play an integral role at each stage
of the implementation of a research project. While there are a number of
programming languages that can be used for text analysis, R widely
adopted in linguistics research. R is a free and open-source programming
language that is specifically designed for statistical computing and
graphics. It has a large and active community of users and developers,
and a robust ecosystem of packages which make it a powerful and flexible
language that is well-suited for core text analysis tasks: data
collection, organization, transformation, analysis, and visualization.
When combined with Quarto for literate programming and GitHub for
version control and collaboration, R provides a robust and reproducible
workflow for text analysis.

\hypertarget{applications}{%
\subsection{Applications}\label{applications}}

So what are some applications of text analysis? Most public facing
applications stem from Computational Linguistic research, often known as
\textbf{Natural Language Processing} (NLP) by practitioners. Whether it
be using search engines, online translators, submitting your paper to
plagiarism detection software, \emph{etc.} many of the text analysis
methods we will cover are at play.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

What are some other public facing applications of text analysis that you
are aware of?

\end{tcolorbox}

In academia the use of quantitative text analysis is even more
widespread, despite the lack of public fanfare. In linguistics, text
analysis is applied to a wide range of topics and research questions in
both theoretical and applied subfields, as seen in
Example~\ref{exm-linguistic-theory} and
Example~\ref{exm-linguistic-applied}.

\begin{example}[]\protect\hypertarget{exm-linguistic-theory}{}\label{exm-linguistic-theory}

Theoretical linguistics

\begin{itemize}
\tightlist
\item
  Hay (\protect\hyperlink{ref-Hay2002}{2002}) use a corpus study to
  investigate the role of frequency and phonotatics in affix ordering in
  English.
\item
  Riehemann (\protect\hyperlink{ref-Riehemann2001}{2001}) explores the
  extent to which idiomatic expressions (\emph{e.g.} `raise hell') are
  lexical or syntactic units.
\item
  Bresnan (\protect\hyperlink{ref-Bresnan2007a}{2007}) investigate the
  claim that possessed deverbal nouns in English (\emph{e.g.} `the
  city's destruction') are subject to a syntactic constraint that
  requires the possessor to be affected by the action denoted by the
  deverbal noun.
\end{itemize}

\end{example}

\begin{example}[]\protect\hypertarget{exm-linguistic-applied}{}\label{exm-linguistic-applied}

Applied linguistics

\begin{itemize}
\tightlist
\item
  Wulff, Stefanowitsch, and Gries
  (\protect\hyperlink{ref-Wulff2007}{2007}) explore differences between
  British and American English at the lexico-syntactic level in the
  \emph{into}-causative construction (\emph{e.g.} `He tricked me into
  employing him.').
\item
  Eisenstein et al. (\protect\hyperlink{ref-Eisenstein2012}{2012}) track
  the geographic spread of neologisms (\emph{e.g.} `bruh', `af',
  '-\_\_-') from city to city in the United States using Twitter data
  collected between 6/2009 and 5/2011.
\item
  Bychkovska and Lee (\protect\hyperlink{ref-Bychkovska2017}{2017})
  investigates possible differences between L1-English and L1-Chinese
  undergraduate students' use of lexical bundles, multiword sequences
  which are extended collocations (\emph{e.g.} `as the result of'), in
  argumentative essays.
\item
  Jaeger and Snider (\protect\hyperlink{ref-Jaeger2007}{2007}) use a
  corpus study to investigate the phenomenon of syntactic persistence,
  the increased tendency for speakers to use a particular syntactic form
  over an alternate when the syntactic form has been recently processed.
\item
  Voigt et al. (\protect\hyperlink{ref-Voigt2017}{2017}) explore
  potential racial disparities in officer respect in police body camera
  footage.
\item
  Olohan (\protect\hyperlink{ref-Olohan2008}{2008}) investigate the
  extent to which translated texts differ from native texts do to
  `explicitation'.
\end{itemize}

\end{example}

So too, text analysis is used in a variety of other fields where insight
from language is sought, as seen in Example~\ref{exm-other-fields}.

\begin{example}[]\protect\hypertarget{exm-other-fields}{}\label{exm-other-fields}

Language-related fields

\begin{itemize}
\tightlist
\item
  Kloumann et al. (\protect\hyperlink{ref-Kloumann2012}{2012}) explore
  the extent to which languages are positively, neutrally, or negatively
  biased.
\item
  Mosteller and Wallace (\protect\hyperlink{ref-Mosteller1963}{1963})
  provide a method for solving the authorship debate surrounding The
  Federalist papers.
\item
  Conway et al. (\protect\hyperlink{ref-Conway2012}{2012}) investigate
  whether the established drop in language complexity of rhetoric in
  election seasons is associated with election outcomes.
\end{itemize}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Language is a key component of human communication and interaction. What
are some other areas of research in and outside linguistics that you
think could be explored using text analysis methods?

\end{tcolorbox}

These studies in Examples \ref{exm-linguistic-theory},
\ref{exm-linguistic-applied}, and \ref{exm-other-fields} are just a few
illustrations of the contributions of text analysis as the primary
method to gain a deeper understanding of language structure, function,
variation, and acquisition. As a method, however, text analysis can also
be used to support other research methods. For example, text analysis
can be used collect data, generate authentic materials, provide
linguistic annotation, to generate hypotheses, for either qualitative
and/ or quantitative approaches. Together these efforts contribute to a
more robust language science by incorporating externally valid data and
providing methodological triangulation
(\protect\hyperlink{ref-Francom2022}{Francom 2022}).

In sum, the applications highlighted in this section underscore the
versatility of text analysis as a research method. Whether it be in the
public sphere or in academia, text analysis methods furnish a set of
powerful tools for gaining insight from language data.

\hypertarget{summary}{%
\section*{Summary}\label{summary}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter I started with some general observations about the
difficulty of making sense of a complex world. The standard approach to
overcoming inherent human limitations in sense making is science. In the
21st century the toolbelt for doing scientific research and exploration
has grown in terms of the amount of data available, the statistical
methods for analyzing the data, and the computational power to manage,
store, and share the data, methods, and results from quantitative
research. The methods and tools for deriving insight from data have made
significant inroads in and outside academia, and increasingly figure in
the quantitative investigation of language. Text analysis is a
particular branch of this enterprise based on observational data from
real-world language and is used in a wide variety of fields.

In the end I hope that you enjoy this exploration into text analysis.
Although the learning curve at times may seem steep --the experience you
will gain will not only improve your data literacy, research skills, and
programmings skills but also enhance your appreciation for the richness
of human language and its important role in our everyday lives.

\hypertarget{actitivies}{%
\section*{Actitivies}\label{actitivies}}
\addcontentsline{toc}{section}{Actitivies}

\markright{Actitivies}

The following activities build on your introduction to R and Quarto in
the previous chapter. In these activities you will uncover more features
offered by Quarto which will enhance your ability to produce
comprehensive reproducible research documents. You will apply the
capabilities of Quarto in a practical context conveying the objectives
and key discoveries from a primary research article.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-.html}{Academic
writing with Quarto}\\
\textbf{How}: Read Recipe 1 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore additional functionality in Quarto: numbered
sections, table of contents, in-line citations and a document-final
references list, and cross-referenced tables and figures.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-1}{Literate
programming II}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 1.\\
\textbf{Why}: To put into practice Quarto functionality to communicate
the aim(s) and main finding(s) from a primary research article and to
interpret a related plot.

\end{tcolorbox}

\hypertarget{questions}{%
\section*{Questions}\label{questions}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  How has scientific research and exploration changed in the 21st
  century?
\item
  What are the three basic skill sets that make up the data science
  toolbelt?
\item
  What are the benefits of reproducible research in data science?
\item
  Explain the trade-offs between experimental and observational data
  collection methods.
\item
  What is text analysis and how is it used in various fields?
\item
  Identify research in an area of interest in linguistics that has taken
  a quantitative approach to text analysis.
\item
  In your own words, define literate programming?
\item
  What are the benefits of literate programming?
\item
  What are the benefits of using R and Quarto for literate programming?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Technical exercises}

\begin{itemize}
\tightlist
\item
  Create a literate programming document in Quarto. Edit the yaml header
  to reflect details of the work and add your work with the data types
  in R to code chunks. Add, commit, and push the project to GitHub.
\item
  In the Quarto document, explore using R to create vectors and explore
  their properties.
\item
  Explore the following resources and with the goal to identify a
  quantitative text analysis project. \href{https://rpubs.com/}{Rpubs},
  \href{https://github.com}{GitHub},
  \href{https://datacamp.com}{DataCamp},
  \href{https://kaggle.com}{Kaggle},
  \href{https://r-bloggers.com}{R-bloggers}.
\item
  \faIcon{wrench} \ldots{} more to come \ldots{}
\end{itemize}

\end{tcolorbox}

\part{Foundations}

Before working on the specifics of a data project, it is important to
establish a fundamental understanding of the characteristics of each of
the levels in the ``Data, Information, Knowledge, and Insight Hierarchy
(DIKI)'' (see Figure~\ref{fig-diki-hierarchy}) and the roles each of
these levels have in deriving insight from data. In
Chapter~\ref{sec-understanding-data} we will explore the Data and
Information levels drawing a distinction between two main types of data
(populations and samples) and then cover how data is structured and
transformed to generate information (datasets) that is fit for
statistical analysis. In Chapter~\ref{sec-approaching-analysis} I will
outline the importance and distinct types of statistical procedures
(descriptive and analytic) that are commonly used in text analysis.
Chapter~\ref{sec-framing-research} aims to tie these concepts together
and cover the required steps for preparing a research blueprint to
conduct an original text analysis project.

\hypertarget{sec-understanding-data}{%
\chapter{Understanding data}\label{sec-understanding-data}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
The goal is to turn data into information, and information into insight.

--- Carly Fiorina
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Describe the difference between data and information.
\item
  Understand how the tidy approach to data organization can enhance the
  quality and usability of data.
\item
  Articulate the importance of documentation in promoting reproducible
  research.
\end{itemize}

\end{tcolorbox}

In this chapter, the groundwork is laid for deriving insights from text
analysis by focusing on content and structure of data and information.
The concepts of populations and samples are introduced, highlighting
their similarities and key differences. Connecting these topics to text
analysis, language samples, or corpora, are explored, discussing their
types, sources, formats, and ethical considerations. Subsequently, key
concepts in creating information from data, such as organization and
transformation, are examined. Finally, the importance of documentation
in quantitative research is emphasized through addressing data origin
and data dictionaries.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Objects, Packages
and functions}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To introduce you to the main types of objects in R and to
understand the role and use of functions and packages in R programming.

\end{tcolorbox}

\hypertarget{sec-ud-data}{%
\section{Data}\label{sec-ud-data}}

Data is data, right? The term `data' is so common in popular vernacular
it is easy to assume we know what we mean when we say `data'. But as in
most things, where there are common assumptions there are important
details that require more careful consideration. Let's turn to the first
key distinction that we need to make to start to break down the term
`data': the difference between populations and samples.

\hypertarget{sec-ud-populations}{%
\subsection{Populations}\label{sec-ud-populations}}

The first thing that comes to many people's mind when the term
population is used is human populations (derived from Latin `populus').
Say for example we pose the question --What's the population of
Milwuakee? When we speak of a population in these terms we are talking
about the total sum of individuals living within the geographical
boundaries of Milwaukee. In concrete terms, a
\index{population}\textbf{population} an idealized set of objects or
events in reality which share a common characteristic or belong to a
specific category. The term to highlight here is idealized. Although we
can look up the US Census report for Milwaukee and retrieve a figure for
the population, this cannot truly be the population. Why is that? Well,
whatever method that was used to derive this numerical figure was surely
incomplete. If not incomplete, by the time someone recorded the figure
some number of residents of Milwaukee moved out, moved in, were born, or
passed away. In either case, this example serves to point out that
populations are not fixed and are subject to change over time.

Likewise when we talk about populations in terms of language we dealing
with an idealized aspect of linguistic reality. Let's take the words of
the English language as an analog to our previous example population. In
this case the words are the people and English is the grouping
characteristic. Just as people, words move out, move in, are born, and
pass away. Any compendium of the words of English at any moment is
almost instananeously incomplete. This is true for all populations, save
those relatively rare cases in which the grouping characteristics select
a narrow slice of reality which is objectively measurable and whose
membership is fixed (the complete works of Shakespeare, for example).

In sum, (most) populations are amorphous moving targets. We subjectively
hold them to exist, but in practical terms we often cannot nail down the
specifics of populations. So how do researchers go about studying
populations if they are theoretically impossible to access directly? The
strategy employed is called sampling.

\hypertarget{sec-ud-samples}{%
\subsection{Samples}\label{sec-ud-samples}}

A \index{sample}\textbf{sample} is the product of a subjective process
of selecting a finite set of observations from an idealized population
with the goal of capturing the relevant characteristics of the target
population. The \textbf{degree of representativeness}
\index{sample!representativeness} of a sample is the extent to which the
sample reflects the characteristics of the population. The degree of
representativeness is crucial for research as it directly impacts of any
findings based on the sample.

To maximize the representativeness of a sample, researchers employ a
variety of strategies. One of the first and sometimes the easiest
strategy is to increase the \index{sample size}\textbf{sample size}. A
larger sample will always be more representative than a smaller sample.
Sample size, however, is often not enough. It is not hard to imagine a
large sample which by chance captures only a subset of the features of
the population. Another step to enhance sample representativeness is to
apply \textbf{random sampling}. Together a large random sample has an
even better chance of reflecting the main characteristics of the
population better than a large or random sample. But, random as random
is, we still run the risk of acquiring a skewed sample (\emph{i.e.} a
sample with low representativeness).

To help mitigate these issues, there are two more strategies that can be
applied to improve sample representativeness. Note, however, that while
size and random samples can be applied to any sample with few
assumptions about internal characteristics of the population, these next
two strategies require decisions depend on the presumed internal
characteristics of the population.

The first of these more informed sampling strategies is called
\textbf{stratified sampling}. Stratified samples make (educated)
assumptions about sub-components within the population of interest. With
these sub-populations in mind, large random samples are acquired for
each sub-population, or strata. At a minimum, stratified samples can be
no less representative than random sampling alone, but the chances that
the sample is better increases. Can there be problems in the approach?
Yes, and on two fronts. First knowledge of the internal components of a
population are often based on a limited or incomplete knowledge of the
population (remember populations are idealized). In other words, strata
are selected subjectively by researchers using various heuristics some
of which are based on some sense of `common knowledge'.

The second front on which stratified sampling can err concerns the
relative sizes of the sub-components relative to the whole population,
which is known as \textbf{balance}. Even if the relevant sub-components
are identified, their relative size adds another challenge which
researchers must address in order to maximize the representativeness of
a sample.

Together, large randomly selected and balanced stratified samples set
the benchmark for sampling. However, hitting this ideal is not always
feasible. There are situations where sizeable samples are not
accessible. Alternatively, there may be instances where the population
or its strata are not well understood. In such scenarios, researchers
have to work with the most suitable sample they can obtain given the
limitations of their research project.

\hypertarget{corpora}{%
\subsection{Corpora}\label{corpora}}

A key feature of a sample is that it is purposely selected to model a
target population. In text analysis, a purposely sampled collection of
texts, of the type defined here, is known as a \textbf{corpus}
(\emph{pl.} corpora). A set of texts or documents which have not been
selected purposely lack a \textbf{sampling frame}, and therefore is not
a corpus. The sampling frame, hence the populations modeled, in any
given corpus will vary. It is key to vet corpora to ensure that the
resource's sampling frame and the research project's target populations
align as closely as possible to safeguard the integrity of research
findings later in the research process.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

\begin{figure}[H]

\begin{minipage}[t]{0.48\linewidth}
The `Standard Sample of Present-Day American English' (known commonly as
the Brown Corpus) is widely recognized as one of the first large,
machine-readable corpora. Compiled by Kucera and Francis
(\protect\hyperlink{ref-Kucera1967}{1967}), the corpus is comprised of
1,014,312 words from edited English prose published in the United States
in 1961.Given the sampling frame for this corpus visualized in
Figure~\ref{fig-ud-brown-distribution}, can you determine what language
population this corpus aims to represent? What types of research might
this corpus support or not support?\end{minipage}%
%
\begin{minipage}[t]{0.03\linewidth}
~\end{minipage}%
%
\begin{minipage}[t]{0.48\linewidth}

\begin{figure}[H]

{\centering \includegraphics{understanding-data_files/figure-pdf/fig-ud-brown-distribution-1.pdf}

}

\caption{\label{fig-ud-brown-distribution}Overview of the sampling frame
of the Brown Corpus.}

\end{figure}

\end{minipage}%

\end{figure}

\end{tcolorbox}

\hypertarget{types}{%
\subsubsection{Types}\label{types}}

Let's take a look at some key characteristics, attributes, and features
that distinguish corpora.

\hypertarget{reference}{%
\paragraph{Reference}\label{reference}}

The least common and most ambitious corpus resources are those which aim
to model the characteristics of a language population. These are known
as \textbf{reference corpora}. These are projects designed with wide
sampling frames, and require significant investments of time in corpus
design and implementation (and continued development) that are usually
undertaken by research teams (\protect\hyperlink{ref-Adel2020}{del
2020}).

The \href{https://www.anc.org/}{American National Corpus (ANC)} or the
\href{http://www.natcorp.ox.ac.uk/}{British National Corpus (BNC)} are
corpora which aim to model the general characteristics of a variety of
the English language, the former of American English and the later
British English. Reference corpora exist for other languages as well:
Spanish \href{http://corpus.rae.es/creanet.html}{Reference Corpus of
Present-Day Spanish (CREA)}, German
\href{https://www.ids-mannheim.de/digspra/kl/projekte/korpora/}{The
German Reference Corpus (DeReKo)}, Turkish
\href{https://www.tnc.org.tr/}{Turkish National Corpus (TNC)}, and many
others.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Of note is the fact that, at present, most of the world's languages lack
reference corpus resources, or any corpus resources whatsoever.
``Low-resourced'' languages are often less studied, resource scarce,
less available in born-digital formats, etc.
(\protect\hyperlink{ref-Magueresse2020}{Magueresse, Carles, and
Heetderks 2020}).

Visit the
\href{https://www.clarin.eu/resource-families/reference-corpora}{Clarin
overview} on reference corpora and then visit
\href{https://lremap.elra.info/}{LRE Map}. Can you find a reference
corpus for a language you speak or are interested in studying? If not,
consider what can be done to address this gap in the research community.

\end{tcolorbox}

\hypertarget{specialized}{%
\paragraph{Specialized}\label{specialized}}

\textbf{Specialized corpora} aim to represent more specific populations.
The population may be defined either by modality, genre, time, location,
or speaker-oriented characteristics, or some combination thereof. What
specialized corpora lack in breadth of coverage, they make up for in
depth of coverage by providing a more targeted representation of
specific language populations.

The
\href{https://www.linguistics.ucsb.edu/research/santa-barbara-corpus}{Santa
Barbara Corpus of Spoken American English (SBCSAE)}, as you can imagine
from the name of the resource, aims to model spoken American English. No
claim to written English is included. There are even more specific types
of corpora which attempt to model other types of sub-populations such as
\href{https://www.coventry.ac.uk/research/research-directories/current-projects/2015/british-academic-written-english-corpus-bawe/}{academic
writing},
\href{https://www.clarin.eu/resource-families/cmc-corpora}{computer-mediated
communication (CMC)}, language use in specific
\href{http://ice-corpora.net/ice/index.html}{regions of the world}, a
\href{https://www.wgtn.ac.nz/lals/resources/corpora-default/corpora-wsc}{country},
a \href{https://cesa.arizona.edu}{region of a country}, \emph{etc}.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

\begin{figure}[H]

\begin{minipage}[t]{0.58\linewidth}
Grieve, Nini, and Guo (\protect\hyperlink{ref-Grieve2018}{2018})
compiled a 8.9 billion-word corpus of geotagged posts from Twitter
between 2013-2014 in the United States. The authors provide a
\href{https://isogloss.shinyapps.io/isogloss/}{search interface} to
explore relationship between lexical usage and geographic location.
Explore this corpus searching for terms related to slang (``hella'',
``wicked''), geographical (``mountain'', ``river''), meteorological
(``snow'', ``rain''), and/ or any other term types. What types of
patterns do you find? What are the benefits and/ or limitations of this
type of data and/ or interface?\end{minipage}%
%
\begin{minipage}[t]{0.03\linewidth}
~\end{minipage}%
%
\begin{minipage}[t]{0.39\linewidth}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{figures/ud-word-mapper.png}

}

\caption{\label{fig-ud-word-mapper}Example distribution of the term
`Ya'll' the Word Mapper project.}

\end{figure}

\end{minipage}%

\end{figure}

\end{tcolorbox}

Another set of specialized corpora are resources which aim to compile
texts from different languages or different language varieties for
direct or indirect comparison. Corpora that are directly comparable,
that is they include source and translated texts, are called
\textbf{parallel corpora}. Parallel corpora include different languages
or language varieties that are indexed and aligned at some linguistic
level (\emph{i.e.} word, phrase, sentence, paragraph, or document), see
\href{https://opus.nlpl.eu/}{OPUS}. Corpora that are compiled with
different languages or language varieties but are not directly aligned
are called \textbf{comparable corpora}. The comparable language or
language varieties are sampled with the same or similar sampling frame,
for example
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0402}{Brown}
and
\href{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/0167}{LOB}
corpora.

The aim of the quantitative text researcher is to select the corpus, or
corpora, which best align with the purpose of the research. For example,
a general corpus such as the American National Corpus may be better
suited to address a question dealing with the way American English
works, but this general resource may lack detail in certain areas, such
as \href{https://mtsamples.com/index.asp}{medical language}, that may be
vital for a research project aimed at understanding changes in medical
terminology. Furthermore, a researcher studying spoken language might
collect a corpus of transcribed conversations from a particular
community or region, such as the SBCSAE. While this would not include
every possible spoken utterance produced by members of that group, it
could be considered a representative sample of the population of speech
in that context.

\hypertarget{sources}{%
\subsubsection{Sources}\label{sources}}

\hypertarget{published}{%
\paragraph{Published}\label{published}}

The most common source of data used in contemporary quantitative
research is the internet. On the web an investigator can access corpora
published for research purposes. Many organizations exist around the
globe that provide access to published corpora in browsable catalogs, or
\textbf{repositories}. There are repositories dedicated to language
research, in general, such as the
\href{https://www.ldc.upenn.edu/}{Language Data Consortium} or that
specialize in specific domains, such as the spoken language repository
\href{http://talkbank.org/}{TalkBank}. It is always advisable to start
looking for the available language data in a repository. The advantage
of beginning your data search in repositories is that a repository,
especially those geared towards the linguistic community, will make
identifying language corpora faster than through a general web search.
Furthermore, repositories often require certain standards for corpus
format and documentation for publication.

Repositories are by no means the only source of published corpora on the
web. Researchers from around the world provide access to corpora and
datasets on their own sites or through data sharing platforms. Corpora
of various sizes and scopes will often be accessible on a dedicated
homepage or appear on the homepage of a sponsoring institution. These
resources may be available for download or via search inferaces. Finding
these resources is often a matter of doing a web search with the word
`corpus' and a list of desired attributes, including language, modality,
register, \emph{etc}.

As part of a general movement towards reproducibility, more corpora are
available on \textbf{data sharing platforms} such as
\href{https://github.com/}{GitHub}, \href{https://zenodo.org/}{Zenodo},
\href{http://www.re3data.org/}{Re3data}, \href{https://osf.io/}{OSF},
\emph{etc}. These platforms enable researchers to securely store,
manage, and share data with others. Support is provided for various
types of data, including documents and code, and as such they are a good
place to look as they often include reproducible research projects as
well.

\hypertarget{develop}{%
\paragraph{Develop}\label{develop}}

Language corpora prepared by researchers and research groups listed on
repositories or hosted by the researchers themselves is often the first
place to look for data. The web, however, contains a wealth of language
and language-related data that can be accessed by researcher to compile
their own corpus. There are two primary ways to attain language data
from the web. The first is through an \textbf{Application Programming
Interface} (API). APIs are, as the title suggests, programming
interfaces which allow access, under certain conditions, to information
that a website or database accessible via the web contains.

The second, more involved, way to acquire data from the web is is
through the process of web scraping. \textbf{Web scraping} is the
process of harvesting data from the public-facing web. Language texts
may be found on sites as uploaded files, such as pdf or doc (Word)
documents, or found displayed as the primary text of a site. Given the
wide variety of documents uploaded and language behavior recorded daily
on news sites, blogs and the like, compiling a corpus has never been
easier. Having said that, how the data is structured and how much data
needs to be retrieved can pose practical obstacles to collecting data
from the web, particularly if the approach is to acquire the data by
manually instead of automating the task.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

The process of corpus development is a topic in and of itself. For a
more in-depth discussion of the process, see del
(\protect\hyperlink{ref-Adel2020}{2020}).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Explore some of the resources listed on the
\href{https://qtalr.github.io/qtalrkit/articles/guide-5.html}{qtalrkit
compansion site} and consider their sampling frames. Can you think of a
research question or questions that this resource may be well-suited to
support research into? What types of questions would be
less-than-adequate for a given resource?

\end{tcolorbox}

\hypertarget{ethical-considerations}{%
\paragraph{Ethical considerations}\label{ethical-considerations}}

Just because data is available on the web does not mean it is free to
use. Repositories, APIs, and individual data resources often have
licensing agreements and terms of use, ranging from public domain to
proprietary licenses. Public domain licenses, such as those found in
Project Gutenberg, allow anyone to use the data for any purpose.
\href{https://creativecommons.org/about/cclicenses/}{Creative Commons
licenses}, like those used by the American National Corpus, Wikipedia,
and TalkBank, span from public domain to more restrictive uses,
including requirements for attribution or prohibiting commercial use.
Even more restrictive licenses, such as those for the Corpus of
Contemporary American English and the British National Corpus, may
require a fee to access and use the data, even for research purposes.

Respecting intellectual property rights is crucial when working with
corpus data. Violating these rights can lead to legal and ethical
issues, including lawsuits, fines, and damage to one's professional
reputation. To avoid these problems, researchers must ensure they have
the necessary permissions to use copyrighted works in their corpora.
Obtaining permissions involves contacting the author or publisher and
requesting consent to use their work for research purposes. Documenting
all obtained permissions and providing attribution and/ or citation is
essential respecting the intellectual property rights of others.

\hypertarget{formats}{%
\subsubsection{Formats}\label{formats}}

Whether you are using a published corpus or developing your own, it is
important to understand how the data you want to work with is formatted.
When referring to the format of a corpus, this includes the folder and
file structure, the file types, the internal structure of the files
themselves, and how file content is encoded electronically.

\hypertarget{folder-and-file-structure}{%
\paragraph{Folder and file structure}\label{folder-and-file-structure}}

Some corpus resources are contained in a single file, such as a
spreadsheet or a text file, but more often than not a corpus will be
comprised of multiple files and folders. The folder and file structure
will reflect the organization of the corpus and may include sub-folders
for different types or groupings of data. In addition to the corpus data
itself, metadata and documentation will often be included in the corpus
folder structure. The corpus data may be grouped by language, modality,
register, or other attributes such as types of linguistic annotation.

To illustrate, in Example~\ref{exm-toy-corpus} we have the file and
folder structure of a toy corpus.

\begin{example}[]\protect\hypertarget{exm-toy-corpus}{}\label{exm-toy-corpus}

Toy corpus structure

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{corpus/}
\ExtensionTok{}\NormalTok{ documentation/}
\ExtensionTok{}\NormalTok{    README.md}
\ExtensionTok{}\NormalTok{    LICENSE}
\ExtensionTok{}\NormalTok{ metadata/}
\ExtensionTok{}\NormalTok{    speakers.csv}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    spoken/}
\ExtensionTok{}\NormalTok{       inter{-}09{-}a.xml}
\ExtensionTok{}\NormalTok{       inter{-}09{-}b.xml}
\ExtensionTok{}\NormalTok{       convo{-}09{-}a.xml}
\ExtensionTok{}\NormalTok{       ...}
\ExtensionTok{}\NormalTok{    written/}
\ExtensionTok{}\NormalTok{       essay{-}09{-}a.xml}
\ExtensionTok{}\NormalTok{       essay{-}09{-}b.xml}
\ExtensionTok{}\NormalTok{       respo{-}09{-}a.xml}
\ExtensionTok{}\NormalTok{       ...}
\end{Highlighting}
\end{Shaded}

\end{example}

In this example, we have a corpus folder with three sub-folders:
\emph{documentation/}, \emph{metadata/}, and \emph{data/}. The
\emph{data/} folder contains two sub-folders: \emph{spoken/} and
\emph{written/}. Each folder contains the relevant data files.

Where a single file is easy to download from the web, a corpus with a
more complex folder structure can be more difficult to access. For that
reason, many corpus resources are packaged into and made into a single
compressed file. \textbf{File compression} has two benefits: it
preserves the folder structure in a format which is contained in a
single file and it also reduces the overall storage size. Common file
compression formats are \emph{.zip} and \emph{.tar.gz}. So a compressed
corpus file for the example above may be named something like
\emph{corpus.zip} or \emph{corpus.tar.gz}. To access the original data
within a compressed file, one must use a decompression tool or software
to extract the contents after downloading it.

\hypertarget{file-types}{%
\paragraph{File types}\label{file-types}}

In our toy corpus example, you may have noticed that each of the
filenames appear with either \emph{.md}, \emph{.csv}, \emph{.xml}, or
nothing appended. These are examples of \textbf{file extensions}. File
extensions a short sequence of characters, usually preceded by a period
(\emph{.}) which are used to indicate the type or format of file. File
extensions help both users and software programs to identify the content
and purpose of a file.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

If you are working on your own desktop computer, you may not see the
file extensions. This is because the file explorer is configured to hide
them by default. To see the file extensions, you will need to change the
settings in your file explorer. Use a search engine to find instructions
for your operating system.

\end{tcolorbox}

In addition to those listed above, other file extensions often
encountered when working with data for text analysis include
\emph{.txt}, \emph{.pdf}, \emph{.docx}, \emph{.xlsx}, \emph{.json}, and
\emph{.html}. Common file extensions will often be associated with
specific software programs on your computer, especially those which are
directly associated with proprietary software such as \emph{.docx} for
Microsoft Word or \emph{.xlsx} for Microsoft Excel. However, many file
extensions are not directly associated with any specific software
program and can be opened and edited with any text editor.

It is important to note that file extensions are helpful conventions,
but they are not a guarantee of the file type or structure of the file
content. Furthermore, corpus developers may create their own file
extensions to signal the unique structure of their data. For example,
the \emph{.utt} file extension used in the Switchboard Dialogue Act
Corpus (SWDA) or the \emph{.cha} extension used for TalkBank resource
transcripts signal project-specific structuring. In either case, it is
recommended to open the file in a text editor to inspect the structure
of the file content to confirm the file structure before processing the
data contained therein.

\hypertarget{file-content}{%
\paragraph{File content}\label{file-content}}

The internal structure of the content of corpus data files is an
important aspect of any corpus both in terms of what data is included
and how to approach accessing and processing the data. A corpus may
include various types of linguistic (\emph{e.g.} part of speech,
syntactic structure, named entities, \emph{etc.}) or non-linguistic
(\emph{e.g.} source, dates, speaker information, \emph{etc.})
attributes. These attributes are known as \textbf{metadata}, or data
about data. As a general rule, files which include more metadata tend to
be more internally structured. Internal file structure refers to the
degree to which the content is easy to query and analyze by a computer.
Let's review characteristics of the three main types of file structure
types and associate common file extensions that files in each have.

\textbf{Unstructured data} is data which does not have a
machine-readable internal structure. This is the case for plain text
files (\emph{.txt}), which are simply a sequence of characters. For
example, in Example~\ref{exm-masc-text} we see a snippet of a plain text
file from the the Manually Annotated Sub-Corpus of American English
(MASC) (\protect\hyperlink{ref-Ide2008}{Ide et al. 2008}):

\begin{example}[]\protect\hypertarget{exm-masc-text}{}\label{exm-masc-text}

MASC plain text

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}Hotel California}

\NormalTok{Fact: Sound is a vibration. Sound travels as a mechanical wave through a medium, and in space, there is no}
\NormalTok{medium. So when my shuttle malfunctioned and the airlocks didn\textquotesingle{}t keep the air in, I heard nothing. After the}
\NormalTok{first whoosh of the air being sucked away, there was lightning, but no thunder. Eyes bulging in}
\NormalTok{panic, but no screams. Quiet and peaceful, right? Such a relief to never again hear my crewmate Jesse natter}
\NormalTok{about his girl back on Earth and that all{-}expenses{-}paid vacation{-}for{-}two she won last time he was on leave. I}
\NormalTok{swore, if I ever had to see a photo of him in a skimpy bathing suit again, giving the camera a cheesy thumbs{-}up}
\NormalTok{from a lounge chair on one of those white sandy beaches, I\textquotesingle{}d kiss a monkey. Metaphorically, of course.}
\end{Highlighting}
\end{Shaded}

\end{example}

Other examples of files which often contain unstructured data include
\emph{.pdf} and \emph{.docx} files. While these file types may contain
data which appears structured to the human eye, the structure is not
designed to be machine-readable. As such the data would typically be
read into R as a vector of \textbf{character strings}. It is possible to
perform only the most rudimentary queries on this type of data, such as
string matches. For anything more informative, it is necessary to
further process this data.

On the other end of the spectrum, \textbf{structured data} is data which
conforms to a tabular format in which elements in tables and
relationships between tables are defined. This makes querying and
analyzing easy and efficient. Relational databases (\emph{e.g.} MySQL,
PostgreSQL, etc.) are designed to store and query structured data. The
data frame object in R is also a structured data format. In each case,
the data is stored in a tabular format in which each row represents a
single observation and each column represents a single attribute whose
values are of the same type.

In Example~\ref{exm-masc-df} we see an example of an R data frame object
which overlaps with the data in the plain text file above in
Example~\ref{exm-masc-text}:

\begin{example}[]\protect\hypertarget{exm-masc-df}{}\label{exm-masc-df}

MASC data frame

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   title             date modality domain          ref\_num word  lemma pos  }
\NormalTok{   \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}            \textless{}}\KeywordTok{dbl}\NormalTok{\textgreater{} \textless{}}\KeywordTok{fct}\NormalTok{\textgreater{}    \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}             \textless{}}\KeywordTok{dbl}\NormalTok{\textgreater{} \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{} \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{} \textless{}}\KeywordTok{chr}\NormalTok{\textgreater{}}
\NormalTok{ 1 Hotel California  2008 Writing  General Fiction       0 \textgreater{}     \textgreater{}     NN   }
\NormalTok{ 2 Hotel California  2008 Writing  General Fiction       1 Hotel hotel NNP  }
\NormalTok{ 3 Hotel California  2008 Writing  General Fiction       2 Cali cali NNP  }
\NormalTok{ 4 Hotel California  2008 Writing  General Fiction       3 Fact  fact  NNP  }
\NormalTok{ 5 Hotel California  2008 Writing  General Fiction       4 :     :     :    }
\NormalTok{ 6 Hotel California  2008 Writing  General Fiction       5 Sound sound NNP  }
\NormalTok{ 7 Hotel California  2008 Writing  General Fiction       6 is    be    VBZ  }
\NormalTok{ 8 Hotel California  2008 Writing  General Fiction       7 a     a     DT   }
\NormalTok{ 9 Hotel California  2008 Writing  General Fiction       8 vibr vibr NN   }
\NormalTok{10 Hotel California  2008 Writing  General Fiction       9 .     .     .    }
\NormalTok{11 Hotel California  2008 Writing  General Fiction      10 Sound sound NNP  }
\end{Highlighting}
\end{Shaded}

\end{example}

Here we see that the data is stored in a tabular format with each row
representing a single observation (\texttt{word}) and each column
representing a single attribute. Internally, R applies a schema to
ensure the values in each column are of the same type (\emph{e.g.}
\texttt{\textless{}chr\textgreater{}},
\texttt{\textless{}dbl\textgreater{}},
\texttt{\textless{}fct\textgreater{}}, \emph{etc.}). This structured
format is designed to be easy to query and analyze and as such is the
primary format for data analysis in R.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

It is conventional to work with column names for datasets in R using the
same conventions that are used for naming objects. It is a matter of
taste which convention is used, but I have adopted
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html\#snake_case}{snake
case} as my personal preference (\emph{e.g} \texttt{ref\_num}). There
are also
\href{https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/names.html}{alternatives}.
Regardless of the convention you choose, it is good practice to be
consistent.

It is also of note that the column names should be balanced for
meaningfulness and brevity. This brevity is of practical concern but can
be somewhat opaque. For questions into the meaning of the column and is
values consult the resource's dataset documentation, consult
Section~\ref{sec-ud-documentation}.

\end{tcolorbox}

\textbf{Semi-structured data} falls between unstructured and structured
data. This covers a wide range of file structuring approaches. For
example, a otherwise plain text file with part-of-speech tags appended
to each word is minimally structured (Example~\ref{exm-masc-pos}).

\begin{example}[]\protect\hypertarget{exm-masc-pos}{}\label{exm-masc-pos}

MASC plain text with part-of-speech tags

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textgreater{}/NN Hotel/NNP California/NNP Fact/NNP :/: Sound/NNP is/VBZ a/DT vibration/NN ./. Sound/NNP travels/VBZ as/IN a/DT mechanical/JJ wave/NN through/IN a/DT medium/NN ,/, and/CC in/IN space/NN ,/, there/EX is/VBZ no/DT medium/NN ./. So/RB when/WRB my/PRP$ shuttle/NN malfunctioned/JJ and/CC the/DT airlocks/NNS did/VBD n\textquotesingle{}t/RB keep/VB the/DT air/NN in/IN ,/, I/PRP heard/VBD nothing/NN ./. After/IN the/DT}
\end{Highlighting}
\end{Shaded}

\end{example}

Towards the more structured end of semi-structured data, many file
formats including \emph{.xml} and \emph{.json} contain highly
structured, hierarchical data. For example, in
Example~\ref{exm-masc-xml} shows a snippet from a \emph{.xml} file from
the MASC corpus.

\begin{example}[]\protect\hypertarget{exm-masc-xml}{}\label{exm-masc-xml}

MASC XML

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}}\KeywordTok{a}\OtherTok{ xml:id=}\StringTok{"penn{-}N65571"}\OtherTok{ label=}\StringTok{"tok"}\OtherTok{ ref=}\StringTok{"penn{-}n0"}\OtherTok{ as=}\StringTok{"anc"}\NormalTok{\textgreater{}                                                                                                                                                        }
\NormalTok{  \textless{}}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"base"}\OtherTok{ value=}\StringTok{"}\DecValTok{\&gt;}\StringTok{"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"msd"}\OtherTok{ value=}\StringTok{"NN"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"string"}\OtherTok{ value=}\StringTok{"}\DecValTok{\&gt;}\StringTok{"}\NormalTok{/\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{a}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{node}\OtherTok{ xml:id=}\StringTok{"penn{-}n1"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{link}\OtherTok{ targets=}\StringTok{"seg{-}r1"}\NormalTok{/\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{node}\NormalTok{\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{a}\OtherTok{ xml:id=}\StringTok{"penn{-}N65599"}\OtherTok{ label=}\StringTok{"tok"}\OtherTok{ ref=}\StringTok{"penn{-}n1"}\OtherTok{ as=}\StringTok{"anc"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"base"}\OtherTok{ value=}\StringTok{"hotel"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"msd"}\OtherTok{ value=}\StringTok{"NNP"}\NormalTok{/\textgreater{}}
\NormalTok{    \textless{}}\KeywordTok{f}\OtherTok{ name=}\StringTok{"string"}\OtherTok{ value=}\StringTok{"Hotel"}\NormalTok{/\textgreater{}}
\NormalTok{  \textless{}/}\KeywordTok{fs}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{a}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{example}

The format of semi-structured data is often influenced by
characteristics of the data or reflect an author's individual
preferences. It is sometimes the case that data will be semi-structured
in a less-standard format. For example, the SWDA corpus includes a
\emph{.utt} file extension for files which contain utterances annotated
with dialogue act tags.

\begin{example}[]\protect\hypertarget{exm-swda-utt}{}\label{exm-swda-utt}

SWDA \emph{.utt} file

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{o          A.1 utt1: Okay.  /}
\NormalTok{qw          A.1 utt2: \{D So, \}}

\NormalTok{qy\^{}d          B.2 utt1: [ [ I guess, +}

\NormalTok{+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /}

\NormalTok{+          B.4 utt1: I think, ] + \{F uh, \} I wonder ] if that worked. /}

\NormalTok{qy          A.5 utt1: Does it say something? /}
\end{Highlighting}
\end{Shaded}

\end{example}

Whether standard or not, semi-structured data is often designed to be
machine-readable. As with unstructured data, the ultimate goal is to
convert the data into a structured format and augment the data where
necessary to prepare it for a particular research analysis.

\hypertarget{file-encoding}{%
\paragraph{File encoding}\label{file-encoding}}

The last aspect to consider about corpus formats is \textbf{file
encoding}. For a computer to display and process text characters, it
must be encoded in a way that the computer can understand (\emph{i.e.}
1's and 0's). Historically, character encoding schemes were developed to
represent characters from specific character script sets (\emph{e.g.}
ASCII only includes characters from the English alphabet). However, as
the need for a consistent and more inclusive way to encode characters
from multiple languages and scripts became apparent, the Unicode
standard, Unicode Transformation Format (UTF), was developed in the
early 1990s. UTF encodings (UTF-8, UTF-16, and UTF-32) are now the most
common way to encode text data and modern computers typically use them
by default. Although other more script-specific encoding schemes can
still be found in older data (e.g.~ISO-8859, Windows-1252, Shift JIS).

When working with corpus data, it is important to know if the encoding
scheme used for the data is compatible with your computing environment's
default (most likely UTF). If it is not, you will need to convert the
data to a compatible encoding scheme. Rest assured, there is support in
R for converting between different encoding schemes if the need arises.

\hypertarget{information}{%
\section{Information}\label{information}}

Identifying an adequate corpus resource, in terms of content, licensing,
and formatting, for the target research question is the first step in
moving a quantitative text research project forward. The next step is to
select the components or characteristics of this resource that are
relevant for the research and then move to organize the attributes of
this data into a more informative format. This is the process of
converting corpus data into a \index{dataset}\textbf{dataset} --a
tabular representation of particular attributes of the data as the basis
for generating information. Once the data represented as dataset, it is
often manipulated and transformed adjusting and augmenting the data such
that it better aligns with the research question and the analytical
approach.

\hypertarget{sec-ud-organization}{%
\subsection{Organization}\label{sec-ud-organization}}

Data alone is not informative. Only through explicit organization of the
data in a way that makes relationships and meaning explicit does data
become information. In this form, our data is called a dataset. This is
a particularly salient hurdle in text analysis research. Many textual
sources are unstructured or semi-structured, that is relationships that
will be used in the analysis have yet to be purposefully drawn and
organized from the data.

\hypertarget{sec-ud-tidy-data}{%
\subsubsection{Tidy Data}\label{sec-ud-tidy-data}}

The selection of the attributes from a corpus and the juxtaposition of
these attributes in a relational format, or dataset, that converts data
into information is known as \textbf{data curation}. The process of data
curation minimally involves creating a base dataset, or \emph{curated
dataset}, which establishes the main informational associations
according to philosophical approach outlined by Wickham
(\protect\hyperlink{ref-Wickham2014a}{2014}).

In this work, a \textbf{tidy dataset} refers both to the structural
(physical) and informational (semantic) organization of the dataset.
Physically, a tidy dataset is a tabular data structure, illustrated in
Figure~\ref{fig-ud-tidy-format-image}, where each \emph{row} is an
observation and each \emph{column} is a variable that contains measures
of a feature or attribute of each observation. Each cell where a given
row-column intersect contains a \emph{value} which is a particular
attribute of a particular observation for the particular
observation-feature pair also known as a \emph{data point}.

\begin{figure}[H]

{\centering \includegraphics[width=2.76in,height=\textheight]{figures/ud-tidy.drawio.png}

}

\caption{\label{fig-ud-tidy-format-image}Visual summary of the tidy
format.}

\end{figure}

In terms of semantics, columns and rows both contribute to the
informational value of the dataset. Let's start with columns. In a tidy
dataset, each column is a variable, an attribute that can take on a
number of values. Although variables vary in terms of values, they do
not in type. A variable is of one and only one informational type.
Statistically speaking, informational types are defined as
\textbf{levels of measurement}, a classification system used to
semantically distiguish between types of variables. There are four
levels (or types) in this system: nominal, ordinal, interval, and ratio.

In practice, however, text analysis researchers often group these levels
into three main informational types: categorical, ordinal, and numeric
(\protect\hyperlink{ref-Gries2021a}{S. T. Gries 2021}). What do these
informational types represent? \textbf{Categorical data} is for labeled
data or classes that answer the question ``what?'' \textbf{Ordinal data}
is categorical data with rank order that answers the question ``what
order?'' \textbf{Numeric data} is ordinal data with equal intervals
between values that answers the question ``how much or how many?''

Let's look at an example of a tidy dataset. Using the criteria just
described, let's see if we can identify the informational values
(categorical, ordinal, or numeric) of the variables that appear in a
snippet from the MASC corpus in dataset form in
Table~\ref{tbl-ud-info-values-masc}.

\hypertarget{tbl-ud-info-values-masc}{}
\begin{table}
\caption{\label{tbl-ud-info-values-masc}MASC dataset variables. }\tabularnewline

\centering
\begin{tabular}{llrrllr}
\toprule
title & modality & date & ref\_num & word & pos & num\_letters\\
\midrule
Hotel California & Writing & 2008 & 0 & > & NN & 1\\
Hotel California & Writing & 2008 & 1 & Hotel & NNP & 5\\
Hotel California & Writing & 2008 & 2 & California & NNP & 10\\
Hotel California & Writing & 2008 & 3 & Fact & NNP & 4\\
Hotel California & Writing & 2008 & 4 & : & : & 1\\
\addlinespace
Hotel California & Writing & 2008 & 5 & Sound & NNP & 5\\
Hotel California & Writing & 2008 & 6 & is & VBZ & 2\\
Hotel California & Writing & 2008 & 7 & a & DT & 1\\
Hotel California & Writing & 2008 & 8 & vibration & NN & 9\\
Hotel California & Writing & 2008 & 9 & . & . & 1\\
\bottomrule
\end{tabular}
\end{table}

We have seven variables listed as headers for each of the columns. We
could go one-by-one left-to-right but let's take another tack. Instead,
let's identify all those variables that cannot be numeric --these are
all the non-numeral variables: \texttt{title}, \texttt{modality},
\texttt{word}, and \texttt{pos}. The question to ask of these variables
is whether they represent an order or rank. Since titles, modalities,
words, and parts-of-speech are not ordered values, they are all
categorical.

Now in relation to \texttt{date}, \texttt{ref\_num}, and
\texttt{num\_letters}. All three are numerals, so they could be numeric.
But they could also be numeral representations of ordinal data. Before
we can move forward, we need to make sure we understand what each
variable means and how it is measured, or \textbf{operationalized}. The
variable name and the values can be helpful in this respect.
\texttt{date} is what it sounds like, a date, and is operationalized as
a year in the Gregorian calendar. And \texttt{num\_letters} seems quite
descriptive as well, number of letters, appearing as a letter count. But
in some cases it may be opaque as to what is being measured by the
variable name alone, for example \texttt{ref\_num}, and one will have to
refer to the dataset documentation. In this case \texttt{ref\_num} is a
reference number operationalized as a unique identifier for each word
per document in the corpus.

With this in mind, let's return to the question of whether
\texttt{date}, \texttt{ref\_num}, and \texttt{num\_letters} are numeric
or ordinal. Starting with the trickiest one, \texttt{date}, we can ask
the question to identify numeric data: ``how much or how many?''. In the
case of \texttt{date}, the answer is neither. A date is a point in time,
not a quantity. So \texttt{date} is not numeric. But it does provide
information about order. Hence, \texttt{date} is ordinal.
\texttt{ref\_num} is also ordinal because the question ``what order?''
can be asked of it. Finally, \texttt{num\_letters} is numeric because it
answers the question ``how many?''.

Let's turn to the second semantic value of a tidy dataset. In a tidy
dataset, each row is an observation. But an observation of what? This
depends on what the unit of observation is. That sounds circular, but
its not. The \textbf{unit of observation} is simply the primary entity
that is being observed or measured
(\protect\hyperlink{ref-Sedgwick2015}{Sedgwick 2015}). Even without
context, it can often be identified in a dataset by looking at the level
of specificity of the variable values and asking what each variable
describes. When one variable appears to be the most individualized and
other variables appear to describe that variable, then the most
individualized variable is likely the unit of observation of the
dataset, \emph{i.e.} the meaning of each observation.

Applying these strategies to the Table in \ref{tbl-ud-info-values-masc},
we can see that each observation at its core is a word. We see that the
values of each observation are the attributes of each word.
\texttt{word} is the most individualized variable and the \texttt{pos}
(part-of-speech), \texttt{num\_letters}, and \texttt{ref\_num} all
describe the word.

The other variables \texttt{title}, \texttt{modality}, and \texttt{date}
are not direct attributes of the word. Instead, they are attributes of
the document in which the word appears. Together, however, they all
provide information about the word.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Data can be organized in many ways. It is important to make clear that
data in tabular format in itself does not constitute a dataset, in the
tidy sense we will be using. Can you think of examples of tabular
information that would not be in a tidy format? What would be the
implications of this for data analysis?

\end{tcolorbox}

As we round out this section on data organization, it is important to
stress that the purpose of curation is to represent the corpus data in
an informative, tidy format. A curated dataset serves as a reference
point making relationships explicit, enabling more efficient querying,
and paving the way for further processing before analysis. In the
subsequent section, we will highlight common approaches to modifying the
curated dataset, either row-wise or column-wise, to make it more
amenable to the particular aims of a given analysis.

\hypertarget{sec-ud-transformation}{%
\subsection{Transformation}\label{sec-ud-transformation}}

At this point have introduced the first step creating a dataset ready
for analysis, data curation. However, a curated dataset is rarely the
final organizational step before proceeding to statistical analysis.
Many times, if not always, the curated dataset requires \textbf{data
transformation} to derive or generate new data for the dataset. This
process may incur row-wise (observation) or column-wise (variable) level
changes, as illustrated in Figure~\ref{fig-ud-transformations}.

\begin{figure}[H]

{\centering \includegraphics[width=2.65in,height=\textheight]{figures/ud-transformations.drawio.png}

}

\caption{\label{fig-ud-transformations}Visualization of row-wise and
column-wise transformation operations on a dataset.}

\end{figure}

The results build on and manipulate the curated dataset to produce a
\emph{derived dataset}. While there is typically one curated dataset
that serves as the base organizational dataset, there may be multiple
derived datasets, each aligning with the informational needs of specific
analyses in the research project.

In what follows, we will discuss the most common types of data
transformation: text normalization, variable recoding, text
tokenization, variable generation, and observation/ variable merging.
Note, however, that the order in which these transformations are applied
in a given research project is not fixed and will vary depending on the
dataset and the research question(s) to be addressed.

\hypertarget{sec-ud-text-normalization}{%
\subsubsection{Text normalization}\label{sec-ud-text-normalization}}

The process of text normalization aims to prepare and standardize text.
It is often a preliminary step in data transformation processes which
include variables with text. The aim is to convert the text into a
uniform format to reduce unwanted variation and noise.

Let's take a toy dataset, in Table~\ref{tbl-ud-text-dataset}, as an
example starting point. In this dataset, we have two variables,
\texttt{text\_id} and \texttt{text}. It only has one observation.

\hypertarget{tbl-ud-text-dataset}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9000}}@{}}
\caption{\label{tbl-ud-text-dataset}A toy dataset with two variables,
\texttt{text\_id} and \texttt{text}.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & It's a beautiful day in the US, and our group decided to visit the
famous Grand Canyon. As we reached the destination, Jane said, ``I can't
believe we're finally here!'' The breathtaking view left us speechless;
indeed, it was a sight to behold. During our trip, we encountered
tourists from different countries, sharing stories and laughter. For all
of us, this experience will be cherished forever. \\
\end{longtable}

The types of transformations we apply will depend on the specific needs
of the project, but can include those found in
Table~\ref{tbl-ud-text-normalization}.

\hypertarget{tbl-ud-text-normalization}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4500}}@{}}
\caption{\label{tbl-ud-text-normalization}Common text normalization
tasks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relevant example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical purpose
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Task name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relevant example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Lowercasing & \texttt{"Text"} to \texttt{"text"} & Minimizing case
sensitivity in subsequent analysis \\
Removal of Punctuation and Special Characters &
\texttt{"Hello,\ World!"} to \texttt{"Hello\ World"} & Removing
non-alphanumeric characters that may not carry semantic value \\
Adjustment of Forms & \texttt{"colour"} to \texttt{"color"},
\texttt{"it\textquotesingle{}s"} to \texttt{"it\ is"}, \texttt{"1"} to
\texttt{"one"} & Standardizing variations in spelling, contractions, and
numeric forms to a common format \\
\end{longtable}

These transformations are column-wise operations, meaning they preserve
the number of rows in the dataset. They also preserve the number of
columns, but \emph{do} change the values of the variables. These tasks
should be applied with an understanding of how the changes will impact
the analysis. For example, lowercasing can be useful for reducing
differences between words that are otherwise identical, yet differ in
case due to word position in a sentence (``The'' versus ``the'').
However, lowercasing can also be problematic if the case of the word
carries semantic value, such as in the case of ``US'' (United States)
and ``us'' (first person plural pronoun).

Let's be conservative and only apply lowercasing to our toy dataset as
seen in Table~\ref{tbl-ud-text-dataset-lowercase}.

\hypertarget{tbl-ud-text-dataset-lowercase}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9000}}@{}}
\caption{\label{tbl-ud-text-dataset-lowercase}A toy dataset with two
variables, \texttt{text\_id} and \texttt{text}, where the text has been
lowercased.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & it's a beautiful day in the us, and our group decided to visit the
famous grand canyon. as we reached the destination, jane said, ``i can't
believe we're finally here!'' the breathtaking view left us speechless;
indeed, it was a sight to behold. during our trip, we encountered
tourists from different countries, sharing stories and laughter. for all
of us, this experience will be cherished forever. \\
\end{longtable}

When text normalization steps are motivated and applied with foresight
they serve to enhance the quality of the data and improves the
reliability of subsequent transformation steps.

\hypertarget{sec-ud-variable-recoding}{%
\subsubsection{Variable recoding}\label{sec-ud-variable-recoding}}

Recoding is the process of transforming the values of one or more
variables into new values which are more amenable to analysis. The aim
is to simplify complex variables, making it easier to identify patterns
and trends relevant for the research question. This is a column-wise
operation which can be applied to categorical or numeric variables.

Let's return to the MASC dataset and demonstrate recoding of categorical
and numeric variables. In Table~\ref{tbl-ud-info-values-masc} the
\texttt{pos} variable whose values represent the part-of-speech (POS) of
each token in the text. The measure is a POS tag from the Penn Treebank
tagset (\protect\hyperlink{ref-Marcus1993}{Marcus, Santorini, and
Marcinkiewicz 1993}). This tagset makes twelve major and 45 minor
grammatical class distinctions. In an analysis that aims to explore only
major class distinctions, it would be useful to recode the \texttt{pos}
variable into major classes only (\emph{i.e.} noun, pronoun, adjective,
verb, adverb, \emph{etc.}) to facilitate queries, summaries, and
visualizations.

\hypertarget{tbl-ud-recode-categorical}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.2179}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1154}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0641}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1026}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1410}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0513}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1538}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1538}}@{}}
\caption{\label{tbl-ud-recode-categorical}A toy dataset with three
variables, \texttt{text\_id}, \texttt{pos}, \texttt{major\_pos}, where
the \texttt{pos} variable has been recoded into major grammatical
classes \texttt{major\_pos}.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
title
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ref\_num
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
major\_pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
num\_letters
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
title
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ref\_num
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
major\_pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
num\_letters
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hotel California & Writing & 2008 & 0 & \textgreater{} & NN & noun &
1 \\
Hotel California & Writing & 2008 & 1 & Hotel & NNP & noun & 5 \\
Hotel California & Writing & 2008 & 2 & California & NNP & noun & 10 \\
Hotel California & Writing & 2008 & 3 & Fact & NNP & noun & 4 \\
Hotel California & Writing & 2008 & 4 & : & : & punctuation & 1 \\
Hotel California & Writing & 2008 & 5 & Sound & NNP & noun & 5 \\
Hotel California & Writing & 2008 & 6 & is & VBZ & verb & 2 \\
Hotel California & Writing & 2008 & 7 & a & DT & determiner & 1 \\
Hotel California & Writing & 2008 & 8 & vibration & NN & noun & 9 \\
Hotel California & Writing & 2008 & 9 & . & . & punctuation & 1 \\
\end{longtable}

In Table~\ref{tbl-ud-recode-categorical}, the \texttt{pos} variable has
been recoded into major grammatical classes. The \texttt{major\_pos}
variable is a categorical variable with 12 levels, one for each major
grammatical class in the Penn Treebank tagset. While the demonstration
here demonstrates the simplification of a categorical variable, recoding
can also be used to transliterate categorical variables. Continuing with
the theme of POS tags, the \texttt{pos} variable could be recoded into a
different tagset, such as the Universal Dependencies tagset
(\protect\hyperlink{ref-Nivre2016}{Nivre et al. 2016}).

Now, let's look at recoding the numeric variable \texttt{num\_letters}.
This variable represents the number of letters in each token. In the
MASC dataset, the \texttt{num\_letters} variable is a numeric variable
with a range of values from 1 to 21. In some analyses, it may be useful
to recode this variable into discrete categories, or bins, such as
short, medium, and long words.

\hypertarget{tbl-ud-recode-numeric}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0889}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0444}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1333}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1333}}@{}}
\caption{\label{tbl-ud-recode-numeric}The MASC dataset with the
\texttt{num\_letters} variable recoded into three categories: short,
medium, and long words in \texttt{word\_length}.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
title
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ref\_num
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
major\_pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
num\_letters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_length
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
title
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ref\_num
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
major\_pos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
num\_letters
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_length
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hotel California & Writing & 2008 & 0 & \textgreater{} & NN & noun & 1 &
short \\
Hotel California & Writing & 2008 & 1 & Hotel & NNP & noun & 5 &
medium \\
Hotel California & Writing & 2008 & 2 & California & NNP & noun & 10 &
long \\
Hotel California & Writing & 2008 & 3 & Fact & NNP & noun & 4 &
medium \\
Hotel California & Writing & 2008 & 4 & : & : & punctuation & 1 &
short \\
Hotel California & Writing & 2008 & 5 & Sound & NNP & noun & 5 &
medium \\
Hotel California & Writing & 2008 & 6 & is & VBZ & verb & 2 & short \\
Hotel California & Writing & 2008 & 7 & a & DT & determiner & 1 &
short \\
Hotel California & Writing & 2008 & 8 & vibration & NN & noun & 9 &
long \\
Hotel California & Writing & 2008 & 9 & . & . & punctuation & 1 &
short \\
\end{longtable}

In Table~\ref{tbl-ud-recode-numeric} the variable \texttt{word\_length}
appears with the values \texttt{short}, \texttt{medium}, and
\texttt{long}. This is now a categorical variable of type ordinal. Of
note, is that the operational definition of used to create these word
length bins should be made explicit in the documentation of the dataset.

In sum, recoding is a useful data transformation technique that can be
used to simplify complex variables, making it easier to identify
patterns and trends relevant for the research question.

\hypertarget{sec-ud-text-tokenization}{%
\subsubsection{Text tokenization}\label{sec-ud-text-tokenization}}

A text-oriented transformation step is \textbf{text tokenization}. This
process involves adapting the text such that it reflects the target
linguistic unit that will be used in the analysis. This is a row-wise
operation expanding the number of rows, if the linguistic unit is
smaller than the original variable, or reducing the number of rows, if
the linguistic unit is larger than the original variable. At its core,
tokenization is the process which enables the quantitative analysis of
text.

Text variables can be tokenized at any linguistic level. To illustrate,
consider our toy dataset from Table~\ref{tbl-ud-text-dataset-lowercase}.
We can tokenize the text at the sentence level, in
Table~\ref{tbl-ud-text-dataset-tokenization-sentence}, by splitting the
text at the period followed by a space. This results in a dataset with
four observations, one for each sentence in the original text.

\hypertarget{tbl-ud-text-dataset-tokenization-sentence}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9000}}@{}}
\caption{\label{tbl-ud-text-dataset-tokenization-sentence}A toy dataset
with two variables, \texttt{text\_id} and \texttt{sentence}, where the
text has been tokenized at the sentence level.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sentence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & it's a beautiful day in the us, and our group decided to visit the
famous grand canyon \\
1 & as we reached the destination, jane said, ``i can't believe we're
finally here!'' the breathtaking view left us speechless; indeed, it was
a sight to behold \\
1 & during our trip, we encountered tourists from different countries,
sharing stories and laughter \\
1 & for all of us, this experience will be cherished forever. \\
\end{longtable}

It is important to make explicit what the operationalization of our
linguistic unit is as common terms such as sentence, word, \emph{etc.}
can be defined in different ways. For example, the sentence tokenization
above is based on the assumption that sentences are separated by a
period followed by a space. This is a suitable definition for this text,
but likely will not be for other English text or for other languages/
writing scripts. For words, a very simple operationalization is to use
whitespace separation (\emph{e.g.} ``I cannot believe it.'' -- {[}``I'',
``cannot'', ``believe'', ``it.''{]}). However, this approach does not
handle puntuation marks (\emph{e.g.} {[}``it.''{]}) or contractions
(\emph{e.g.} {[}``can't''{]}). A more sophisticated operationalization
will be necessary for these, and possibly other, cases.

Another important token unit is the \(n\)-gram. Words or characters can
be grouped into contiguous sequences with a moving window of a certain
size \(n\). Single unit windows are referred to as unigrams, two units
as bigrams, three units as trigrams, and so on. Let's tokenize our toy
dataset at the bigram level for words using a simple whitespace
separation for words, as seen in Table~\ref{tbl-ud-text-dataset-bigram}.

\hypertarget{tbl-ud-text-dataset-bigram}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-ud-text-dataset-bigram}A toy dataset with two
variables, \texttt{text\_id} and \texttt{bigram}, where the text has
been tokenized at the bigram word level.}\tabularnewline
\toprule\noalign{}
text\_id & word \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
text\_id & word \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & it's a \\
1 & a beautiful \\
1 & beautiful day \\
1 & day in \\
1 & in the \\
1 & the us \\
1 & us and \\
1 & and our \\
1 & our group \\
1 & group decided \\
\end{longtable}

In Table~\ref{tbl-ud-text-dataset-bigram} we see that the first bigram
is ``it's a'' --the first two words (based on whitespace separation) in
the text. The second bigram is ``a toy'' --the second and third words in
the text. This continues to the end of the text. \(N\)-gram tokenization
can be useful to capture context that would otherwise would be lost from
tokenizing words or characters at the unigram level.

Up to this point our tokens have been surface forms. That is, they are
the actual words or characters as they appear in the text. However, we
may want to reduce the tokens to their base form, removing their
inflectional forms. This is known as \textbf{lemmatization}. For
example, the word ``run'' is the lemma of the words ``running'',
``runs'', and ``ran''. Let's lemmatize the third sentence in our toy
dataset. For comparison, \texttt{word} and \texttt{lemma} are shown
side-by-side in Table~\ref{tbl-ud-text-dataset-lemmatization}.

\hypertarget{tbl-ud-text-dataset-lemmatization}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-ud-text-dataset-lemmatization}A toy dataset with two
variables, \texttt{text\_id} and \texttt{word}, where the text has been
tokenized at the unigram word level and lemmatized.}\tabularnewline
\toprule\noalign{}
text\_id & word & lemma \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
text\_id & word & lemma \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & during & during \\
1 & our & our \\
1 & trip & trip \\
1 & we & we \\
1 & encountered & encounter \\
1 & tourists & tourist \\
1 & from & from \\
1 & different & different \\
1 & countries & country \\
1 & sharing & share \\
1 & stories & story \\
1 & and & and \\
1 & laughter & laughter \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-alt} Case study}

Inflectional family size is the number of inflectional forms for a given
word and can be calculated from a corpus by counting the number of
surface forms for each lemma in the corpus
(\protect\hyperlink{ref-Kostic2003}{Kosti, Markovi, and Baucal 2003}).
Baayen, Feldman, and Schreuder
(\protect\hyperlink{ref-Baayen2006}{2006}) found that words with larger
inflectional family size are associated with faster word recognition
times in lexical processing tasks.

\end{tcolorbox}

Together tokenization and lemmatization are powerful tools for
transforming text. If our dataset contains more robust linguistic
annotation or that annotation can be generated (see
Section~\ref{sec-ud-variable-generation}), this information can also be
leveraged to tokenize language into a format that is easier to explore
and quantify in an analysis.

\hypertarget{sec-ud-variable-generation}{%
\subsubsection{Variable generation}\label{sec-ud-variable-generation}}

The process of variable generation aims to augment existing variables or
create new ones, and as such it is a column-wise operation. Generation
can include applying calculations or extracting relevant information
from existing variables or enhancing text variables with linguistic
annotation. Simplifying a bit, generation helps make implicit attributes
explicit. The results of this process enables direct access during
analysis to features that were otherwise hidden or difficult to access.

Let's highlight a some common calculation and extraction examples that
generate variables. First, let's look at the calculation of measures. In
text analysis, measures are often used to describe the properties of a
document or linguistic unit. For example, the number of words in a
corpus document, the lengths of sentences, the number of clauses in a
sentence, \emph{etc.}. In turn, these measures can be used to calculate
other measures, such as lexical diversity or syntactic complexity
measures.

In terms of extraction, the goal is to distill relevant information from
existing variables. For example, extracting the year from a date
variable, or extracting the first name from a full name variable. In
text analysis, extraction is often used to extract information from text
variables. Say we have a dataset with a variable containing conversation
utterances. We may want to extract some characteristic from those
utterances and capture their occurrence in a new variable.

But what if we want to extract linguistic information from a text
variable that is not explicitly present in the text? This is where
linguistic annotation comes in. Linguistic annotation is the process of
enriching text with linguistic information, such as morphological
features, part-of-speech tags, syntactic structure, \emph{etc.}. This
can be done manually by linguist coders and/ or done using natural
language processing (NLP) tools, many of which are available in R (see
Chapter~\ref{sec-transform-datasets}).

To illustrate the process of generating linguistic annotation with
existing tools, I will use the plain text version of the MASC. In
Table~\ref{tbl-ud-masc-plain-dataset}, the text has been organized into
a dataset and tokenized into sentences. The \texttt{text\_id} variable
is a unique identifier for each document, and the \texttt{sentence\_id}
variable is a unique identifier for each sentence.

\hypertarget{tbl-ud-masc-plain-dataset}{}
\begin{table}
\caption{\label{tbl-ud-masc-plain-dataset}A MASC sample document in dataset tokenized into sentences. }\tabularnewline

\centering
\begin{tabular}{lrl}
\toprule
text\_id & sentence\_id & sentence\\
\midrule
1 & 1 & >Hotel California Fact: Sound is a vibration.\\
1 & 2 & Sound travels as a mechanical wave through a medium, and in space, there is no medium.\\
1 & 3 & So when my shuttle malfunctioned and the airlocks didn't keep the air in, I heard nothing.\\
1 & 4 & After the first whoosh of the air being sucked away, there was lightning, but no thunder.\\
1 & 5 & Eyes bulging in panic, but no screams.\\
\bottomrule
\end{tabular}
\end{table}

Applying a pre-trained model from the
\href{https://universaldependencies.org/}{Universal Dependencies
(UD)}\footnote{The Universal Dependency project is an effort to develop
  cross-linguistically consistent treebank annotation for many
  languages. The project has developed a set of annotation guidelines
  and a set of tools for generating linguistic annotation. The project
  has also developed a set of pre-trained models for many languages.}
project, we can generate linguistic annotation for each token in the
MASC.

\hypertarget{tbl-ud-generate-annotation}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0515}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0882}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0662}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0735}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0368}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.5441}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1397}}@{}}
\caption{\label{tbl-ud-generate-annotation}Automatic linguistic
annotation for grammatical category and syntactic structure for an
example English sentence from the MASC.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sentence\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
token\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
token
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
xpos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
syntactic\_relation
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sentence\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
token\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
token
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
xpos
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
features
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
syntactic\_relation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 4 & 1 & After & IN & NA & mark \\
1 & 4 & 2 & the & DT & Definite=Def\textbar PronType=Art & det \\
1 & 4 & 3 & first & JJ & Degree=Pos\textbar NumType=Ord & amod \\
1 & 4 & 4 & whoosh & NN & Number=Sing & nsubj:pass \\
1 & 4 & 5 & of & IN & NA & case \\
1 & 4 & 6 & the & DT & Definite=Def\textbar PronType=Art & det \\
1 & 4 & 7 & air & NN & Number=Sing & nmod \\
1 & 4 & 8 & being & VBG & VerbForm=Ger & aux:pass \\
1 & 4 & 9 & sucked & VBN &
Tense=Past\textbar VerbForm=Part\textbar Voice=Pass & advcl \\
1 & 4 & 10 & away & RB & NA & advmod \\
1 & 4 & 11 & , & , & NA & punct \\
1 & 4 & 12 & there & EX & NA & expl \\
1 & 4 & 13 & was & VBD &
Mood=Ind\textbar Number=Sing\textbar Person=3\textbar Tense=Past\textbar VerbForm=Fin
& root \\
1 & 4 & 14 & lightning & NN & Number=Sing & nsubj \\
1 & 4 & 15 & , & , & NA & punct \\
1 & 4 & 16 & but & CC & NA & cc \\
1 & 4 & 17 & no & DT & NA & det \\
1 & 4 & 18 & thunder & NN & Number=Sing & conj \\
1 & 4 & 19 & . & . & NA & punct \\
\end{longtable}

The annotated dataset now includes the key variables \texttt{xpos} (Penn
treebank tags), \texttt{features} (morphological features), and
\texttt{syntactic\_relation}. The results of this process can then be
further transformed as need be to fit the needs of the analysis.

A word of caution: automated linguistic annotation offers rapid access
to abundant and highly dependable linguistic data for numerous
languages. However, linguistic annotation tools are not infallible. They
are tools developed by training computational algorithms to identify
patterns in previously annotated and verified datasets, resulting in a
language model. This model is then employed to predict linguistic
annotations for new language data (as seen in
Table~\ref{tbl-ud-generate-annotation}). The accuracy of the linguistic
annotation heavily relies on the congruence between the language
sampling framework of the trained data and the language data set to be
automatically annotated.

\hypertarget{sec-ud-obs-variable-merging}{%
\subsubsection{Observation/ variable
merging}\label{sec-ud-obs-variable-merging}}

The processing of merging datasets is a transformation step which can be
row-wise or column-wise. Row-wise merging is the process of combining
datasets by appending observations from one dataset to another.
Column-wise merging is the process of combining datasets by appending
variables from one dataset to another. In either case, merging provides
a way to enrich a dataset by incorporating additional information.

To merge in row-wise manner the datasets involved in the process must
have the same variables and variable types. This process is often
referred to as \textbf{concatenating datasets}. It can be thought of as
stacking datasets on top of each other to create a larger dataset.
Remember, having the same variables and variable types is not the same
has having the same values.

Take, for example, a case when a corpus resource contains data for two
populations. In the course of curating and transforming the datasets it
may make more sense to work with the datasets separately. However, when
it comes time to analyze the data, it may be more convenient to work
with the datasets as a single dataset. In this case, the datasets can be
concatenated to create a single dataset.

To illustate, consider the toy datasets in
Table~\ref{tbl-ud-merge-dataset-written} and
Table~\ref{tbl-ud-merge-dataset-spoken}.

\hypertarget{tbl-ud-merge-dataset-written}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}
\caption{\label{tbl-ud-merge-dataset-written}Toy dataset of written text
data.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. \\
\end{longtable}

\hypertarget{tbl-ud-merge-dataset-spoken}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}
\caption{\label{tbl-ud-merge-dataset-spoken}Toy dataset of spoken text
data.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. \\
\end{longtable}

These datasets, in Table~\ref{tbl-ud-merge-dataset-written} and
Table~\ref{tbl-ud-merge-dataset-spoken}, contain the same variables and
variable types, but different observations --one in which the sample
contains written language and the other spoken. Conveniently, they can
be concatenated to create a single dataset that contains all of the
observations, as seen in Table~\ref{tbl-ud-merge-dataset-concat}.

\hypertarget{tbl-ud-merge-dataset-concat}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5500}}@{}}
\caption{\label{tbl-ud-merge-dataset-concat}Toy dataset of written and
spoken text data concatenated.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. \\
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. \\
\end{longtable}

Merging datasets can be performed in a column-wise manner as well. In
this process, the datasets need not have the exact same variables and
variable types, rather it is required that the datasets share a common
variable of the same informational type that can be used to index the
datasets. This process is often referred to as \textbf{joining
datasets}.

Corpus resources often include metadata in stand-off annotation format.
That is, the metadata is not embedded in the corpus files, but rather is
stored in a separate file. The metatdata and corpus files will share a
common variable which is used to join the metadata with the corpus
files.

To exemplify, here's another toy dataset that shares the
\texttt{participant\_id} index with the previous dataset in
Table~\ref{tbl-ud-merge-dataset-concat} and includes the variables
\texttt{native\_speaker\_eng}, \texttt{age}, and \texttt{gender}:

\hypertarget{tbl-ud-merge-vars-participant}{}
\begin{longtable}[]{@{}llrl@{}}
\caption{\label{tbl-ud-merge-vars-participant}Toy dataset of participant
data with a shared variable \texttt{participant\_id} to index the
datasets.}\tabularnewline
\toprule\noalign{}
participant\_id & native\_speaker\_eng & age & gender \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
participant\_id & native\_speaker\_eng & age & gender \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & Yes & 28 & M \\
P2 & No & 35 & M \\
P3 & Yes & 42 & F \\
P4 & No & 26 & F \\
P5 & Yes & 31 & M \\
P6 & No & 39 & F \\
\end{longtable}

This dataset provides additional information about each participant,
such as their English native speaker status, age, and gender.

Since the two datasets share the \texttt{participant\_id} variable, we
can merge them to create a new dataset that combines the information
from both datasets, as we see in Table~\ref{tbl-ud-merge-join}.

\hypertarget{tbl-ud-merge-join}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0800}}@{}}
\caption{\label{tbl-ud-merge-join}Joining variables from two datasets
based on a shared index variable.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
native\_speaker\_eng
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
participant\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
native\_speaker\_eng
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
P1 & T1 & Written & Technology has revolutionized our lives in many
ways. It has made communication easier, faster, and more efficient. &
Yes & 28 & M \\
P3 & T3 & Written & Climate change is a pressing issue that affects
everyone on Earth. We must take immediate action to reduce our carbon
footprint. & Yes & 42 & F \\
P5 & T5 & Written & Education is the key to personal and societal
growth. Investing in quality education will lead to a brighter future
for all. & Yes & 31 & M \\
P2 & T2 & Spoken & Hello, my name is X. I am a software engineer working
at XYZ company. & No & 35 & M \\
P4 & T4 & Spoken & Hi, I'm X, and I work as a project manager. My main
responsibility is to ensure that projects are completed on time and
within budget. & No & 26 & F \\
P6 & T6 & Spoken & Hi, my name is X, and I'm a teacher. I teach English
at a local high school. & No & 39 & F \\
\end{longtable}

Joining datasets is a powerful tool for enriching a dataset with
additional column-wise information. It is important to note that merging
datasets can also remove information in a row-wise manner. For example,
when merging two datasets with a shared variable, it is possible to
remove observations that do not have a match one of the two datasets.
This process effectively filters out observations not shared between the
two datasets. On the other hand, an anti-join explicitly removes
observations that are shared between the two datasets.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

In some analyses, it may be useful to remove words with little semantic
value, such as articles, prepositions, and conjunctions or words that
are very common in the language. These are known as stopwords. There are
various predefined lists of stopwords for different languages available
on the web and through R in the \texttt{stopwords} package
(\protect\hyperlink{ref-R-stopwords}{Benoit, Muhr, and Watanabe 2021}).
Anti-joining a stopword list with a dataset of word tokens is often used
to remove stopwords from the dataset.

However, it is important to note the criteria used to determine which
words are considered stopwords in a particular resource may not fit a
researcher's needs or the characteristics of the data. Learn more about
approaches to identifying stopwords in Kaur and Buttar
(\protect\hyperlink{ref-Kaur2018}{2018}).

\end{tcolorbox}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In sum, the transformation steps described here collectively aim to
produce higher quality datasets that are relevant in content and
structure to submit to analysis. The process may include one or more of
the previous transformations but is rarely linear and is most often
iterative. It is typical to do some normalization then generation, then
recoding, and then return to normalizing, and so forth. This process is
highly idiosyncratic given the characteristics of the curated dataset
and the ultimate goal for the derived dataset(s).

\hypertarget{sec-ud-documentation}{%
\section{Documentation}\label{sec-ud-documentation}}

As we have seen in this chapter, acquiring corpus data and converting
that data into information involves a number of conscious decisions and
implementation steps. As a favor to ourselves, as researchers, and to
the research community, it is crucial to document these decisions and
steps. This makes it both possible to retrace our own steps and also
provides a guide for future researchers that want to reproduce and/ or
build on your research. A programmatic approach to quantitative research
helps ensure that the implementation steps are documented and
reproducible but it is also vital that the decisions that are made are
documented as well. This includes data origin information for the
acquired corpus data and data dictionaries for the curated and derived
datasets.

\hypertarget{sec-ud-data-origin}{%
\subsection{Data origin}\label{sec-ud-data-origin}}

Data acquired from corpus resources should be accompanied by information
about the \textbf{data origin}. Table~\ref{tbl-ud-data-origin} provides
a list of the types of information that should be included in the data
origin information.

\hypertarget{tbl-ud-data-origin}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\caption{\label{tbl-ud-data-origin}Data origin
information.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Resource name & Name of the corpus resource. \\
Data source & URL, DOI, \emph{etc.} \\
Data sampling frame & Language, language variety, modality, genre,
\emph{etc.} \\
Data collection date(s) & The date or date range of the data
collection. \\
Data format & Plain text, XML, HTML, \emph{etc.} \\
Data schema & Relationships between data elements: files, folders,
\emph{etc.} \\
License & CC BY, CC BY-NC, \emph{etc.} \\
Attribution & Citation information for the data source. \\
\end{longtable}

For many corpus resources, the corpus documentation will include all or
most of this information as part of the resource download or documented
online. If this information is not present in the corpus resource or you
compile your own, it is important to document this information yourself.
This information can be documented in file, such as a plain text file or
spreadsheet, that is included with the corpus resource.

\hypertarget{sec-ud-data-dictionaries}{%
\subsection{Data dictionaries}\label{sec-ud-data-dictionaries}}

The process of organizing the data into a dataset, curation, and
modifications to the dataset in preparation for analysis,
transformation, each include a number of project-specific decisions.
These decisions should be documented.

On the one hand each dataset that is created should have a \textbf{data
dictionary} file. A data dictionary is a document, usually in a
spreadsheet format, that describes the variables in a dataset. The key
information that should be included in a data dictionary is provided in
Table~\ref{tbl-ud-data-dictionary}.

\hypertarget{tbl-ud-data-dictionary}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\caption{\label{tbl-ud-data-dictionary}Data dictionary
information.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Information
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Variable name & The name of the variable as it appears in the dataset,
\emph{e.g.} \texttt{participant\_id}, \texttt{modality}, \emph{etc.} \\
Readable variable name & A human-readable name for the variable,
\emph{e.g.} `Participant ID', `Language modality', \emph{etc.} \\
Variable type & The type of information that the variable contains,
\emph{e.g.} `categorical', `ordinal', \emph{etc.} \\
Variable description & A prose description expanding on the readable
name and can include measurement units, allowed values, \emph{etc.} \\
\end{longtable}

Organizing this information in a tabular format, such as a spreadsheet,
can make it easy for others to read and understand your data dictionary.

On the other hand, the data curation and transformation steps should be
documented in the code that is used to create the dataset. This is one
of the valuable features of a programmatic approach to quantitative
research. The transparency of this documentation is enhanced by using
\textbf{literate programming} strategies to intermingling prose
descriptions and code the steps in the same, reproducible document.

By providing a comprehensive data dictionary and using a programmatic
approach to data curation and transformation, you ensure that others can
easily understand and work with your dataset, facilitating collaboration
and reproducibility.

\hypertarget{summary-1}{%
\section*{Summary}\label{summary-1}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on data and information --the first two
components of DIKI Hierarchy. This process is visualized in
Figure~\ref{fig-understanding-data-vis-sum}.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{figures/ud-diki.drawio.png}

}

\caption{\label{fig-understanding-data-vis-sum}Understanding data:
visual summary}

\end{figure}

First a distinction is made between populations and samples, the latter
being a intentional and subjective selection of observations from the
world which attempt to represent the population of interest. The result
of this process is known as a corpus. Whether developing a corpus or
selecting an existing a corpus it is important to vet the sampling frame
for its applicability and viability as a resource for a given research
project.

Once a viable corpus is identified, then that corpus is converted into a
curated dataset which adopts the tidy dataset format where each column
is a variable, each row is an observation, and the intersection of
columns and rows contain values. This curated dataset serves to
establish the base informational relationships from which your research
will stem.

The curated dataset will most likely require transformations which may
include normalization, tokenization, recoding, generation, and/ or
merging to enhance the usefulness of the information to analysis. A
derived dataset or set of datasets will the result from this process.

Finally, documentation should be implemented at the acquisition,
curation, and transformation stages of the analysis project process. The
combination of data origin, data dictionary, and literate programming
files establishes documentation of the data and implementation steps to
ensure transparent and reproducible research.

\hypertarget{activities}{%
\section*{Activities}\label{activities}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

In the following activities you will learn how to read, inspect, and
write data and datasets in R using reproducible strategies.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-2.html}{Reading,
inspecting, and writing data}\\
\textbf{How}: Read Recipe 2 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To use literate programming in Quarto to work with R
coding strategies for reading, inspecting, and writing datasets.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-2}{Reading,
inspecting, and writing data}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 2.\\
\textbf{Why}: To read datasets from packages and from plain-text files,
inspect and report characteristics of datasets, and write datasets to
plain-text files.

\end{tcolorbox}

\hypertarget{questions-1}{%
\section*{Questions}\label{questions-1}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  What is the difference between a population and a sample?
\item
  Why is it important to vet a corpus before using it in a research
  project?
\item
  What is a curated dataset in the context of linguistic research?
\item
  What is the difference between a variable, an observation, and a
  value?
\item
  Why is it important to identify the informationasl types of variables
  in a dataset?
\item
  What kinds of transformations may be performed on a curated dataset to
  enhance its usefulness for analysis?
\item
  What is an transformed dataset and why is it important in linguistic
  research?
\item
  Why is documentation important in the process of conducting linguistic
  analysis?
\item
  How does a programmatic approach enhance documentation in linguistic
  research?
\item
  How does documenting the corpus data and the curated and derived
  datasets contribute to transparent and reproducible research in
  linguistics?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Technical questions}

\begin{itemize}
\tightlist
\item
  Creating a sample corpus.
\item
  Writing a corpus documentation.
\item
  Converting a corpus to a derived dataset.
\item
  Writing a data dictionary.
\item
  Transforming a derived dataset.
\item
  Merging datasets.
\item
  Writing a dataset to disk.
\item
  Consider (an example dataset) and its data dictionary, write a script
  to read the dataset, inspect it, and write it to disk.
\item
  Consider a dataset and its data dictionary what appears to be the unit
  of analysis and the unit of observation?
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-approaching-analysis}{%
\chapter{Approaching analysis}\label{sec-approaching-analysis}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
Statistical thinking will one day be as necessary for efficient
citizenship as the ability to read and write.

--- H.G. Wells
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Recall the fundamental concepts and principles of statistics in data
  analysis.
\item
  Articulate the roles of diagnostic, analytic, and interpretive
  statistics in quantitative analysis.
\item
  Compare the similarities and differences between analytic approaches
  to data analysis.
\end{itemize}

\end{tcolorbox}

In this chapter I will build on the notions of data and information from
the previous chapter. The aim of analysis is to derive knowledge from
information, the next step in the DIKI Hierarchy. Where the creation of
information from data involves human intervention and conscious
decisions, as we have seen, deriving knowledge from information involves
another level of intervention. The goal is to break down complex
information into simpler components which are more readily
interpretable. In what follows, we will cover the main steps in the
process of analysis. The first is to inspect the data to ensure its
quality and understand its characteristics. The second is to interrogate
the data to uncover patterns and relationships and interpret the
findings. To conclude this chapter I will outline methods to and the
importance of communicating the analysis results and procedure in a
transparent and reproducible manner.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Data
visualization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To explore data visually in text and in graphics.

\end{tcolorbox}

\hypertarget{sec-aa-diagnose}{%
\section{Diagnose}\label{sec-aa-diagnose}}

The purpose of diagnostic measures is to inspect your data to ensure its
quality and understand its characteristics. There are two primary types
of diagnostic measures: verfication and description. Verification
methods are applied to catch missing or erroneous data while descriptive
methods are used to gain a better understanding of the data. Although
treated in two separate sections, in practice these methods are
complementary and are often addressed in tandem.

To ground this discussion I will introduce a new dataset. This dataset
is drawn from the Barcelona English Language Corpus (BELC)
(\protect\hyperlink{ref-Munoz2006}{Muoz 2006}), which is found in the
TalkBank repository. I've selected the ``Written composition'' task from
this corpus which contains 80 writing samples from 36 second language
learners of English at different ages. Participants were given the task
of writing for 15 minutes on the topic of ``Me: my past, present and
future''. Data was collected for participants from one to three times
over the course of seven years (at 10, 12, 16, and 17 years of age).

In Table~\ref{tbl-aa-belc-dd} we see the data dictionary for the BELC
dataset which reflects structural and transformational steps I've done
so we start with a tidy dataset with \texttt{word} as the unit of
observation.

\hypertarget{tbl-aa-belc-dd}{}
\begin{table}
\caption{\label{tbl-aa-belc-dd}Data dictionary for the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & description & variable\_type\\
\hline
part\_id & Participant ID & Unique identifier for each participant & categorical\\
\hline
sex & Participant's sex & Sex of the participant & categorical\\
\hline
group & Time group & Longitudinal group to which the participant belongs & ordinal\\
\hline
month\_age & Participant's age in months & Age of the participant in months & numeric\\
\hline
utt\_id & Utterance ID & Unique identifier for each utterance & numeric\\
\hline
word\_id & Word ID & Unique identifier for each word within an utterance & numeric\\
\hline
word & Word & The word spoken by the participant & categorical\\
\hline
lemma & Word lemma & Base form of the word & categorical\\
\hline
pos & Part of speech & Grammatical category of the word & categorical\\
\hline
\end{tabular}
\end{table}

The data dictionary provides a easily accessible overview of the
dataset. This includes a human-readable mapping from variable names to
variable descriptions. Further, it provides information about the type
of variable (e.g., categorical, ordinal, numeric). As we will see the
informational type of variables is key to diagnostic measures, as well
as all other components of analysis.

\hypertarget{sec-aa-verfiy}{%
\subsection{Verify}\label{sec-aa-verfiy}}

Although a dataset has undergone curation and transformation, it is
still important to verify the data. This is a process of checking the
data to ensure that it is accurate and complete. In the case that it is
not, consideration should be given to how to address the issues.

The most basic and usually the first step is to check for
\textbf{missing data} to ensure that all necessary data points are
present. In Table~\ref{tbl-aa-belc-skim-missing}, there are missing
values for the \texttt{lemma} and \texttt{pos} variables in the BELC
dataset.

\hypertarget{tbl-aa-belc-skim-missing}{}
\begin{table}
\caption{\label{tbl-aa-belc-skim-missing}Summary output for missing values in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|r|r}
\hline
variable & type & n\_missing & complete\_rate\\
\hline
part\_id & character & 0 & 1.000\\
\hline
sex & character & 0 & 1.000\\
\hline
group & character & 0 & 1.000\\
\hline
word & character & 0 & 1.000\\
\hline
lemma & character & 79 & 0.985\\
\hline
pos & character & 23 & 0.996\\
\hline
month\_age & numeric & 0 & 1.000\\
\hline
utt\_id & numeric & 0 & 1.000\\
\hline
word\_id & numeric & 0 & 1.000\\
\hline
\end{tabular}
\end{table}

There are two primary approaches to dealing with missing data: deletion
and recoding. Since these missing values account for only 1.5\% and
0.4\% of the data respectively, we might be safe to remove these
observations. Another approach is to recode the missing values by either
applying a unique value for missing values (e.g., \texttt{NULL}) or by
imputing values. Imputing values is usually done by replacing missing
values with some middle-of-the-road value (e.g., mean, median, mode),
but other, more nuanced approaches are possible.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

For more information on missing data, see the \faIcon{wrench} in this
book.

\end{tcolorbox}

In either case, it is important to consider the implications of missing
data for the analysis. For example, if the missing data is not at random
or include a sizeable portion of the values of interest, then the
analysis may be biased.

Value coding schemes, annotation errors, or other issues may result in
\textbf{anomalies} in the data. These are values that are unusual,
unexpected, or inconsistent with the rest of the data or effect the
treatment of the data for the particular analysis to be performed.

For categorical variables, this may include values that are not expected
or are not in the set of values that are expected. A summary of the
values for a given variable can be used as a first step to identify
anomalies. In Table~\ref{tbl-aa-belc-skim-categorical}, we see the
minimum and maximum number of characters and the number of unique values
for each categorical variable in the BELC dataset.

\hypertarget{tbl-aa-belc-skim-categorical}{}
\begin{table}
\caption{\label{tbl-aa-belc-skim-categorical}Summary output for categorical variables in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|r|r|r}
\hline
variable & min\_chars & max\_chars & num\_unique\\
\hline
part\_id & 3 & 3 & 36\\
\hline
sex & 4 & 6 & 2\\
\hline
group & 2 & 2 & 4\\
\hline
word & 1 & 20 & 913\\
\hline
lemma & 1 & 20 & 774\\
\hline
pos & 1 & 9 & 38\\
\hline
\end{tabular}
\end{table}

From our knowledge of the data, we can gauge whether these values are
expected. For example, \texttt{sex} has two values; likely corresponding
to some coding of `male' and `female'. The variable \texttt{part\_id}
has 36 distinct values, which is expected since there are 36
participants and \texttt{group} has four, corresponding to the
longitudinal time groups. It is also possible to gauge the expected
values for \texttt{lemma} as we know that these are the base words and
should be less than the number of words in the dataset.

Further verfication of the categorical variables is need, of course.
This may include aggregating the data to see the distribution of values
and/ or checking the values against the documentation.

Let's now consider numeric variables. Numeric variables, by their very
nature, do not lend themselves to the same type of summary used for
categorical variables (\emph{i.e.} character lengths, number of unique
values, or aggregation) to detect anomalies. For numeric variables there
are two types of anomalies that we will consider: outliers and errors in
coding. \textbf{Outliers} are anomalies that are extreme values that are
not representative of the great majority of the data points. To
determine what is extreme, we need to consider the distribution of the
data, that is, the range of values and the frequency of values. It is
rarely the case that we can eyeball the distribution of the data based
on raw values. Instead, a combination of summary statistics and
visualizations are used to determine the distribution of the data. For
this reason, the detection of outliers is often carried out as part of
the descriptive assessment of the data, as we will see in
Section~\ref{sec-aa-describe}.

On the other hand, coding anomalies are values that are not expected or
are not in the set of values that are expected. These can sometimes be
detected by visual inspection of the data. For example, in
Table~\ref{tbl-aa-belc-numeric}, we see the first 10 observations for
each variable in the BELC dataset.

\hypertarget{tbl-aa-belc-numeric}{}
\begin{table}
\caption{\label{tbl-aa-belc-numeric}First 10 observations for variables in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|r|r|r|l|l|l}
\hline
part\_id & sex & group & month\_age & utt\_id & word\_id & word & lemma & pos\\
\hline
L01 & female & T2 & 153 & 0 & 0 & I & I & pro:sub\\
\hline
L01 & female & T2 & 153 & 0 & 1 & was & be & cop\\
\hline
L01 & female & T2 & 153 & 0 & 2 & born & born & adj\\
\hline
L01 & female & T2 & 153 & 0 & 3 & in & in & prep\\
\hline
L01 & female & T2 & 153 & 0 & 4 & Barcelona & Barcelona & n:prop\\
\hline
L01 & female & T2 & 153 & 0 & 5 & and & and & coord\\
\hline
L01 & female & T2 & 153 & 0 & 6 & I & I & pro:sub\\
\hline
L01 & female & T2 & 153 & 0 & 7 & live & live & v\\
\hline
L01 & female & T2 & 153 & 0 & 8 & in & in & prep\\
\hline
L01 & female & T2 & 153 & 0 & 9 & Barcelona & Barcelona & n:prop\\
\hline
\end{tabular}
\end{table}

Leaving \texttt{month\_age} aside, we see that the other two numeric
variables \texttt{utt\_id} and \texttt{word\_id} index utterances and
words respectively. However, in contrast to \texttt{part\_id} which is a
categorical variable as it serves as a unique identifier for each
participant, these variables are numeric as they serve to not only index
utterances and words but also to provide a measure of how many
utterances or words have been produced. Seen in this light, \texttt{0}
for the first value of \texttt{utt\_id} and \texttt{word\_id} is
unexpected. To adjust for this, we can add \texttt{1} to each value of
these variables.

\hypertarget{sec-aa-describe}{%
\subsection{Describe}\label{sec-aa-describe}}

The goal of descriptive statistics is to summarize the data in order to
understand and prepare the data for the analysis approach to be
performed. This is accomplished through a combination of statistic
measures and/ or tabular or graphic summaries. The choice of descriptive
statistics is guided by the type of data, as well as the question(s)
being asked of the data.

To that end, let's consider a reconfiguration of the BELC dataset, in
Table~\ref{tbl-aa-belc-reconfig}, which will provide a more illustrative
dataset.

\hypertarget{tbl-aa-belc-reconfig}{}
\begin{table}
\caption{\label{tbl-aa-belc-reconfig}First 10 observations of the reconfigured BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|r|r|r|r}
\hline
essay\_id & part\_id & sex & group & tokens & types & ttr & prop\_l2\\
\hline
E1 & L01 & female & T2 & 79 & 46 & 0.582 & 0.987\\
\hline
E2 & L02 & female & T1 & 18 & 18 & 1.000 & 0.667\\
\hline
E3 & L02 & female & T3 & 101 & 53 & 0.525 & 1.000\\
\hline
E4 & L05 & female & T1 & 20 & 17 & 0.850 & 0.900\\
\hline
E5 & L05 & female & T3 & 158 & 80 & 0.506 & 0.987\\
\hline
E6 & L05 & female & T4 & 184 & 94 & 0.511 & 0.995\\
\hline
E7 & L07 & male & T3 & 98 & 60 & 0.612 & 1.000\\
\hline
E8 & L07 & male & T4 & 134 & 84 & 0.627 & 0.978\\
\hline
E9 & L10 & female & T1 & 38 & 28 & 0.737 & 0.974\\
\hline
E10 & L10 & female & T3 & 118 & 74 & 0.627 & 1.000\\
\hline
\end{tabular}
\end{table}

In this new configuration, the unit of observation is now
\texttt{essay\_id}. Each of the following variable are attributes or
measures of this variable. The new variables in this dataset are
aggregates of the previous BELC dataset: \texttt{tokens} is the number
of total words, \texttt{types} is the number of unique words,
\texttt{ttr} is the ratio of unique words to total words. This is known
as the Type-Token Ratio and it is a standard metric for measuring
lexical diversity. Finally, the proportion of L2 words (English) to the
total words (tokens) is provided in \texttt{prop\_l2}.

In descriptive statistics, there are four basic questions that are asked
of each of the variables in the dataset. Each correspond to a different
type of descriptive measure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Central Tendency: Where do the data points tend to be located?
\item
  Dispersion: How spread out are the data points?
\item
  Distribution: What is the overall shape of of the data points?
\item
  Interdependence: How are these data points related to other data?
\end{enumerate}

\hypertarget{sec-aa-central-tendency}{%
\subsubsection{Central tendency}\label{sec-aa-central-tendency}}

The central tendency is measure which aims to summarize the data points
in a variable as the most representative, middle or most typical value.
There are three common measures of central tendency: the mode, mean and
median. Each differ in how they summarize the data points.

The \textbf{mode} is the value, or values, that appears most frequently
in a set of values. If there are multiple values with the highest
frequency, then the variable is said to be multimodal. The most
versatile of the central tendency measures as it can be applied to all
levels of measurement, although the mode is not often used for numeric
variables as it is not as informative as other measures.

The more common measures for numeric variables are the mean and the
median. The \textbf{mean} is a summary statistic calculated by summing
all the values and dividing by the number of values. The \textbf{median}
is calculated by sorting all the values in the variable and then
selecting the middle value. Given that the mean and median are
calculated differently, they will not always yield the same result.
Differences that appear between the mean and median will be of interest
to us later in this chapter.

\hypertarget{dispersion}{%
\subsubsection{Dispersion}\label{dispersion}}

The mean, median, and mode provide summary information where data points
tend to be located. However, they do not provide us with any
understanding as to how representative this value is. To provide this
context, the spread of the values around the central tendency, or
\textbf{dispersion}, is calculated.

For categorical variables, the spread is framed in terms of how balanced
the values are across the levels. One way to do this is to calculate the
(normalized) entropy. \textbf{Entropy} is a measure of uncertainty. The
more balanced the values are across the levels, the higher the entropy.
The less balanced the values are across the levels, the lower the
entropy. Normalized entropy scores range from 0 to 1, with 0 indicating
that all the values are the same and 1 indicating that all the values
are different.

The most common measure of dispersion for numeric variables is the
\textbf{standard deviation}. The standard deviation is calculated by
taking the square root of the variance. The \textbf{variance} is the
average of the squared differences from the mean. So, more succinctly,
the standard deviation is a measure of the spread of the values around
the mean. Where the standard deviation is anchored to the mean, the
\textbf{interquartile range} (IQR) is tied to the median. The median
represents the sorted middle of the values, in other words the 50th
percentile. The IQR is the difference between the 75th percentile and
the 25th percentile. Again, just as the mean and the median, the
standard deviation and the IQR are calculated in different ways, they
are not always the same.

Let's now consider the relevant central tendency and dispersion of the
variables in the BELC dataset in
Table~\ref{tbl-aa-belc-descriptive-stats}.

\begin{table}

\caption{\label{tbl-aa-belc-descriptive-stats}Central tendency and
dispersion of the variables in the BELC
dataset}\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-belc-descriptive-stats-1}Categorical variables }

{\centering 

\tabularnewline

\centering
\begin{tabular}{l|l|r}
\hline
variable & top\_counts & norm\_entropy\\
\hline
essay\_id & E1: 1, E10: 1, E11: 1, E12: 1 & 1.000\\
\hline
part\_id & L05: 3, L10: 3, L11: 3, L12: 3 & 0.983\\
\hline
sex & fem: 48, mal: 32 & 0.971\\
\hline
group & T1: 25, T3: 24, T2: 16, T4: 15 & 0.981\\
\hline
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-belc-descriptive-stats-2}Numeric variables }

{\centering 

\tabularnewline

\centering
\begin{tabular}{l|r|r|r|r}
\hline
variable & mean & median & sd & iqr\\
\hline
tokens & 67.62 & 56.50 & 44.20 & 61.25\\
\hline
types & 41.85 & 38.50 & 23.03 & 31.50\\
\hline
ttr & 0.68 & 0.66 & 0.13 & 0.15\\
\hline
prop\_l2 & 0.96 & 0.99 & 0.10 & 0.03\\
\hline
\end{tabular}

}

\end{minipage}%

\end{table}

In Table~\ref{tbl-aa-belc-descriptive-stats-1} we see the measures for
categorical variables. The \texttt{top\_counts} variable gives us a
short list of the most frequent levels of the variable. From
\texttt{top\_count} we can gather whether the variable has one mode or
is multimodel. Both \texttt{essay\_id} and \texttt{part\_id} have the
same most frequent value for the levels listed. On the other hand,
\texttt{sex} and \texttt{group} have a single mode. We can also
appreciate the dispersion of these variables based on the
\texttt{norm\_entropy} of each variable. \texttt{essay\_id} is
completely balanced across the levels, so it has a normalized entropy of
1. the other variables are not as balanced, but still quite balanced as
the normalized entropy is close to 1.

In Table~\ref{tbl-aa-belc-descriptive-stats-2} the numeric variables
have a column for the mean, median, standard deviation, and IQR for
each. The variable \texttt{tokens} has a larger difference between the
mean and median than the other variables and the standard deviation is
relatively large suggesting that the values are more spread out around
the mean. In the case of \texttt{ttr} the mean and median are quite
close and the standard deviation is relatively small suggesting that the
values are more tightly clustered around the mean.

When interpreting these summary values, it is important to only directly
compare column-wise. That is, focusing only on a single variable, not
across variables. Each variable, as is, is measured on a different scale
and only relative to itself can we make sense of the values.

However, we can transform the central tendency and dispersion scores for
numeric variables to make them more comparable by standardizing the
scale of the values. \textbf{Standardization} is a scale-based
transformation that changes the scale of the values to a common scale,
or \emph{z-scores}. It involves two separate transformations: centering
and scaling. \textbf{Centering} is a transformation that subtracts the
mean or median from each value. The result is a mean and median of zero.
\textbf{Scaling} is a transformation that divides each value by the
standard deviation or IQR.

In Table~\ref{tbl-aa-belc-descriptive-stats-standardized}, we see the
same summary statistics as in
Table~\ref{tbl-aa-belc-descriptive-stats-2}, but the values have been
standardized for the mean and standard deviation. The mean is now zero
and the standard deviation is one. This allows us to compare the median
and IQR of the variables more directly.

\hypertarget{tbl-aa-belc-descriptive-stats-standardized}{}
\begin{table}
\caption{\label{tbl-aa-belc-descriptive-stats-standardized}Standardized central tendency and dispersion of numeric variables }\tabularnewline

\centering
\begin{tabular}{l|r|r|r|r}
\hline
variable & mean & median & sd & iqr\\
\hline
tokens & 0 & -0.25 & 1 & 1.39\\
\hline
types & 0 & -0.15 & 1 & 1.37\\
\hline
ttr & 0 & -0.19 & 1 & 1.14\\
\hline
prop\_l2 & 0 & 0.25 & 1 & 0.27\\
\hline
\end{tabular}
\end{table}

One more caveat to keep in mind is that we need to be mindful of the
nature of the data being standardized and what the standardized values
mean. For example, the variables \texttt{tokens} and \texttt{types} were
originally counts. But the standardized values are not interpretable as
counts, they are now on a different scale --specifically a z-score
scale. In the same way since the \texttt{ttr} and \texttt{prop\_l2}
variables were originally proportions, the standardized values are also
not interpretable as proportions. One additional twist, however, is that
the original scales for these pairs of variables were not the same:
\texttt{tokens} and \texttt{types} were counts, but \texttt{ttr} and
\texttt{prop\_l2} were proportions. So, even though the standardized
values are on the same scale, they are not directly comparable.

Beyond comparing central tendency and dispersion across variables,
standarization is useful for analytic statistics to mitigate the
influence of variables with large values. In some cases, the statistical
method will require standardization of variables before analysis.

\hypertarget{distributions}{%
\subsubsection{Distributions}\label{distributions}}

Summary statistics of the central tendency and dispersion of a variable
provide a sense of the most representative value and how spread out the
data is around this value. However, to gain a more comprehensive
understanding of the variable, it is key to consider the frequencies of
all the data points. The \textbf{distribution} of a variable is the
pattern or shape of the data that emerges when the frequencies of all
data points are considered. This can reveal patterns that might not be
immediately apparent from summary statistics alone. Understanding the
frequency and distribution of data points is vital as it informs
subsequent choices of statistical analysis and evaluative methods,
ensuring they are appropriate for the specific characteristics of the
data.

When assessing the distribution of categorical variables, we can use a
frequency table or bar plot. A \textbf{frequency table} is a useful
method to display the frequency and proportion of each level in a
categorical variable in a clear and concise manner. In
Table~\ref{tbl-aa-belc-frequency-table} we see the frequency table for
the variable \texttt{sex}.

\hypertarget{tbl-aa-belc-frequency-table}{}
\begin{table}
\caption{\label{tbl-aa-belc-frequency-table}Frequency table for the variable \texttt{sex}. }\tabularnewline

\centering
\begin{tabular}{l|r|r}
\hline
sex & frequency & proportion\\
\hline
female & 48 & 0.6\\
\hline
male & 32 & 0.4\\
\hline
\end{tabular}
\end{table}

A \textbf{bar plot} is a type of plot where the x-axis is a categorical
variable and the y-axis is the frequency of the values. The frequency is
represented by the height of the bar. The variables can be ordered by
frequency, alphabetically, or some other order.
Figure~\ref{fig-aa-belc-barplots} is a bar chart for the variables
\texttt{sex}, \texttt{group}, and \texttt{part\_id}, ordered
alphabetically.

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-barplots-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-barplots-1}Sex}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-barplots-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-barplots-2}Time group}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-barplots-3.pdf}

}

}

\subcaption{\label{fig-aa-belc-barplots-3}Participant ID}
\end{minipage}%

\caption{\label{fig-aa-belc-barplots}Bar plots for categorical variables
\texttt{sex}, \texttt{group}, \texttt{part\_id} in the BELC dataset.}

\end{figure}

So for a frequency table or barplot, we can see the frequency of each
level of a categorical variable. This gives us some knowledge about the
BELC dataset: there are more girls in the dataset, more essays appear in
first and third time groups, and the number of essays written by each
participant is scattered from one to three. If we were to see any
clearly loopsided categories, this would be a sign of imbalance in the
data and we would need to consider how this might impact our analysis.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

The goal of descriptive statistics is to summarize the data in a way
that is meaningful and interpretable. With this in mind, compare the
frequency table in \ref{tbl-aa-belc-frequency-table} and bar plot in
\ref{fig-aa-belc-barplots-1}. Does one provide a more interpretable
summary of the data? Why or why not? Are there any other ways you might
communicate this distribution more effectively?

\end{tcolorbox}

For numeric variables, understanding the distribution is more complex,
and also more important. In essence, however, we are assessing two
things: the appearance of outliers in relation to and the overall shape
of the distribution.

Now, a frequency table, as in Table~\ref{tbl-aa-belc-frequency-table},
does not summarize the distribution of a numeric variable in a concise,
readily human-consumable format. Instead, the distribution of a numeric
variable is best understood visually.

The most common visualizations of the distribution of a numeric variable
are histograms and density plots. \textbf{Histograms} are a type of bar
plot where the x-axis is a numeric variable and the y-axis is the
frequency of the values falling within a determined range of values, or
bins. The frequency of values within each bin is represented by the
height of the bars. \textbf{Density plots} are a smoothed version of
histograms. The y-axis of a density plot is the probability of the
values. When frequent values appear closely together, the plot line is
higher. When the frequency of values is lower or more spread out, the
plot line is lower. An example of these plots is show in
Figure~\ref{fig-aa-belc-histogram-density-tokens} for the variable
\texttt{tokens}.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-tokens-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-density-tokens-1}Histogram}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-tokens-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-density-tokens-2}Density plot}
\end{minipage}%

\caption{\label{fig-aa-belc-histogram-density-tokens}Distribution plots
for the variable \texttt{tokens}.}

\end{figure}

Both the histogram in
Figure~\ref{fig-aa-belc-histogram-density-tokens-1} and the density plot
in Figure~\ref{fig-aa-belc-histogram-density-tokens-2} show the
distribution of the variable \texttt{tokens} in slightly different ways
which translate into trade-offs in terms of interpretability.

The histogram shows the frequency of the values in bins. The number of
bins and/ or binwidth can be changed for more or less granularity. A
rough grain histogram shows the general shape of the distribution, but
it is difficult to see the details of the distribution. A fine grain
histogram shows the details of the distribution, but it is difficult to
see the general shape of the distribution. The density plot shows the
general shape of the distribution, but it hides the details of the
distribution. Given this trade-off, it is often useful explore outliers
with histograms and the overall shape of the distribution with density
plots.

In Figure~\ref{fig-aa-belc-histograms} we see histograms for the
variables \texttt{tokens}, \texttt{types}, and \texttt{ttr}.

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-histograms-1}Number of tokens}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-histograms-2}Number of types}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histograms-3.pdf}

}

}

\subcaption{\label{fig-aa-belc-histograms-3}Type-token ratio score}
\end{minipage}%

\caption{\label{fig-aa-belc-histograms}Histograms for numeric variables
\texttt{tokens}, \texttt{types}, and \texttt{ttr}.}

\end{figure}

Focusing on the details captured in the histogram we are better able to
detect potential outliers. Outliers can reflect valid values that are
simply extreme or they can reflect something erroneous in the data. To
distinguish between these two possibilities, it is important to know the
context of the data. Take, for example,
Figure~\ref{fig-aa-belc-histograms-3}. We see that there is a bin near
the value 1.0. Given that the type-token ratio is a ratio of the number
of types to the number of tokens, it is unlikely that the type-token
ratio would be exactly 1.0 as this would mean that every word in an
essay is unique. Another, less dramatic, example is the bin to the far
right of Figure~\ref{fig-aa-belc-histograms-1}. In this case, the bin
represents the number of tokens in an essay. An uptick in the number of
essays with a large number of tokens is not surprising and would not
typically be considered an outlier. On the other hand, consider the bin
near the value 0 in the same plot. It is unlikely that a true essay
would have 0, or near 0, words and therefore a closer look at the data
is warranted.

It is important to recognize that outliers contribute undue influence to
overall measures of central tendency and dispersion. To appreciate this,
let's consider another helpful visualization called a \textbf{boxplot}.
A boxplot is a visual representation which aims to represent the central
tendency, dispersion, and distribution of a numeric variable in one
plot.

\begin{figure}[H]

{\centering \includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-1.pdf}

}

\caption{\label{fig-aa-belc-boxplot}Boxplot for the variable
\texttt{ttr}.}

\end{figure}

In Figure~\ref{fig-aa-belc-boxplot} we see a boxplot for \texttt{ttr}
variable. The box in the middle of the plot represents the interquartile
range (IQR) which is the range of values between the first quartile and
the third quartile. The solid line in the middle of the box represents
the median. The lines extending from the box are called `whiskers' and
provide the range of values which are within 1.5 times the IQR. Values
outside of this range are plotted as individual points.

Now let's consider boxplots from another angle. In
Figure~\ref{fig-aa-belc-histogram-boxplot-comparison-2} I've plotted the
boxplot horizontally, right below the histogram in
Figure~\ref{fig-aa-belc-histogram-boxplot-comparison-1}. In this view,
we can see that a boxplot is a simplifed histogram augmented with
central tendency and dispersion statistics. While histograms focus on
the frequency distribution of data points, boxplots focus on the data's
quartiles and potential outliers.

\begin{figure}

\begin{minipage}[t]{\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-boxplot-comparison-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-boxplot-comparison-1}Histogram}
\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-boxplot-comparison-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-boxplot-comparison-2}Boxplot
(horizontal)}
\end{minipage}%

\caption{\label{fig-aa-belc-histogram-boxplot-comparison}Histogram and
boxplot for the variable \texttt{ttr}.}

\end{figure}

I've added a dashed line in
Figure~\ref{fig-aa-belc-histogram-boxplot-comparison-1} and
Figure~\ref{fig-aa-belc-histogram-boxplot-comparison-2} to signal the
mean in this set of plots, but it is not typically included. I include
the dashed line to make a point: the mean is more sensitive to outliers
than the median. As I pointed out in
Section~\ref{sec-aa-central-tendency}, the mean is the sum of all values
divided by the number of values. If there are extreme values, the mean
will be pulled in the direction of the extreme values. The median,
however, is the middle value and a few extreme values have less effect.
So, when central tendency is reported, if there is a sizeable difference
between the mean and the median, measures of dispersion will be larger
and the direction of the difference can be used to infer the presence of
outliers.

Returning to outliers, it is important to address them to safeguard the
accuracy of the analysis. There are two main ways to address outliers:
1) transform the data and 2) eliminate observations with outliers
(\textbf{trimming}). Trimming is more extreme as it removes data but can
be the best approach for true outliers. Transforming the data is an
approach to mitigating the influence of extreme but valid values.
\textbf{Transformation} involves applying a mathematical function to the
data which changes the scale and/ or shape of the distribution, but does
not remove data nor does it change the relative order of the values.

In Figure~\ref{fig-aa-belc-histogram-density-trimmed}, we see two
boxplots. Figure~\ref{fig-aa-belc-boxplot-trimmed-1} is the original
\texttt{ttr} data and Figure~\ref{fig-aa-belc-boxplot-trimmed-2}
reflects the data trimmed to remove outliers. In this case, we have
removed essays with a type-token ratio of 1.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-trimmed-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-boxplot-trimmed-1}Type-token ratio score}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-trimmed-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-boxplot-trimmed-2}Type-token ratio score
(trimmed)}
\end{minipage}%

\caption{\label{fig-aa-belc-boxplot-trimmed}Boxplots for \texttt{ttr}
before and after trimming.}

\end{figure}

We can now appreciate the relatively larger effect that the outliers had
on the mean value of the \texttt{ttr} variable. As outliers are removed
as the difference between the mean and median will become smaller.

The exploration the data points with histograms and boxplots has helped
us to identify outliers. Now we turn to the question of the overall
shape of the distribution. The key question is whether the observed
distribution of each variable approximates the Normal Distribution, or
not.

The \textbf{Normal Distribution} is a theoretical distribution where the
values are symmetrically dispersed around the central tendency (mean/
median). In terms we can now understand, this means that the mean and
median are the same. The Normal Distribution is important because many
statistical tests assume that the data distribution is normal or near
normal.

Stepping away from our BELC dataset, I've created simulated data that
fit normal and non-normal, or skewed, distributions. I present each of
these distributions as density plots with mean and median line overlays
in Figure~\ref{fig-aa-distributions}.

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-1.pdf}

}

}

\subcaption{\label{fig-aa-distributions-1}Left skewed distribution}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-2.pdf}

}

}

\subcaption{\label{fig-aa-distributions-2}Normal distribution}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-distributions-3.pdf}

}

}

\subcaption{\label{fig-aa-distributions-3}Right skewed distribution}
\end{minipage}%

\caption{\label{fig-aa-distributions}Mean and median for normal and
skewed distributions.}

\end{figure}

A \textbf{Normal Distribution}, illustrated in
Figure~\ref{fig-aa-distributions-2}, is a distribution where the values
are symmetrically dispersed around the central tendency (mean/ median).
This means that in a theoretical distribution that the mean and median
are the same. The Normal Distribution is also known as the
\textbf{Gaussian Distribution} or the \textbf{Bell Curve}, for the
hallmark bell shape of the distribution. In this distribution, extreme
values are less likely than values near the center.

A \textbf{skewed distribution} is not a specific type of distribution
but rather a characteristic than many distributions can exhibit where
the values are not symmetrically dispersed around the central tendency.
A distribution in which values tend to disperse to the left of the
central tendency is \textbf{left skewed} as in
Figure~\ref{fig-aa-distributions-1} and dispersion to the right is
\textbf{right skewed} as in Figure~\ref{fig-aa-distributions-3}.

Data that are normally, or near-normally distributed are often analyzed
using parametric tests while data that exhibit a skewed distributed are
often analyzed using non-parametric tests. Divergence from normality is
not a binary distinction. Rather, it is a matter of degree. A visual
inspection is usually sufficient for experienced researchers to
determine whether a distribution is normal or skewed. However, for those
who are less experienced or if you want to be more precise, there are
two primary measures which can help ascertain the degree to which a
distribution is normal: skewness and kurtosis. \textbf{Skewness} is a
measure of the degree to which a distribution is asymmetrical.
\textbf{Kurtosis} is a measure of the degree to which a distribution is
peaked.

In Table~\ref{tbl-aa-skewness-kurtosis} I provide the skewness and
kurtosis scores for our simulated distributions along with central
tendency measures for context.

\hypertarget{tbl-aa-skewness-kurtosis}{}
\begin{table}
\caption{\label{tbl-aa-skewness-kurtosis}Skewness and kurtosis for normal and skewed distributions. }\tabularnewline

\centering
\begin{tabular}{l|r|r|l|r|r}
\hline
distribution & mean & median & histogram & skewness & kurtosis\\
\hline
Left skew & 0.746 & 0.767 &  & -0.711 & 3.27\\
\hline
Normal & 0.016 & 0.009 &  & 0.065 & 2.93\\
\hline
Right skew & 0.254 & 0.233 &  & 0.711 & 3.27\\
\hline
\end{tabular}
\end{table}

All things distribution are matters of degree, so there are no hard and
fast rules for determining whether a distribution is normal or skewed.
However, there are some general guidelines that can be used to determine
the degree to which a distribution is normal or skewed, as shown in
Table~\ref{tbl-aa-skewness-kurtosis}.

\begin{table}

\caption{\label{tbl-aa-skewness-kurtosis}Rules of thumb for skewness and
kurtosis scores.}\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-skewness}Skewness scores}

{\centering 

\begin{tabular}[t]{ll}
\toprule
Score Range & Evaluation\\
\midrule
-0.5 to 0.5 & Approximately symmetric\\
-1 to -0.5 or 0.5 to 1 & Moderately skewed\\
\textless{} -1 or \textgreater{} 1 & Highly skewed\\
\bottomrule
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-kurtosis}Kurtosis scores}

{\centering 

\begin{tabular}[t]{ll}
\toprule
Score Range & Evaluation\\
\midrule
\textless{} 3 & Less peaked than normal\\
Equal to 3 & Normal peak\\
\textgreater{} 3 & More peaked than normal\\
\bottomrule
\end{tabular}

}

\end{minipage}%

\end{table}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

Another approach for visually summarizing a single numeric variable is
the Empirical Cumulative Distribution Function, or \emph{ECDF}. An ECDF
plot is a summary of the cummulative proportion of each of the values of
a numeric variable. In addition to providing insight into the
distribution of a variable, ECDF plots can be useful in determing what
proportion of the values fall above or below a certain percentage of the
data.

\end{tcolorbox}

The question is which type of distribution does each numeric variable in
the BELC dataset fit? Comparing the variables \texttt{ttr},
\texttt{types} and \texttt{prop\_l2} in
Figure~\ref{fig-aa-belc-histogram-density-trimmed} to the three
distributions in Figure~\ref{fig-aa-distributions}, we see that all
three numeric variables in the BELC dataset are skewed to some degree.

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-1}Type-token
ratio score}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-2}Number of
types}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-histogram-density-trimmed-3.pdf}

}

}

\subcaption{\label{fig-aa-belc-histogram-density-trimmed-3}Proportion of
L2 words}
\end{minipage}%

\caption{\label{fig-aa-belc-histogram-density-trimmed}Histogram/ Density
plots for numeric variables in the BELC dataset.}

\end{figure}

Figure~\ref{fig-aa-belc-histogram-density-trimmed-1} for \texttt{ttr}
has some right skewing but not as much as \texttt{types} in
Figure~\ref{fig-aa-belc-histogram-density-trimmed-2}. \texttt{prop\_l2}
in Figure~\ref{fig-aa-belc-histogram-density-trimmed-3} is the most
skewed of the three variables. As mentioned earlier, skewed
distributions can take many forms, some are more skewed than others.

To view statistics on our three variables in
Figure~\ref{fig-aa-belc-histogram-density-trimmed}, we can calculate the
skewness and kurtosis.

\hypertarget{tbl-aa-belc-skewness-kurtosis}{}
\begin{table}
\caption{\label{tbl-aa-belc-skewness-kurtosis}Skewness and kurtosis for numeric variables in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|r|r|l|r|r}
\hline
distribution & mean & median & histogram & skewness & kurtosis\\
\hline
ttr & 0.655 & 0.648 &  & 0.319 & 2.90\\
\hline
types & 46.044 & 46.000 &  & 0.407 & 2.45\\
\hline
tokens & 75.338 & 77.000 &  & 0.669 & 2.98\\
\hline
prop\_l2 & 0.986 & 0.990 &  & -1.273 & 4.13\\
\hline
\end{tabular}
\end{table}

Given the characteristics of the numeric variables in the BELC dataset,
although none of them are perfectly normal, but only \texttt{prop\_l2}
is highly skewed. Therefore, if we intend to use these variables `as-is'
in statistical measures or tests, we now know whether to choose
parametric or non-parametric alternatives.

In the case that a variable is highly skewed, it is often useful to
attempt transform the variable to reduce the skewness. In contrast to
scale-based transformations (\emph{e.g.} centering and scaling),
shape-based transformations change the scale and the shape of the
distribution. The most common shape-based transformation is the
logarithmic transformation. The \textbf{logarithmic transformation}
(log-transformation) takes the log (typically base 10) of each value in
a variable. The log-transformation is useful for reducing the skewness
of a variable as it compresses large values and expands small values. If
the skewness is due to these factors, the log-transformation can help.

It is important to note, however, that if scale-based transformations
are to be applied to a variable, they should be applied after the
log-transformation as the log of negative values is undefined.

\hypertarget{interdependence}{%
\subsubsection{Interdependence}\label{interdependence}}

We have covered the first three of the four questions we are interested
in asking in a descriptive analysis. The fourth, and last, question is
whether there is mutual dependence between variables. If so, what is the
directionality and how strong is the dependence? Knowing the answers to
these questions will help frame our approach to analysis.

To assess interdependence, the number and information types of the
variables under consideration are important. Let's start by considering
two variables. If we are working with two variables, we are dealing with
a \textbf{bivariate} relationship. Given there are three informational
types (categorical, ordinal, and numeric), there are six logical
bivariate combinations: categorical-categorical, categorical-ordinal,
categorical-numeric, ordinal-ordinal, ordinal-numeric, and
numeric-numeric.

The directionality of a relationship will take the form of a tabular or
graphic summary depending on the informational value of the variables
involved. In Table~\ref{tbl-aa-summary-types}, we see the appropriate
summary types for each of the six bivariate combinations.

\hypertarget{tbl-aa-summary-types}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2200}}@{}}
\caption{\label{tbl-aa-summary-types}Appropriate summary types for
different combinations of variable types.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Categorical} & Contingency table & Contingency table/ Bar plot &
Pivot table/ Boxplot \\
\textbf{Ordinal} & - & Contingency table/ Bar plot & Pivot table/
Boxplot \\
\textbf{Numeric} & - & - & Scatterplot \\
\end{longtable}

Let's first start with the combinations that include a categorical or
ordinal variable. Categorical and ordinal variables reflect measures of
class-type information, with add meaningful ranks to ordinal variables.
To assess a relationship with these variable types, a table is always a
good place to start. When combined together, a contingency table is the
appropriate table. A \textbf{contingency table} is a cross-tabulation of
two class-type variables, basically a two-way frequency table. This
means that three of the six bivariate combinations are assessed with a
contingency table: categorical-categorical, categorical-ordinal, and
ordinal-ordinal.

In Table~\ref{tbl-aa-belc-contingency-tables} we see contingency tables
for the categorical variable \texttt{sex} and ordinal variable
\texttt{group} in the BELC dataset.

\begin{table}

\caption{\label{tbl-aa-belc-contingency-tables}Contingency tables for
categorical variable \texttt{sex} and ordinal variable \texttt{group} in
the BELC dataset.}\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-belc-contingency-tables-1}Counts }

{\centering 

\tabularnewline

\centering
\begin{tabular}{l|r|r|r}
\hline
group & female & male & Total\\
\hline
T1 & 7 & 9 & 16\\
\hline
T2 & 11 & 4 & 15\\
\hline
T3 & 13 & 10 & 23\\
\hline
T4 & 9 & 5 & 14\\
\hline
Total & 40 & 28 & 68\\
\hline
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}
\subcaption{\label{tbl-aa-belc-contingency-tables-2}Percentages }

{\centering 

\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
group & female & male & Total\\
\hline
T1 & 43.75\% & 56.25\% & 100.00\%\\
\hline
T2 & 73.33\% & 26.67\% & 100.00\%\\
\hline
T3 & 56.52\% & 43.48\% & 100.00\%\\
\hline
T4 & 64.29\% & 35.71\% & 100.00\%\\
\hline
Total & 58.82\% & 41.18\% & 100.00\%\\
\hline
\end{tabular}

}

\end{minipage}%

\end{table}

A contingency table may include only counts, as in
Table~\ref{tbl-aa-belc-contingency-tables-1}, or may include proportions
or percentages in an effort to normalize the counts and make them more
comparable, as in Table~\ref{tbl-aa-belc-contingency-tables-2}.

It is sometimes helpful to visualize a contingency table as a bar plot
when there are a larger number of levels in either or both of the
variables. Again, looking at the relationship between \texttt{sex} and
\texttt{group}, we see that we can plot the counts or the proportions.
In Figure~\ref{fig-aa-belc-bar-plots}, we see both.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-bar-plots-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-bar-plots-1}Counts}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-bar-plots-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-bar-plots-2}Proportions}
\end{minipage}%

\caption{\label{fig-aa-belc-bar-plots}Bar plots for the relationship
between \texttt{sex} and \texttt{group} in the BELC dataset.}

\end{figure}

To summarize and assess the relationship between a categorical or an
ordinal variable and a numeric variable, we cannot use a contingency
table. Instead, this type of relationship is best summarized in a table
using a summary statistic in a \textbf{pivot table}. A pivot table is a
table in which a class-type variable is used to group a numeric variable
by some summary statistic appropriate for numeric variables, \emph{e.g.}
mean, median, standard deviation, \emph{etc.}

In Table~\ref{tbl-aa-belc-pivot-table}, we see a pivot table for the
relationship between \texttt{group} and \texttt{tokens} in the BELC
dataset. Specifically, we see the mean number of tokens by group.

\hypertarget{tbl-aa-belc-pivot-table}{}
\begin{table}
\caption{\label{tbl-aa-belc-pivot-table}Pivot table for the relationship between \texttt{group} and
\texttt{tokens} in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|r}
\hline
group & mean\_tokens\\
\hline
T1 & 35.4\\
\hline
T2 & 62.5\\
\hline
T3 & 85.0\\
\hline
T4 & 118.9\\
\hline
\end{tabular}
\end{table}

We see that the mean number of tokens increases from Group T1 to T4,
which is consistent with the idea that the students in the higher groups
are writing longer essays.

Although a pivot table may be appropriate for targeted numeric
summaries, a visualization is often more informative for assessing the
dispersion and distribution of a numeric variable by a categorical or
ordinal variable. There are two main types of visualizations for this
type of relationship: a boxplot and a \textbf{violin plot}. A violin
plot is a visualization that summarizes the distribution of a numeric
variable by a categorical or ordinal variable, adding the overall shape
of the distribution, much as a density plot does for histograms.

In Figure~\ref{fig-aa-belc-boxplot-violin-plot}, we see both a boxplot
and a violin plot for the relationship between \texttt{group} and
\texttt{tokens} in the BELC dataset.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-violin-plot-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-boxplot-violin-plot-1}Boxplot}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-boxplot-violin-plot-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-boxplot-violin-plot-2}Violin plot}
\end{minipage}%

\caption{\label{fig-aa-belc-boxplot-violin-plot}Boxplot and violin plot
for the relationship between \texttt{group} and \texttt{tokens} in the
BELC dataset.}

\end{figure}

From the boxplot in Figure~\ref{fig-aa-belc-boxplot-violin-plot-1}, we
see that the general trend towards more tokens used by students in
higher groups. But we can also appreciate the dispersion of the data
within each group looking at the boxes and whiskers. On the surface it
appears that the data for groups T1 and T3 are closer to each other than
groups T2 and T4, in which there is more variability within these
groups. Furthermore, we can see outliers in groups T1 and T3, but not in
groups T2 and T4. From the violin plot in
Figure~\ref{fig-aa-belc-boxplot-violin-plot-2}, we can see the same
information, but we can also see the overall shape of the distribution
of tokens within each group. In this plot, it is very clear that group
T4 includes a wide range of token counts.

The last bivariate combination is numeric-numeric. To summarize this
type of relationship a scatterplot is used. A \textbf{scatterplot} is a
visualization that plots each data point as a point in a two-dimensional
space, with one numeric variable on the x-axis and the other numeric
variable on the y-axis. Depending on the type of relationship you are
trying to assess, you may want to add a trend line to the scatterplot. A
trend line is a line that summarizes the overall trend in the
relationship between the two numeric variables. To assess the extent to
which the relationship is linear, a straight line is drawn which
minimizes the distance between the line and the points.

In Figure~\ref{fig-aa-belc-scatter-plot}, we see a scatterplot and a
scatterplot with a trend line for the relationship between \texttt{ttr}
and \texttt{types} in the BELC dataset.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-scatter-plot-1.pdf}

}

}

\subcaption{\label{fig-aa-belc-scatter-plot-1}Points}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-belc-scatter-plot-2.pdf}

}

}

\subcaption{\label{fig-aa-belc-scatter-plot-2}Points with a linear trend
line}
\end{minipage}%

\caption{\label{fig-aa-belc-scatter-plot}Scatter plot for the
relationship between \texttt{ttr} and \texttt{types} in the BELC
dataset.}

\end{figure}

We see that there is an apparent positive relationship between these two
variables, which is consistent with the idea that as the number of types
increases, the type-token ratio increases. In other words, as the number
of unique words increases, so does the lexical diversity of the text.
Since we are evaluating a linear relationship, we are assessing the
extent to which there is a \textbf{correlation} between \texttt{ttr} and
\texttt{types}. A correlation simply means that as the values of one
variable change, the values of the other variable change in a consistent
manner.

Once a sense of the directionality of a relationship can be established,
the next step is to gauge the relative strength, or association.
\textbf{Association} refers to any relationship in which there is a
dependency between two variables. Quantitative measures of association,
in combination with tabular and visual summaries, can provide a more
complete picture of the relationship between two variables.

There are a number of measures of assocation, depending on the types of
variables being assessed and, for numeric variables, whether the
distribution is normal (parametric) or non-normal (non-parametric), as
seen in Table~\ref{tbl-aa-association-measures}.

\hypertarget{tbl-aa-association-measures}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\label{tbl-aa-association-measures}Measures of association or
correlation strength for different combinations of variable
types.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric\emph{Non-parametric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric\emph{Parametric}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categorical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Ordinal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric\emph{Non-parametric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Numeric\emph{Parametric}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Categorical} & Chi-square \(\chi^2\), Cramr's \(V\) & Goodman
and Kruskal's \(\gamma\) & Rank biserial Correlation & Point-biseral
Correlation \\
\textbf{Ordinal} & - & Kendall's \(\tau\) & Kendall's \(\tau\) &
Pearson's \(r\) \\
\textbf{Numeric}\emph{Non-parametric} & - & - & Kendall's \(\tau\) &
Pearson's \(r\) \\
\textbf{Numeric}\emph{Parametric} & - & - & - & Pearson's \(r\) \\
\end{longtable}

Association measures often are expressed as a number between -1 and 1,
where 0 indicates no association, -1 indicates a perfect negative
association, and 1 indicates a perfect positive association. The closer
the number is to 0, the weaker the association. The closer the number is
to -1 or 1, the stronger the association. Association statistics are
often accompanied by a \textbf{confidence interval} (CI), which is a
range of values that is likely to contain the true value of the
association in the population. The confidence interval is expressed as a
percentage, such as 95\%, which means that if we were to repeat the
study 100 times, 95 of those studies would produce a confidence interval
that contains the true value of the association in the population. If
the range between the lower and higher bounds of the confidence interval
contains 0, then the association is likely no different than chance.

Given these measures and interpretations, let's consider the different
types of bivariate relationships we have seen so far in the BELC
dataset. The first interdependence we explored involved the categorical
variable \texttt{sex} and the ordinal variable \texttt{group}. This
relationship may not be of primary interest to a study on L2 writing,
but it is a good example of how to assess the strength of an association
between a categorical and ordinal variable. Furthermore, it could be the
case that we want to assess whether we have widely unbalanced female/
male proportions in our time groups.

Using Table~\ref{tbl-aa-association-measures}, we see that we can use
Goodman and Kruskal's \(\gamma\) (gamma) to assess the strength of the
association between these two variables. The measures of association in
Table~\ref{tbl-aa-gamma} suggest that the proportion of male
participants is higher in group T1 and lower in group T2. However, these
associations are moderately strong, as the gamma value is near \(\pm\)
0.4.

\hypertarget{tbl-aa-gamma}{}
\begin{table}
\caption{\label{tbl-aa-gamma}Gamma for the relationship between \texttt{sex} and \texttt{group} in
the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|r|r|r|r}
\hline
Parameter1 & Parameter2 & r & CI & CI\_low & CI\_high\\
\hline
sex.female & group.T1 & -0.381 & 0.95 & -0.568 & -0.157\\
\hline
sex.female & group.T2 & 0.389 & 0.95 & 0.167 & 0.575\\
\hline
sex.male & group.T1 & 0.381 & 0.95 & 0.157 & 0.568\\
\hline
sex.male & group.T2 & -0.389 & 0.95 & -0.575 & -0.167\\
\hline
\end{tabular}
\end{table}

When paired with Figure~\ref{fig-aa-belc-bar-plots} we can appreciate
that groups T1 and T2 have contrasting proportions of females to males
and that groups T3 and T4 are more closely proportioned. This
observation should be considered when approaching statistical analyses
in which categorical variables required (near) equal proportions of
categories.

Now let's take a look at a more interesting relationship, the one
between the ordinal variable \texttt{group} and the numeric variable
\texttt{tokens}. Since we determined that \texttt{tokens} was near
normally distributed, we can choose the parametric version of our
association measure, Pearson's \(r\). The measures of association in
Table~\ref{tbl-aa-pearson} suggest that there is a negative association
between group T1 and a positive one betwen group T4 and \texttt{tokens},
which is consistent with the idea that as the group number increases,
the number of tokens increases. These associations are moderate to
strong, as the Pearson's \(r\) values are near \(\pm\) 0.5. However, the
other groups (T2 and T3) have very weak assocations with \texttt{tokens}
and the CI includes 0, which means that the association is likely no
different than chance.

\hypertarget{tbl-aa-pearson}{}
\begin{table}
\caption{\label{tbl-aa-pearson}Pearson's r for the relationship between \texttt{group} and
\texttt{tokens} in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|r|r|r|r}
\hline
Parameter1 & Parameter2 & r & CI & CI\_low & CI\_high\\
\hline
group.T1 & tokens & -0.520 & 0.95 & -0.675 & -0.322\\
\hline
group.T2 & tokens & -0.160 & 0.95 & -0.384 & 0.082\\
\hline
group.T3 & tokens & 0.161 & 0.95 & -0.080 & 0.385\\
\hline
group.T4 & tokens & 0.521 & 0.95 & 0.322 & 0.675\\
\hline
\end{tabular}
\end{table}

These association measures suggest that there is a relationship between
\texttt{group} and \texttt{tokens}, but that the relationship is not the
same for all groups. This may due to a number of factors, such as the
number of participants in each group, the effect of outliers within
particular levels, etc. or may simply underscore that the relationship
between \texttt{group} and \texttt{tokens} is not linear. What we do
with this information will depend on our research aims. Whatever the
case, we can use these measures to inform our next steps, as we will see
in the next section.

Finally, let's look at the relationship between the numeric variables
\texttt{ttr} and \texttt{types}. Since we determined both \texttt{ttr}
and \texttt{types} are normally distributed, we can choose the
parametric version of our association measure, Pearson's \(r\). The
measures of association in Table~\ref{tbl-aa-pearson-2} suggest that
there is a negative association between \texttt{ttr} and \texttt{types},
which is consistent with the idea that as the number of types increases,
the type-token ratio decreases. This association is strong, as Pearson's
\(r\) value is near 0.6.

\hypertarget{tbl-aa-pearson-2}{}
\begin{table}
\caption{\label{tbl-aa-pearson-2}Pearson's r for the relationship between \texttt{ttr} and \texttt{types}
in the BELC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|r|r|r|r}
\hline
Parameter1 & Parameter2 & r & CI & CI\_low & CI\_high\\
\hline
ttr & types & -0.606 & 0.95 & -0.738 & -0.43\\
\hline
\end{tabular}
\end{table}

Before moving on to the next section, it is important to remember than
through the process of diagnostic measures, we gain a thorough
understanding of our data's characteristics and quality, preparing us
for the next step in our analysis. However, remember that these measures
do not exist in isolation. The decisions we make at this stage, from
handling missing data to understanding the distribution of our
variables, can have significant implications on our subsequent analysis.
So, this initial step of data analysis deserves our careful attention
and scrutiny.

\hypertarget{sec-aa-analyze}{%
\section{Analyze}\label{sec-aa-analyze}}

Having ensured that our dataset is clean, valid, and thoroughly
understood, we can proceed to the next key stage of our data analysis
process - employing analytic methods. The goal of analysis, generally,
is to generate knowledge from information. The type of knowledge
generated and the process by which it is generated, however, differ and
can be broadly grouped into three analysis types: exploratory,
predictive, and inferential.

In this section I will provide an overview of how each of these analysis
types are tied to research aims and how the general purpose of each type
affect: (1) how to \emph{identify} the variables of interest, (2) how to
\emph{interrogate} these variables, and (3) how to \emph{interpret} the
results. I will structure the discussion of these analysis types moving
from the least structured (inductive) to most structured (deductive)
approach to deriving knowledge from information with the aim to provide
enough information for you to identify these research approaches in the
literature and to make appropriate decisions as to which approach your
research should adopt.

\hypertarget{sec-aa-explore}{%
\subsection{Explore}\label{sec-aa-explore}}

In \textbf{Exploratory Data Analysis (EDA)}, we use a variety of methods
to identify patterns, trends, and relations within and between
variables. The goal of EDA is uncover insights in an inductive,
data-driven manner. That is to say, that we do not enter into EDA with a
fixed hypothesis in mind, but rather we explore intuition, probe
anecdote, and follow hunches to identify patterns and relationships and
to evaluate whether and why they are meaningful. We are admittedly
treading new or unfamiliar terrain letting the data guide our analysis.
This means that we can use and reuse the same data to explore different
angles and approaches adjusting our methods and measures as we go. In
this way, EDA is an iterative, meaning generating process.

In line with the investigative nature of EDA, the identification of
variables of interest is a discovery process. We most likely have a
intution about the variables we would like to explore, but we are able
to adjust our variables as need be to suit our research aims. When the
identification and selection of variables is open, the process is known
as \textbf{feature engineering}. A process that is much an art as a
science, feature engineering leverages a mixture of relevant domain
knowledge, intuition, and trial and error to identify features that
serve to best represent the data and to best serve the research aims.
Furthermore, the roles of features in EDA are fluid --no variable has a
special status, as seen in Figure~\ref{fig-eda-variables}. We will see
that in other types of analysis, some or all the roles of the variables
are fixed.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/aa-eda-variables.drawio.png}

}

\caption{\label{fig-eda-variables}Roles of variables in EDA.}

\end{figure}

For illustrative purposes let's consider the State of the Union Corpus
(SOTU) (\protect\hyperlink{ref-R-quanteda.corpora}{Benoit 2020}). The
presidential addresses and a set of metadata variables are included in
the corpus. I've subsetted this corpus to only include U.S. presidents
since 1946. A tabular preview of the first 10 addresses (truncated for
display) can be found in Table~\ref{tbl-eda-sotu-corpus}.

\hypertarget{tbl-eda-sotu-corpus}{}
\begin{table}
\caption{\label{tbl-eda-sotu-corpus}First ten addresses from the SOTU Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l}
\hline
president & date & delivery & party & addresses\\
\hline
Truman & 1946-01-21 & written & Democratic & To the Congress of the United States: A quarter...\\
\hline
Truman & 1947-01-06 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1948-01-07 & spoken & Democratic & Mr. President, Mr. Speaker, and Members of the ...\\
\hline
Truman & 1949-01-05 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1950-01-04 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1951-01-08 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1952-01-09 & spoken & Democratic & Mr. President, Mr. Speaker, Members of the Cong...\\
\hline
Truman & 1953-01-07 & written & Democratic & To the Congress of the United States: I have th...\\
\hline
Eisenhower & 1953-02-02 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
Eisenhower & 1954-01-07 & spoken & Republican & Mr. President, Mr. Speaker, Members of the Eigh...\\
\hline
\end{tabular}
\end{table}

A dataset such as this one could serve as a starting point to explore
many different types of research questions. In order to maintain
research coherence so our efforts to not careen into a free-for-all, we
need to tether our feature engineering to a unit of analysis that is
relevant to the research question. A \textbf{unit of analysis} is the
entity that we are interested in studying. Not to be confused with the
unit of observation, which is the entity that we are able to observe and
measure (\protect\hyperlink{ref-Sedgwick2015}{Sedgwick 2015}).

To demonstrate the distinction, let's look consider different approaches
to analyzing the SOTU dataset. For example, the unit of analysis could
be the language of particular presidents, party ideology, or political
rhetoric in general and the unit of observation could be individual
words, phrases, sentences, etc. In some cases the unit of analysis and
the unit of observation are the same. For example, if we were interested
in potential changes use of the word ``terrorist'' over time in SOTU
addresses, the unit of analysis and the unit of observation would be the
same --individual addresses. So, depending on the perspective we are
interested in investigating, the choice of how to approach engineering
features to gain insight will vary.

By the same token, approaches for interrogating the dataset can differ
significantly, between research projects and within the same project,
but for instructive purposes, let's draw a distinction between
descriptive methods and unsupervised learning methods, as seen in
Table~\ref{tbl-eda-methods}.

\hypertarget{tbl-eda-methods}{}
\begin{table}
\caption{\label{tbl-eda-methods}Some common EDA methods }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Descriptive methods & Unsupervised learning methods\\
\hline
Frequency analysis & Cluster analysis\\
\hline
Keyness analysis & Topic Modeling\\
\hline
Co-occurence analysis & Vector Space Models\\
\hline
\end{tabular}
\end{table}

The first group, \textbf{descriptive methods} can be seen as a (more
robust) extenstion of the descriptive statistics covered earlier in this
chapter including statistic, tabular, and visual techniques. For
example, a frequency analysis of the SOTU dataset could be used to
identify the most common words used by U.S. political parties in their
addresses, in Figure~\ref{fig-aa-eda-sotu-descriptive-1}, or a
co-occurence analysis could be used to identify the most common words
the appear after the term ``free'', in
Figure~\ref{fig-aa-eda-sotu-descriptive-2}, in the dataset.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-eda-sotu-descriptive-1.pdf}

}

}

\subcaption{\label{fig-aa-eda-sotu-descriptive-1}Frequency analysis of
the 20 most frequent terms by party.}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-eda-sotu-descriptive-2.pdf}

}

}

\subcaption{\label{fig-aa-eda-sotu-descriptive-2}Co-occurence analysis
of the terms that appear after the term `free'.}
\end{minipage}%

\caption{\label{fig-aa-eda-sotu-descriptive}Example of descriptive
methods applied to the SOTU dataset.}

\end{figure}

The second group, \textbf{unsupervised learning}, is a subtype of
machine learning in which an algorithm is used to find patterns within
and between variables in the data without any guidance (supervision). In
this way, the algorithm, or machine learner, is left to make connections
and associations wherever they may appear in the input data. If we were
interested in finding word-use continuities and discontinuities between
presidents, we could use a clustering algorithm, seen in
Figure~\ref{fig-aa-eda-sotu-unsupervised-1}. Or if we wanted to uncover
themes \ldots{} \faIcon{wrench} {[}ADD: modify plot{]} we could use a
vector space model, as in Figure~\ref{fig-aa-eda-sotu-unsupervised-2}.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-eda-sotu-unsupervised-1.pdf}

}

}

\subcaption{\label{fig-aa-eda-sotu-unsupervised-1}Hierarchical
clustering of the SOTU corpus.}
\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{approaching-analysis_files/figure-pdf/fig-aa-eda-sotu-unsupervised-2.pdf}

}

}

\subcaption{\label{fig-aa-eda-sotu-unsupervised-2}Word embedding space
in the SOTU corpus.}
\end{minipage}%

\caption{\label{fig-aa-eda-sotu-unsupervised}Example of unsupervised
learning methods applied to the SOTU dataset.}

\end{figure}

Either through descriptive, unsupervised learning methods, or a
combination of both, EDA employs quantitative methods to summarize,
reduce, and sort complex datasets in order to provide the researcher
novel perspective to be qualitatively assessed. Exploratory methods
produce results that require associative thinking and pattern detection.
Speculative as they are, the results from exploratory methods can be
highly informative and lead to new insight and inspire further study in
directions that may not have been expected.

\hypertarget{sec-aa-predict}{%
\subsection{Predict}\label{sec-aa-predict}}

\textbf{Predictive Data Analysis (PDA)} employs a variety of techniques
to examine and evaluate the association strength between a variable or
set of variables, with a specific focus on predicting a target variable.
The aim of PDA is to construct models that can accurately forecast
future outcomes, using either data-driven or theory-driven approaches.
In this process, \textbf{supervised learning} methods, where the machine
learning algorithm is guided (supervised) by a target outcome variable,
are used. This means we don't begin PDA with a completely open-ended
exploration, but rather with an objective - accurate predictions.
However, the path to achieving this objective can be flexible, allowing
us freedom to adjust our models and methods. Unlike EDA, where the
entire dataset can be reused for different approaches, PDA requires a
portion of the data to be reserved for evaluation, enhancing the
validity of our predictive models. Thus, PDA is an iterative process
that combines the flexibility of exploratory analysis with the rigor of
confirmatory analysis.

There are two types of variables in PDA: the outcome variable and the
predictor variables, or features. The \textbf{outcome variable} is the
variable that the researcher is trying to predict. It is the only
variable that is necessarily fixed as part of the research question. The
features are the variables that are used to predict the outcome
variable. An overview of the roles of these variables in PDA is shown in
Figure~\ref{fig-pda-variables}.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/aa-pda-variables.drawio.png}

}

\caption{\label{fig-pda-variables}Roles of variables in PDA.}

\end{figure}

Feature selection can be either data-driven or theory-driven.
Data-driven features are those that are engineered to enhance predictive
power, while theory-driven features are those that are selected based on
theoretical relevance.

Let's consider the Europarl corpus of native, non-native and translated
texts (ENNTT) (\protect\hyperlink{ref-Nisioi2016}{Nisioi et al. 2016}).
This is a monolingual English corpus of translated and non-translated
texts from the European Parliament.

\hypertarget{tbl-enntt-dd}{}
\begin{table}
\caption{\label{tbl-enntt-dd}Data dictionary of the ENNTT corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
session\_id & Session ID & categorical & Unique identifier for each session\\
\hline
seq\_speaker\_id & Sequential Speaker ID & ordinal & Unique numeric identifier for each speaker\\
\hline
state & State & categorical & Country of the session speaker\\
\hline
language & Language & categorical & Original language in which the sentence was uttered\\
\hline
type & Type & categorical & Category of the speaker: natives, nonnatives, or translations\\
\hline
text & Text & categorical & Text spoken in the session\\
\hline
\end{tabular}
\end{table}

Now depending on our research question, we will have a different outcome
variable. If we want to examine the potential linguistic differences
between native and non-native speakers, we will select our outcome
variable to be the \texttt{type} (natives/ nonnatives). The features
selected to use to predict \texttt{type} depend on our research
question. If our research is guided by data, we will choose features
that are specifically designed to boost the ability to predict. On the
other hand, if our research is steered by theory, we will opt for
features that are chosen due to their theoretical significance. In
either case, the original dataset will likely need to be transformed.

The approach to interrogating the dataset includes three main steps:
feature engineering, model selection, and model evaluation. We've
discussed feature engineering, so what is model selection and model
evaluation? And how do we go about performing these steps?

\textbf{Model selection} is the process of choosing a machine learning
algorithm and set of features that produces the best prediction accuracy
for the outcome variable. To refine our approach such that we arrive at
the best combination of algorithm and features, we need to train our
machine learner on a variety of combinations and evaluate the accuracy
of each. We don't want to train and evaluate on the same data, as this
would be cheating, and likely would not produce a model that generalizes
well to new data. Instead, we split our data into two sets: a training
set and a test set. The \textbf{training set} is used to train the
machine learner, while the \textbf{test set} is used to evaluate the
accuracy of the model\footnote{Depending on the application and the
  amount of available data, a third \emph{development set} is sometimes
  created as a pseudo test set to facilitate the testing of multiple
  approaches on data outside the training set before the final
  evaluation on the test set is performed.}. The larger portion of the
data, from 60\% to 80\%, is used for training, while the remaining
portion is used for testing.

The elephant in the room is, what type of machine learning algorithm do
I use? Well, there are many different types of machine learning
algorithms, each with their own strengths and weaknesses. The first
rough cut is to decide what type of outcome variable we are predicting:
categorical or numeric. If the outcome variable is categorical, we are
performing a \textbf{classification} task, and if the outcome variable
is numeric, we are performing a \textbf{regression} task. As we see in
Table~\ref{tbl-pda-algorithms}, there are various algorithms that can be
used for each task.

\hypertarget{tbl-pda-algorithms}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}@{}}
\caption{\label{tbl-pda-algorithms}Some common supervised learning
algorithms used in PDA.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Learner type
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Learner type
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Logistic Regression & Linear Regression & Interpretable \\
Decision Tree & Regression Tree & Interpretable \\
Support Vector Machine & Support Vector Regression & Black box \\
Multilayer Perceptron & Multilayer Perceptron & Black box \\
\end{longtable}

I've included a column in Table~\ref{tbl-pda-algorithms} that
charaterizes a second consideration which is whether we want an
interpretable model or a black box model. When talking about whether a
model is interpretable or not, we are not referring to the evaluation of
the accuracy of the model. Rather, we are referring to the inner
workings of the model itself that allow us to understand how the model
is making its predictions. An \textbf{interpretable model} is one that
can be understood and explored by humans, while a \textbf{black box
model} is one whose inner workings are not trivially unraveled. The
advantage of an interpretable model is that it researchers can go beyond
evaluating prediction accuracy and probe feature-outcome associations.
On the other hand, if the goal is to simply boost prediction accuracy,
interpretability may not be a concern.

Finally, there are a number of algorithm-specific strengths and
weaknesses to be considered in the process of model selection. These
hinge on characteristics of the data, such as the size of the dataset,
the number of features, the type of features, and the expected type of
relationships between features or on computing resources, such as the
amount of time available to train the model or the amount of memory
available to store the model.

\textbf{Model evaluation} is the process of assessing the accuracy of
the model on the test set, which is a proxy for how well the model will
generalize to new data. Model evaluation is performed quantitatively by
calculating the accuracy of the model on the training, to develop the
model, and ultimately, the test set. The accuracy of a model is
calculated by comparing the predicted values to the actual values. For
the results of classification tasks, this results in a contingency
table, known as a confusion matrix. A \textbf{confusion matrix}
juxtaposes predicted and actual values allowing various metrics to be
calculated, for example in Table~\ref{tbl-aa-confusion-matrix}.

\hypertarget{tbl-aa-confusion-matrix}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\label{tbl-aa-confusion-matrix}Confusion matrix for the
utterance type classification task.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted: natives
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted: nonnatives
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted: natives
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Predicted: nonnatives
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Actual: natives} & 26294 (90\% of 29215) & 2921 (10\% of
29215) \\
\textbf{Actual: nonnatives} & 730 (10\% of 7304) & 6574 (90\% of
7304) \\
\end{longtable}

Since regression tasks predict numeric values, the accuracy of the model
is calculated by comparing the difference between the predicted and
actual values.

It is important to note that whether the accuracy metrics are good is to
some degree qualitative judgment. For example, classification accuracy
overall may be relatively high, but the model may be performing poorly
on one of the classes. In this case, the model may not be useful for the
task at hand, despite the overall accuracy.

In the end, PDA offers a versitle path to discover data-driven insights,
to probe theory-driven associations, or even simply to perform tasks
that are too complex or time-consuming for humans to perform.

\hypertarget{sec-aa-infer}{%
\subsection{Infer}\label{sec-aa-infer}}

The most commonly recognized of the three data analysis approaches,
\textbf{Inferential data analysis (IDA)} is the bread-and-butter of
science. IDA is a deductive, theory-driven approach in which all aspects
of analysis stem from a pre-determined premise, or hypothesis, about the
nature of a relationship in the world and then aims to test whether this
relationship is statistically supported given the evidence. Since the
goal is to infer conclusions about a certain relationship in the
population based on a statistical evaluation of a (corpus) sample, the
representativeness of the sample is of utmost importance. Furthermore,
the use of the data is limited to the scope of the hypothesis --that is,
the data cannot be used for exploratory purposes.

The selection of variables and the roles they play in the analysis are
determined by the hypothesis. In a nutshell, a \textbf{hypothesis} is a
formal statement about the state of the world. This statement is
theory-driven meaning that it is predicated on previous research. We are
not exploring or examining relationships, rather we are testing a
specific relationship. In practice, however, we are in fact proposing
two mutally exclusive hypotheses. The first is the \textbf{Alternative
Hypothesis}, or \(H_1\). This is the hypothesis I just described --the
statement grounded in the previous literature outlining a predicted
relationship. The second is the \textbf{Null Hypothesis}, or \(H_0\).
This is the flip-side of the hypothesis testing coin and states that
there is no difference or relationship. Together \(H_1\) and \(H_0\)
cover all logical outcomes.

To connect hypotheses to variable selection and variable roles, let's
consider a study in which a researcher is investigating the claim that
men and women differ in terms of the number of questions they use in
spontaneous conversations. The unit of analysis is individuals (i.e.~men
and women) and the unit of observation is (sponteanous) conversations.

A dataset based on the Switchboard Dialog Act Corpus (SWDA)
(\protect\hyperlink{ref-SWDA2008}{University of Colorado Boulder 2008}),
seen in Table~\ref{tbl-aa-ida-swda-dataset}, aligns well with this
investigation. It is a large collection of transcribed telephone
conversations between strangers. The dataset includes gender information
for each participant and dialog act annotation for each utterance,
including a range of question types.

\hypertarget{tbl-aa-ida-swda-dataset}{}
\begin{table}
\caption{\label{tbl-aa-ida-swda-dataset}Data dictionary of the SWDA dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & description & variable\_type\\
\hline
doc\_id & Document ID & Unique identifier for each document & numeric\\
\hline
speaker\_id & Speaker ID & Unique identifier for each speaker & numeric\\
\hline
sex & Speaker Gender & Gender of the speaker & character\\
\hline
damsl\_tag & DAMSL Tag & A tag indicating the dialogue act of the utterance & character\\
\hline
utterance\_text & Utterance Text & The text of the speaker's utterance & character\\
\hline
\end{tabular}
\end{table}

The Alternative Hypothesis may be formulated in this way:

\(H_1\): Men and women differ in the frequency of the use of questions
in spontaneous conversations.

The Null Hypothesis, then, would be a statement describing the remaining
logical outcomes. Specifically:

\(H_0\): Men and women do \emph{not} differ in the frequency of the use
of questions in spontaneous conversations.

Now, in standard IDA one variable is the dependent variable and one or
more variables are predictor variables. The \textbf{dependent variable},
sometimes referred to as the outcome or response variable, is the
variable which contains the information which is hypothesized to depend
on the information in the predictor variable(s). It is the variable
whose variation a research study seeks to explain. A \textbf{predictor
variable}, sometimes referred to as a independent or explanatory
variable, is a variable whose variation is hypothesized to explain the
variation in the dependent variable.

Returning to our hypothetical study and the hypotheses presented, we can
identify the variables in our study and map them to their roles. The
frequency of questions used by each speaker would be our dependent
variable and the biological sex of the speakers our predictor variable.
This is so because \(H_1\) states the proposition that a speaker's sex
will predict the frequency of questions used. The next step would be to
operationalize what we mean by `frequency of questions' and then
transform the dataset to reflect this definition.

In our hypothetical study we've identified two variables, one dependent
and one predictor. It is important keep in mind that there can be
multiple predictor variables in cases where the dependent variable's
variation is predicted to be related to multiple variables. This
relationship would need to be explicitly part of the original
hypothesis, however. Due to the increasing difficulty for
interpretation, in practice, IDA studies rarely include more than two or
three predictor variables in the same analysis.

Predictor variables add to the complexity of a study because they are
part of our research focus, specifically our hypothesis. It is, however,
common to include other variables which are not of central focus, but
are commonly assumed to contribute to the explanation of the variation
of the dependent variable. Let's assume that the background literature
suggests that the age of speakers also plays a role in the number of
questions that men and women use in spontaneous conversation. Let's also
assume that the data we have collected includes information about the
age of speakers. If we would like to factor out the potential influence
of age on the use of questions and focus on the particular predictor
variables we've defined in our hypothesis, we can include the age of
speakers as a \textbf{control variable}. A control variable will be
added to the statistical analysis and documented in our report but it
will not be included in the hypothesis nor interpreted in our results.

We can now see in Figure~\ref{fig-aa-ida-variables} the variables roles
assigned to variables in a hypothesis-driven study.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{figures/aa-ida-variables.drawio.png}

}

\caption{\label{fig-aa-ida-variables}Roles of variables in IDA.}

\end{figure}

At this point let's look at the main characteristics that need to be
taken into account to statistically interrogate the variables we have
chosen to test our hypothesis. The type of statistical test that one
chooses is based on (1) the informational value of the dependent
variable and (2) the number of predictor variables included in the
analysis. Together these two characteristics go a long way in
determining the appropriate class of statistical test, but other
considerations about the distribution of particular variables
(i.e.~normality), relationships between variables (i.e.~independence),
and expected directionality of the predicted effect may condition the
appropriate method to be applied.

As you can imagine, there are a host of combinations and statistical
tests that apply in particular scenarios, too many to consider in given
the scope of this coursebook (see S. Th. Gries
(\protect\hyperlink{ref-Gries2013a}{2013}) and Paquot and Gries
(\protect\hyperlink{ref-Paquot2020a}{2020}) for a more exhaustive
description). Below I've summarized some common statistical scenarios
and their associated tests which focus on the juxtaposition of
informational values and the number of variables, leaving aside
alternative tests which deal with non-normal distributions, ordinal
variables, \emph{etc.}

In Table~\ref{tbl-ida-methods-monofactorial} we see
\textbf{monofactorial tests}, tests with only one predictor variable.

\hypertarget{tbl-ida-methods-monofactorial}{}
\begin{table}
\caption{\label{tbl-ida-methods-monofactorial}Common monofactorial tests used in IDA. }\tabularnewline

\centering
\begin{tabular}{l|l|l}
\hline
\multicolumn{2}{c|}{Variable roles} & \multicolumn{1}{c}{ } \\
\cline{1-2}
Dependent & Predictor & Test\\
\hline
Categorical & Categorical & Pearson's Chi-squared test\\
\hline
Numeric & Categorical & Student's t-Test\\
\hline
Numeric & Numeric & Pearson's correlation test\\
\hline
\end{tabular}
\end{table}

Table~\ref{tbl-ida-methods-multifactorial} includes a listing of
\textbf{multifactorial tests}, tests with more than one predictor and/
or control variables.

\hypertarget{tbl-ida-methods-multifactorial}{}
\begin{table}
\caption{\label{tbl-ida-methods-multifactorial}Common multifactorial tests used in IDA. }\tabularnewline

\centering
\begin{tabular}{l>{}l>{}ll}
\toprule
\multicolumn{3}{c}{Variable roles} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){1-3}
Dependent & Predictor & Control & Test\\
\midrule
Categorical & \em{varied} & \em{varied} & Logistic regression\\
Numeric & \em{varied} & \em{varied} & Linear regression\\
\bottomrule
\end{tabular}
\end{table}

IDA relies heavily on quantitative evaluation methods to draw
conclusions that can be generalized to the target population. It is key
to understand that our goal in hypothesis testing is not to find
evidence in support of \(H_1\), but rather to assess the likelihood that
we can reliably reject \(H_0\). The metric used to determine if there is
sufficient evidence is based on the probability that given the nature of
the relationship and the characteristics of the data, the likelihood of
there being no difference or relationship is low. The threshold for
likelihood has traditionally been summarized in the \(p\)-value
statistic. In the Social Sciences, a \(p\)-value lower than .05 is
considered \emph{statistically significant} which when interpreted
correctly means that there is more than a 95\% chance that the observed
relationship would not be predicted by \(H_0\). Note that we are working
in the realm of probability, not in absolutes, therefore an analysis
that produces a significant result does not prove \(H_1\) is correct or
that \(H_0\) is incorrect, for that matter. A margin of error is always
present. For this reason, other metrics such as effect size and
confidence intervals are also used to interpret the results of
statistical tests.

\hypertarget{sec-aa-communicate}{%
\section{Communicate}\label{sec-aa-communicate}}

Conducting research should be enjoyable and personally rewarding but the
effort you have invested and knowledge you have generated should be
shared with others. Whether part of a blog, presentation, journal
article, or for your own purposes it is important to document your
analysis results and process in a way that is informative and
interpretable. This enhances the value of your work, allowing others to
learn from your experience and build on your findings.

\hypertarget{sec-aa-report}{%
\subsection{Report}\label{sec-aa-report}}

The most widely recognized form of communicating research is through a
report. A report is a narrative of your analysis, including the research
question, the data you used, the methods you applied, and the results
you obtained. We are both reporting our findings and documenting our
process to inform others of what we did and why we did it but also to
invite readers to evaluate our findings for themselves. The scientific
process is a collaborative one and evaluation by peers is a key
component of the process.

The audience for your report will determine the level of detail and the
type of information you will need to include in your report but there
are some common elements to reference in any report. First, the research
question and/ or hypothesis should be clearly stated and the motivation
for the question should be explained. This will help the reader
understand the context of the analysis and the importance of the
results. Second, diagnostic procedures to verifiy or describe the data
should be explained. This may include anomaly correction, missing data,
data transformation, etc. and/ or descriptive summaries of the data
including assessments of individual variables (central tendency,
dispersion, distribution) and/ or relationships between variables
(association strength). Third, a blueprint of the methods used will
describe the variable selection process, how the variables are
operationalized, what analysis methods are employed, and how the
variables are used in the statistical analysis. Fourth, the results from
the analysis are reported. Reporting details will depend on the type of
analysis and the particular method(s) employed. For inferential analyses
this will include the test statistic(s) and some measure of confidence.
In predictive analyses, accuracy results will be reported. For
exploratory analyses, the reporting of results will vary and often
include visualizations and metrics that require more human
interpretation than the other analysis types. Finally, the results are
interpreted in light of the research question and/ or hypothesis. This
will include a discussion of the limitations of the analysis and a
discussion of the implications of the results for future research.

\hypertarget{sec-aa-document}{%
\subsection{Document}\label{sec-aa-document}}

While a good report will include the most vital information to
understand the procedures, results, and findings of an analysis, there
is much more information generated in the course of an analysis which
does not traditionally appear in prose. If a research project is
conducted programmatically, however, data, code, and documentation can
be made available to others as part of the communication process.
Increasingly, researchers are sharing their data and code as part of the
publication process. This allows others to reproduce the analysis and
verify the results contributing to the collaborative nature of the
scientific process. \faIcon{wrench} {[}CITATION{]}

Together, data, code, and documentation form a \textbf{research
compendium}. As you can imagine the research process can quickly become
complex and unwieldy as the number of files and folders grows. If not
organized properly, it can be difficult to find the information you
need. Furthermore, if not documented, decisions made in the course of
the analysis can be difficult or impossible to trace. For this reason it
is recommendable to follow a set of best practices for organizing and
documenting your research compendium.

We will have more to say about this in the next chapter but for now it
will suffice to point to some key elements in a research compendium.
First, the data used in the analysis should be saved as a separate
file(s). As a given research project progresses to analysis, the data
may be transformed and manipulated to best fit the needs of the
analysis. Preserving the data at each stage adds to the complete picture
of the data from collection to analysis. Second, since you are working
programmatically, you can share your precise analysis step-by-step in
code form. This allows others to reproduce your analysis and verify your
results. Including code comments provides additional information to
communicate the steps taken and your thought process. Finally, a
codebook documents any additional information that helps understand the
research better. This will often include guides for installing software
and running the code to reproduce the analysis and an overview of the
aims of the scripts and the contents of the data and datasets.

\hypertarget{summary-2}{%
\section*{Summary}\label{summary-2}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have focused on description and analysis --the third
component of DIKI Hierarchy. This process is visually summarized in
Figure~\ref{fig-approaching-analysis-vis-sum}.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{figures/aa-diki.drawio.png}

}

\caption{\label{fig-approaching-analysis-vis-sum}Approaching analysis:
visual summary}

\end{figure}

Building on the strategies covered in
\protect\hyperlink{sec-understanding-data}{Chapter 2 ``Understanding
data''} to derive a rich relational dataset, in this chapter we outlined
key points in approaching analysis. The first key step in any analysis
is to perform a diagnostic assessment of the individual variables and
relationships between variables. To select the appropriate descriptive
measures we covered the various informational values that a variable can
take. In addition to providing key information for reporting purposes,
descriptive measures are important to explore so the researcher can get
a better feel for the dataset before conducting an analysis.

We outlined three data analysis types in this chapter: exploratory,
predictive, and inferential. Each of these embodies distinct approaches
to deriving knowledge from data. Ultimately the choice of analysis type
is highly dependent on the goals of the research. Inferential analysis
is centered around the goal of testing a hypothesis, and for this reason
it is the most highly structured approach to analysis. This structure is
aimed at providing the mechanisms to draw conclusions from the results
that can be generalized to the target population. Predictive analysis
has a less-ambitious but at times more relevant goal of examining the
extent to which a given relationship can be established from the data to
provide a model of language that can accurately predict an outcome using
new data. This methodology is highly effective for applying different
algorithmic approaches and examining relationships between an outcome
variable and various configurations of variables. The ability to explore
the data in multiple ways, is also a key strength of employing an
exploratory analysis. The least structured and most variable of the
analysis types, exploratory analyses are a powerful approach to
generating knowledge from data in an area where clear predictions cannot
be made.

I rounded out this chapter with a short description of the importance of
communicating the analysis process and results. Reporting, in its
traditional form, is documented in prose in an article. This reporting
aims to provide the key information that a reader will need to
understand what was done, how it was done, and why it was done. This
information also provides the necessary information for reader's with a
critical eye to understand the analysis in more detail. Yet even the
most detailed reporting in a write-up still leaves many practical, but
key, points of the analysis obscured. A programming approach provides
the procedural steps taken that when shared provide the exact methods
applied. Together with the write-up, a research compendium which
provides the scripts to run the analysis and documentation on how to run
the analysis forms an integral part of creating reproducible research.

\hypertarget{activities-1}{%
\section*{Activities}\label{activities-1}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Add description of outcomes \ldots{}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-3.html}{Descriptive
assessment of datasets}\\
\textbf{How}: Read Recipe 3 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To explore appropriate methods for summarizing variables
in datasets given the number and informational values of the
variable(s).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-3}{Descriptive
assessment of datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 3.\\
\textbf{Why}: To identify and apply the appropriate descriptive methods
for a vector's informational value and to assess both single variables
and multiple variables with the appropriate statistical, tabular, and/
or graphical summaries.

\end{tcolorbox}

\hypertarget{questions-2}{%
\section*{Questions}\label{questions-2}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  What are the key differences between assessment and analysis?
\item
  What are the potential measures of central tendency and dispersion for
  a variable? Does it depend on the informational value of the variable?
\item
  Consider the following variables: \(X\) = number of children, \(Y\) =
  number of siblings, \(Z\) = number of siblings who are older than the
  participant. Which of these variables are categorical, ordinal,
  numeric? What are the measures of central tendency and dispersion for
  each variable?
\item
  What type(s) of tables or plots are appropriate for summarizing a
  variable? What type(s) of tables or plots are appropriate for
  summarizing the relationship between two variables?
\item
  In the following variables and informational values, identify if the
  plots are appropriate for summarizing the relationship.\\
  \faIcon{wrench} \ldots{}
\item
  What are the key differences between exploratory, predictive, and
  inferential analysis?
\item
  How do the goals of the research influence the choice of analysis
  type?
\item
  Given the following research questions, identify which type of
  analysis is most appropriate and why.\\
  \faIcon{wrench} \ldots{}
\item
  How are the results of inferential, predictive, and exploratory
  analysis evaluated?
\item
  Research compendia are an important part of reproducible research.
  What are the key components of a research compendium? What are the
  benefits of sharing a research compendium?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{wrench} Technical exercises}

\begin{itemize}
\tightlist
\item
  Create a contingency table for the following variables:
\item
  Create a plot for the following variables:
\item
  Report these tables and plots with a short interpretation of what they
  show.
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-framing-research}{%
\chapter{Framing research}\label{sec-framing-research}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
Thus, the task is, not so much to see what no one has seen yet; but to
think what nobody has thought yet, about that what everybody sees.

--- Arthur Schopenhauer
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify a research area and problem by listing key strategies and
  describing their contribution towards research identification.
\item
  Explain the significance of a well-framed research question in guiding
  the overall research project.
\item
  Comprehend how the conceptual and practical steps involved in
  developing a research blueprint aid not only the researcher but also
  the broader scientific community.
\end{itemize}

\end{tcolorbox}

At this point in this part of the textbook, we have covered Data,
Information, and Knowledge from the Data to Insight Hierarchy. The goal
has been to provide an orientation to the main building blocks of doing
text analysis. Insight is the last component of the hierarchy. However,
in practical terms, it is the first step to address in an research
project as goals of a research project influence all subsequent steps.

In this chapter we discuss how to frame research, that is how to
position your research project's findings to contribute insight to
understanding of the world. We will cover how to connect with the
literature, selecting a research area and identifying a research
problem, and how to design research best positioned to return relevant
findings that will connect with this literature, establishing a research
aim and research question. We will round out this chapter with a guide
on developing a research blueprint --a working plan to organize the
conceptual and practical steps to implement the research effectively and
in a way that supports communicating the research findings and the
process by which the findings were obtained.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Version
control}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \faIcon{wrench} To \ldots.

\end{tcolorbox}

\hypertarget{sec-fr-frame}{%
\section{Frame}\label{sec-fr-frame}}

Together a research area, problem, aim and question and the research
blueprint that forms the conceptual and practical scaffolding of the
project ensure from the outset that the project is solidly grounded in
the main characteristics of good research. These characteristics,
summarized by Cross (\protect\hyperlink{ref-Cross2006}{2006}), are found
in Table~\ref{tbl-fr-cross-research-char-table}.

\hypertarget{tbl-fr-cross-research-char-table}{}
\begin{table}
\caption{\label{tbl-fr-cross-research-char-table}Characteristics of research (Cross, 2006). }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Characteristic & Description\\
\hline
Purposive & Based on identification of an issue or problem worthy and capable of investigation\\
\hline
Inquisitive & Seeking to acquire new knowledge\\
\hline
Informed & Conducted from an awareness of previous, related research\\
\hline
Methodical & Planned and carried out in a disciplined manner\\
\hline
Communicable & Generating and reporting results which are feasible and accessible by others\\
\hline
\end{tabular}
\end{table}

With these characteristics in mind, let's get started with the first
component to address --connecting with the literature.

\hypertarget{sec-fr-connect}{%
\section{Connect}\label{sec-fr-connect}}

\hypertarget{research-area}{%
\subsection{Research area}\label{research-area}}

The first decision to make in the research process is to identify a
research area. A \textbf{research area} is a general area of interest
where a researcher wants to derive insight and make a contribution to
understanding. For those with an established research trajectory in
language, the area of research to address through text analysis will
likely be an extension of their prior work. For others, which include
new researchers or researcher's that want to explore new areas of
language research or approach an area through a language-based lens, the
choice of area may be less obvious. In either case, the choice of a
research area should be guided by a desire to contribute something
relevant to a theoretical, social, and/ or practical matter of personal
interest. Personal relevance goes a long way to developing and carrying
out \textbf{purposive} and \textbf{inquisitive} research.

So how do we get started? The first step is to reflect on your own areas
of interest and knowledge, be it academic, professional, or personal.
Language is at the heart of the human experience and therefore found in
some fashion anywhere one seeks to find it. But it is a big world and
more often than not the general question about what area to explore
language use is sometimes the most difficult. To get the ball rolling,
it is helpful to peruse disciplinary encyclopedias or handbooks of
linguistics and language-related an academic fields
(e.g.~\href{https://www.sciencedirect.com/referencework/9780080448541/encyclopedia-of-language-and-linguistics}{Encyclopedia
of Language and Linguistics} (\protect\hyperlink{ref-Brown2005}{Brown
2005}),
\href{https://www.sciencedirect.com/book/9781843345978/a-practical-guide-to-electronic-resources-in-the-humanities}{A
Practical Guide to Electronic Resources in the Humanities}
(\protect\hyperlink{ref-Dubnjakovic2010}{Dubnjakovic and Tomlin 2010}),
\href{https://www.routledgehandbooks.com/doi/10.4324/9781315749129}{Routledge
encyclopedia of translation technology}
(\protect\hyperlink{ref-Chan2014}{Chan 2014}))

A more personal, less academic, approach is to consult online forums,
blogs, \emph{etc}. that one already frequents or can be accessed via an
online search. For example, \href{https://www.reddit.com/}{Reddit} has a
wide variety of active subreddits
(\href{https://www.reddit.com/r/LanguageTechnology/}{r/LanguageTechnology},
\href{https://www.reddit.com/r/linguistics/}{r/Linguistics},
\href{https://www.reddit.com/r/corpuslinguistics/}{r/corpuslinguistics},
\href{https://www.reddit.com/r/DigitalHumanities/}{r/DigitalHumanities},
\emph{etc}.). Twitter and Facebook also have interesting posts on
linguistics and language-related fields worth following. Through one of
these social media site you may find particular people that maintain a
blog worth browsing. For example, I follow
\href{https://juliasilge.com/}{Julia Silge},
\href{http://www.rctatman.com/}{Rachel Tatman}, and
\href{https://tedunderwood.com/}{Ted Underwood}, \emph{inter alia}.
Perusing these resources can help spark ideas and highlight the kinds of
questions that interest you.

Regardless of whether your inquiry stems from academic, professional, or
personal interest, try to connect these findings to academic areas of
research. Academic research is highly structured and well-documented and
making associations with this network will aid in subsequent steps in
developing a research project.

\hypertarget{sec-fr-problem}{%
\subsection{Research problem}\label{sec-fr-problem}}

Once you've made a rough-cut decision about the area of research, it is
now time to take a deeper dive into the subject area and jump into the
literature. This is where the rich structure of disciplinary research
will provide aid to traverse the vast world of academic knowledge and
identify a research problem. A \textbf{research problem} highlights a
particular topic of debate or uncertainty in existing knowledge which is
worthy of study.

Surveying the relevant literature is key to ensuring that your research
is \textbf{informed}, that is, connected to previous work. Identifying
relevant research to consult can be a bit of a `chicken or the egg'
problem --some knowledge of the area is necessary to find relevant
topics, some knowledge of the topics is necessary to narrow the area of
research. Many times the only way forward is to jump into conducting
searches. These can be world-accessible resources (\emph{e.g.}
\href{https://scholar.google.com/}{Google Scholar}) or limited-access
resources that are provided through an academic institution (\emph{e.g.}
\href{https://about.proquest.com/en/products-services/llba-set-c}{Linguistics
and Language Behavior Abstracts}), \href{https://eric.ed.gov/}{ERIC},
\href{https://www.ebsco.com/products/research-databases/apa-psycinfo}{PsycINFO},
\emph{etc.}). Some organizations and academic institutions provide
\href{https://guides.zsr.wfu.edu/linguistics}{research guides} to help
researcher's access the primary literature.

Another avenue to explore are journals dedicated to areas in which
linguistics and language-related research is published. In
Table~\ref{tbl-pinboard-journals-linguistics},
Table~\ref{tbl-pinboard-journals-humanities}, and
Table~\ref{tbl-pinboard-journals-cl}, I've listed a number of highly
visible journals in linguistics, digital humanities, and computational
linguistics.

\hypertarget{tbl-pinboard-journals-linguistics}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-linguistics}A list of some linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://www.euppublishing.com/loi/cor">Corpora</a> & An international, peer-reviewed journal of corpus linguistics focusing on the many and varied uses of corpora both in linguistics and beyond.\\
\hline
<a href="https://www.degruyter.com/journal/key/CLLT/html">Corpus Linguistics and Linguistic Theory</a> & Corpus Linguistics and Linguistic Theory (CLLT) is a peer-reviewed journal publishing high-quality original corpus-based research focusing on theoretically relevant issues in all core areas of linguistic research, or other recognized topic areas.\\
\hline
<a href="https://benjamins.com/catalog/ijcl">International Journal of Corpus Linguistics</a> & The International Journal of Corpus Linguistics (IJCL) publishes original research covering methodological, applied and theoretical work in any area of corpus linguistics.\\
\hline
<a href="http://ijls.net/">International Journal of Language Studies</a> & It is a refereed international journal publishing articles and reports dealing with theoretical as well as practical issues focusing on language, communication, society and culture.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-child-language">Journal of Child Language</a> & A key publication in the field, Journal of Child Language publishes articles on all aspects of the scientific study of language behaviour in children, the principles which underlie it, and the theories which may account for it.\\
\hline
<a href="https://www.cambridge.org/core/journals/journal-of-linguistic-geography/all-issues">Journal of Linguistic Geography</a> & The Journal of Linguistic Geography focuses on dialect geography and the spatial distribution of language relative to questions of variation and change.\\
\hline
<a href="http://www.tandfonline.com/toc/njql20/current">Journal of Quantitative Linguistics</a> & Publishes research on the quantitative characteristics of language and text in mathematical form, introducing methods of advanced scientific disciplines.\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-humanities}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-humanities}A list of some humanities journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="http://www.digitalhumanities.org/dhq/">Digital Humanities Quarterly</a> & Digital Humanities Quarterly (DHQ), an open-access, peer-reviewed, digital journal covering all aspects of digital media in the humanities.\\
\hline
<a href="https://academic.oup.com/dsh">Digital Scholarship in the Humanities</a> & DSH or Digital Scholarship in the Humanities is an international, peer reviewed journal which publishes original contributions on all aspects of digital scholarship in the Humanities including, but not limited to, the field of what is currently called the Digital Humanities.\\
\hline
<a href="https://culturalanalytics.org/">Journal of Cultural Analytics</a> & Cultural Analytics is an open-access journal dedicated to the computational study of culture. Its aim is to promote high quality scholarship that applies computational and quantitative methods to the study of cultural objects (sound, image, text), cultural processes (reading, listening, searching, sorting, hierarchizing) and cultural agents (artists, editors, producers, composers).\\
\hline
\end{tabular}
\end{table}

\hypertarget{tbl-pinboard-journals-cl}{}
\begin{table}
\caption{\label{tbl-pinboard-journals-cl}A list of some computational linguistics journals. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
Resource & Description\\
\hline
<a href="https://direct.mit.edu/coli">Computational Linguistics</a> & Computational Linguistics is the longest-running publication devoted exclusively to the computational and mathematical properties of language and the design and analysis of natural language processing systems.\\
\hline
<a href="http://lrec-conf.org/">LREC Conferences</a> & The International Conference on Language Resources and Evaluation is organised by ELRA biennially with the support of institutions and organisations involved in HLT.\\
\hline
<a href="https://transacl.org/index.php/tacl/index">Transactions of the Association for Computational Linguistics</a> & Transactions of the Association for Computational Linguistics (TACL) is anACL-sponsored journalpublished by MIT Pressthat publishes papers in all areas of computational linguistics and natural language processing.\\
\hline
\end{tabular}
\end{table}

To explore research related to text analysis it is helpful to start with
the (sub)discipline name(s) you identified in when selecting your
research area, more specific terms that occur to you or key terms from
the literature, and terms such as `corpus study' or `corpus-based'. The
results from first searches may not turn out to be sources that end up
figuring explicitly in your research, but it is important to skim these
results and the publications themselves to mine information that can be
useful to formulate better and more targeted searches. Relevant
information for honing your searches can be found throughout an academic
publication (article or book). However, pay particular attention to the
abstract, in articles, and the table of contents, in books, and the
cited references. Abstracts and tables of contents often include
discipline-specific jargon that is commonly used in the field. In some
articles there is even a short list of key terms listed below the
abstract which can be extremely useful to seed better and more precise
search results. The references section will contain relevant and
influential research. Scan these references for publications which
appear to narrowing in on topic of interest and treat it like a search
in its own right.

Once your searches begin to show promising results it is time to keep
track and organize these references. Whether you plan to collect
thousands of references over a lifetime of academic research or your aim
is centered around one project, software such as
\href{https://www.zotero.org/}{Zotero}\footnote{\href{https://guides.zsr.wfu.edu/zotero}{Zotero
  Guide}},
\href{https://www.mendeley.com/reference-management/reference-manager}{Mendeley},
or \href{https://bibdesk.sourceforge.io/}{BibDesk} provide powerful,
flexible, and easy-to-use tools to collect, organize, annotate, search,
and export references. Citation management software is indispensable for
modern research --and often free!

As your list of relevant references grows, you will want to start the
investigation process in earnest. Begin skimming (not reading) the
contents of each of these publications, starting with the most relevant
first\footnote{Or what appears to be most relevant. This may change as
  you start to take a closer look.}. Annotate these publications using
highlighting features of the citation management software to identify:
(1) the stated goal(s) of the research, (2) the data source(s) used, (3)
the information drawn from the data source(s), (4) the analysis approach
employed, and (5) the main finding(s) of the research as they pertain to
the stated goal(s). Next, in your own words, summarize these five key
areas in prose adding your summary to the notes feature of the citation
management software. This process will allow you to efficiently gather
and document references with the relevant information to guide the
identification of a research problem, to guide the formation of your
problem statement, and ultimately, to support the literature review that
will figure in your project write-up.

From your preliminary annotated summaries you will undoubtedly start to
recognize overlapping and contrasting aspects in the research
literature. These aspects may be topical, theoretical, methodological,
or appear along other lines. Note these aspects and continue to conduct
more refine searches, annotate new references, and monitor for any
emerging patterns of uncertainty or debate (gaps) which align with your
research interest(s). When a promising pattern takes shape, it is time
to engage with a more detailed reading of those references which appear
most relevant highlighting the potential gap(s) in the literature.

At this point you can focus energy on more nuanced aspects of a
particular gap in the literature with the goal to formulate a problem
statement. A \textbf{problem statement} directly acknowledges a gap in
the literature and puts a finer point on the nature and relevance of
this gap for understanding. This statement reflects your first
deliberate attempt to establish a line of inquiry. It will be a
targeted, but still somewhat general, statement framing the gap in the
literature that will guide subsequent research design decisions.

\hypertarget{sec-fr-define}{%
\section{Define}\label{sec-fr-define}}

\hypertarget{sec-fr-aim}{%
\subsection{Research aim}\label{sec-fr-aim}}

With a problem statement in hand, it is now time to consider the goal(s)
of the research. A \textbf{research aim} frames the type of inquiry to
be conducted. Will the research aim to explore, examine, or explain? In
other words, will the research seek to uncover novel relationships,
assess the potential strength of a particular relationship, or test a
particular relationship? As you can appreciate, the research aim is
directly related to the analysis methods we touched upon in
\protect\hyperlink{sec-approaching-analysis}{Chapter 3}.

To gauge how to frame your research aim, reflect on the literature that
led you to your problem statement and the nature of the problem
statement itself. If the gap at the center of the problem statement is a
lack of knowledge, your research aim may be exploratory. If the gap
concerns a conjecture about a relationship, then your research may take
a predictive approach. When the gap points to the validation of a
relationship, then your research will likely be inferential in nature.
Before selecting your research aim it is also helpful to consult the
research aims of the primary literature that led you to your research
statement.

Typically, a problem statement addressing a subtle, specific issue tends
to adopt research objectives similar to prior studies. In contrast, a
statement focusing on a broader, more distinct issue is likely to have
unique research goals. Yet, this is more of a guideline than a strict
rule.

It's crucial to understand both the existing literature and the nature
of various types of analyses. Being clear about your research goals is
important to ensure that your study is well-placed to produce results
that add value to the current understanding in an informed manner.

\hypertarget{sec-fr-question}{%
\subsection{Research question}\label{sec-fr-question}}

The next step in research design is to craft the research question. A
\index{research question}\textbf{research question} is clearly defined
statement which identifies an aspect of uncertainty and the particular
relationships that this uncertainty concerns. The research question
extends and narrows the line of inquiry established in the research
statement and research aim. To craft a research question, we can use the
research statment for the content and the research aim for the form.

\hypertarget{sec-fr-question-form}{%
\subsubsection{Form}\label{sec-fr-question-form}}

The form of a research question will vary based on the research aim,
which as I mentioned, is inimately connected to the analysis approach.
For inferential-based research, the research question will actually be a
statement, not a question. This statement makes a testable claim about
the nature of a particular relationship --\emph{i.e.} asserts a
hypothesis.

For illustration, let's return to the hypothesis (\(H_1\)) we previously
sketched out in \protect\hyperlink{sec-approaching-analysis}{Chapter 3},
leaving aside the implicit null hypothesis, seen in
Example~\ref{exm-fr-form-infer}.

\begin{example}[]\protect\hypertarget{exm-fr-form-infer}{}\label{exm-fr-form-infer}

Women use more questions than men in spontaneous conversations.

\end{example}

For predictive- and exploratory-based research, the research question is
in fact a question. A reframing of the example hypothesis for a
predictive-based research question might take the form seen in
Example~\ref{exm-fr-form-pred}.

\begin{example}[]\protect\hypertarget{exm-fr-form-pred}{}\label{exm-fr-form-pred}

Can the number of questions used in spontaneous conversations predict if
a speaker is male or female?

\end{example}

And a similar exploratory-based research question might take the form
seen in Example~\ref{exm-fr-form-exp}.

\begin{example}[]\protect\hypertarget{exm-fr-form-exp}{}\label{exm-fr-form-exp}

Do men and women differ in terms of the number of questions they use in
spontaneous conversations?

\end{example}

The central research interest behind these hypothetical research
questions is, admittedly, quite basic. But from these simplified
examples, we are able to appreciate the similarities and differences
between the forms of research statements that correspond to distinct
research aims.

\hypertarget{sec-fr-question-content}{%
\subsubsection{Content}\label{sec-fr-question-content}}

In terms of content, the research question will make reference to two
key components. First, is the unit of analysis. The \textbf{unit of
analysis} is the entity which the research aims to investigate. For our
three example research aims, the unit of analysis is the same, namely
\emph{speakers}. Note, however, that the current unit of analysis is
somewhat vague in the example research questions. A more precise unit of
analysis would include more information about the population from which
the speakers are drawn (\emph{i.e.} English speakers, American English
speakers, American English speakers of the Southeast, \emph{etc}.).

The second key component is the unit of observation. The \textbf{unit of
observation} is the primary element on which the insight into the unit
of analysis is derived and in this way constitutes the essential
organizational unit of the dataset to be analyzed. In our examples, the
unit of observation, again, is unchanged and is \emph{spontaneous
conversations}. Note that while the unit of observation is key to
identify as it forms the organizational backbone of the research, it is
very common for the research to derive variables from this unit to
provide evidence to investigate the research question.

In examples \ref{exm-fr-form-infer}, \ref{exm-fr-form-pred}, and
\ref{exm-fr-form-exp}, we identified the number of conversations as part
of the research question. Later in the research process it will be key
to operationalize this variable. For example, will the number of
conversations be the total number of conversations in the dataset or
will it be the average number of conversations per speaker? These are
important questions to consider as they will influence variable
selection, statistical choices, and ultimately the interpretation of the
results.

\hypertarget{sec-fr-blueprint}{%
\section{Blueprint}\label{sec-fr-blueprint}}

Efforts to craft a research question are a very important aspect of
developing purposive, inquisitive, and informed research (returning to
Cross's characteristics of research). Moving beyond the research
question in the project means developing and laying out the research
design in a way such that the research is \textbf{Methodical} and
\textbf{Communicable}. In this textbook, the method to achieve these
goals is through the development of a research blueprint. The blueprint
includes two components: (1) laying out a conceptual plan and (2)
deriving the organizational scaffolding that will support the
implementation of the research.

As Ignatow and Mihalcea (\protect\hyperlink{ref-Ignatow2017}{2017})
point out:

\begin{quote}
Research design is essentially concerned with the basic architecture of
research projects, with designing projects as systems that allow theory,
data, and research methods to interface in such a way as to maximize a
project's ability to achieve its goals {[}\ldots{]}. Research design
involves a sequence of decisions that have to be taken in a project's
early stages, when one oversight or poor decision can lead to results
that are ultimately trivial or untrustworthy. Thus, it is critically
important to think carefully and systematically about research design
before committing time and resources to acquiring texts or mastering
software packages or programming languages for your text mining project.
\end{quote}

In what follows, I will cover the main aspects of developing a research
blueprint. I will start with the conceptual plan and then move on to the
organizational scaffolding.

\hypertarget{sec-fr-plan}{%
\subsection{Plan}\label{sec-fr-plan}}

Importance of establishing a feasible research design from the outset
and documenting the key aspects required to conduct the research cannot
be understated. On the one hand this process links a conceptual plan to
a tangible implementation. In doing so, a researcher is
better-positioned to conduct research with a clear view of what will be
entailed. On the other hand, a promising research question may present
unexpected challenges once a researcher sets about to implement the
research. This is not uncommon to encounter issues that require
modification or reevaluation of the viability of the project. However, a
well-documented research plan will help a researcher to identify and
address many of these challenges at the conceptual level before
expending effort on the implementation.

Let's now consider the main aspects of developing a research plan:
identifying data source(s), key variables, and analysis methods. Before
we do, however, it is important to reiterate the importance of a
research question or hypothesis before moving forward in research
planning. The research question or hypothesis is the central component
of the research plan. It guides every step which follows from data
selection to interpretation of the analysis results. Furthermore, a
well-founded research question is based on a solid literature review
from which can provide helpful guidance at key choice points in the
research process.

First, \textbf{identify a viable data source}. Viability includes the
accessibility of the data, availability of the data, and the content of
the data. If a purported data source is not accessible and/ or is has
stringent restrictions on its use, then it is not a viable data source.
If a data source is accessible and available, but does not contain the
building blocks needed to address the research question, then it is not
a viable data source. In the case that research is inferential in
nature, the sampling frame of the corpus is of primary importance as the
goal is to generalize the findings to a target population. A corpus
resource should align, to the extent feasible, with this target
population. For predictive and exploratory research, the goal to
generalize a claim is not central and for this reason the there is some
freedom in terms of how representative a corpus sample is of a target
population. Ideally, a researcher will find and be able to model a
language population of target interest. Since the goal, however, is not
to test a hypothesis, but rather to explore particular or evaluate
potential relationships, either in an exploratory or predictive fashion,
the research can often continue with the stipulation that the results
are interpreted in the light of the characteristics of the available
corpus sample.

The second step is to \textbf{identify the key variables} need to
conduct the research are and then ensure that this information can be
derived from the corpus data. The research question will reference the
unit of analysis and the unit of observation, but it is important at
this point to then pinpoint what the key variables will be. If the unit
of observation is spontaneous conversations. The question as to what
aspects of these conversations will be used in the analysis. In the
research questions presented in this chapter, we will want to envision
what needs to be done to derive a variable which measures the number of
questions in each of the conversations. In other research, their may be
features that need to be extracted, recoded, and/ or generated to
address the research question. Other variables of importance may be
non-linguistic in nature. In cases where there the metadata is
incomplete for the goals of the research, it is sometimes possible to
merge metadata from other sources.

The third step is to \textbf{identify a method of analysis} to
interrogate the dataset. The selection of the analysis approach that was
part of the research aim (\emph{i.e.} explore, predict, or infer) and
then the research question goes a long way to narrowing the methods that
a researcher must consider. But there are a number of factors which will
make some methods more appropriate than others.

Exploratory research is the least restricted of the three types of
analysis approaches. Although it may be the case that a research will
not be able to specify from the outset of a project what the exact
analysis methods will be, an attempt to consider what types of analysis
methods will be most promising to provide results to address the
research question goes a long way to steering a project in the right
direction and grounding the research. As with the other analysis
approaches, it is important to be aware of what the analysis methods
available and what type of information they produce in light of the
research question.

For predictive-based research, the informational value of the target
variable is key to deciding whether the prediction will be a
classification task or a regression task. This has downstream effects
when it comes time to evaluate and interpret the results. Although the
feature engineering process in predictive analyses means that the
features do not need to be specified from the outset and can be tweaked
and changed as needed during an analysis, it is a good idea to start
with a basic sense of what features most likely will be helpful in
developing a robust predictive model. Furthermore, while the number and
informational values of the features (predictor variables) are not as
important to selecting a prediction method (algorithm) as they are in
inferential analysis methods, it is important to recognize that
particular algorithms have strengths and shortcomings when working large
numbers and/ or types of features
(\protect\hyperlink{ref-Lantz2013}{Lantz 2013}).

In inferential research, the number and information values of the
variables to be analyzed will be of key importance
(\protect\hyperlink{ref-Gries2013a}{S. Th. Gries 2013}). The
informational value of the dependent variable will again narrow the
search for the appropriate method. The number of predictor variables
also plays an important role. For example, a study with a categorical
dependent variable with a single categorical predictor variable will
lead the researcher to the Chi-squared test. A study with a numeric
dependent variable with multiple predictor variables will lead to linear
regression. Another aspect of note for inference studies is the
consideration of the distribution of numeric variables --a normal
distribution will use a parametric test where a non-normal distribution
will use a non-parametric test. These details need not be nailed down at
this point, but it is helpful to have them on your radar to ensure that
when the time comes to analyze the data, the appropriate steps are taken
to test for normality and then apply the correct test.

The last of the main components of the research plan concerns the
\textbf{interpretation and evaluation of the results}. This step brings
the research plan full circle connecting the research question to the
methods employed. It is important to establish from the outset what the
criteria will be to evaluate the results. This is in large part a
function of the relationship between the research question and the
analysis method. For example, in exploratory research, the results will
be evaluated qualitatively in terms of the associative patterns that
emerge. Predictive and inferential research leans more heavily on
quantitative metrics in particular the accuracy of the prediction or the
strength of the relationship between the dependent and predictor
variable(s), respectively. However, these quantitative metrics require
qualitative interpretation to determine whether the results are
meaningful in light of the research question.

To summarize these planning steps, I've created a checklist in
Table~\ref{tbl-fr-plan-checklist}.

\hypertarget{tbl-fr-plan-checklist}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\caption{\label{tbl-fr-plan-checklist}Research Plan
Checklist}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Steps
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Steps
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Research Question or Hypothesis & Formulate a research question or
hypothesis based on a thorough review of existing literature including
references. This will guide every subsequent step from data selection to
interpretation of results. \\
Data Source(s) & Identify viable data source(s) and vet the sample data
in light of the research question. Consider to what extent the goal is
to generalize findings to a target population, and ensure that the
corpus aligns as much as feasible with this target. \\
Key Variables & Determine the key variables needed for the research,
define how they will be operationalized, and ensure they can be derived
from the corpus data. Additionally, identify any features that need to
be extracted, recoded or generated. \\
Analysis Method & Choose an appropriate method of analysis to
interrogate the dataset. This choice should be in line with your
research aim (\emph{e.g.}, exploratory, predictive, inferential). Be
aware of what each method can offer and how it addresses your research
question. \\
Interpretation \& Evaluation & Establish criteria to interpret and
evaluate the results. This will be a function of the relationship
between the research question and the analysis method. \\
\end{longtable}

In addition to addressing the steps outlined in
Table~\ref{tbl-fr-plan-checklist}, it is also important to document the
strengths and shortcomings of the research plan including the data
source(s), the information to be extracted from the data, and the
analysis methods. If there are potential shortcomings, which there most
often are, sketch out contingency plans to address these shortcomings.
This will help buttress your research and ensure that your time and
effort is well-spent.

Together the information collected from this process will serve to guide
the research and provide a solid foundation for the research write-up.
Furthermore, you may consider pre-registering your research project to
ensure that your plans are well-documented and to provide a timestamp
for your research. Pre-registration can also be a helpful way to get
feedback on your research plan from colleagues and experts in the field.
Popular pre-registration platforms include \href{https://osf.io/}{Open
Science Framework} and
\href{https://www.cos.io/initiatives/prereg}{Center for Open Science}.

\hypertarget{sec-fr-scaffold}{%
\subsection{Scaffold}\label{sec-fr-scaffold}}

The next step in creating a research blueprint is to consider how to
physically implement your project. This includes how to organize files
and directories in a fashion that both provides the researcher a logical
and predictable structure to work with but also ensures that the
research is \textbf{Communicable}. On the one hand, communicable
research includes a strong write-up of the research, but, on the other
hand, it is also important that the research is reproducible.
Reproducibility strategies are a benefit to the researcher (in the
moment and in the future) as it leads to better work habits and to
better teamwork and it makes changes to the project easier.
Reproducibility is also of benefit to the scientific community as shared
reproducible research enhances replicability and encourages cumulative
knowledge development (\protect\hyperlink{ref-Gandrud2015}{Gandrud
2015}).

There are a set of guiding principles to accomplish these goals
(\protect\hyperlink{ref-Gentleman2007}{Gentleman and Temple Lang 2007};
\protect\hyperlink{ref-Marwick2018}{Marwick, Boettiger, and Mullen
2018}), seen in Example~\ref{exm-fr-repro-principles}.

\begin{example}[]\protect\hypertarget{exm-fr-repro-principles}{}\label{exm-fr-repro-principles}

Reproducible Research Principles

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All files should be plain text which means they contain no formatting
  information other than whitespace.
\item
  There should be a clear separation between the data, method, and
  output of research. This should be apparent from the directory
  structure.
\item
  A separation between original, derived, and analysis data should be
  made. Original data should be treated as `read-only'. Any changes to
  the original data should be justified, generated by the code, and
  documented (see point 6).
\item
  Each project file (script) should represent a particular, well-defined
  step in the research process.
\item
  Each project script should be modular --that is, each file should
  correspond to a specific goal in the analysis procedure with input and
  output only corresponding to this step.
\item
  All project scripts should be tied together by a `main' script that is
  used to coordinate the execution of all the project steps.
\item
  Everything should be documented. This includes data collection, data
  preprocessing, analysis steps, script code comments, data description
  in data dictionaries, information about the computing environment and
  packages used to conduct the analysis, and detailed instructions on
  how to reproduce the research.
\end{enumerate}

\end{example}

These seven principles can be physically implemented in numerous ways.
In recent years, there has been a growing number of efforts to create R
packages and templates to quickly generate the scaffolding and tools to
facilitate reproducible research. Some notable R packages include
\href{https://jdblischak.github.io/workflowr/}{workflowr}
(\protect\hyperlink{ref-R-workflowr}{Blischak, Carbonetto, and Stephens
2023}), \href{http://projecttemplate.net/}{ProjectTemplate}
(\protect\hyperlink{ref-R-ProjectTemplate}{White 2023}), and
\href{https://github.com/ropensci/targets}{targets}
(\protect\hyperlink{ref-R-targets}{Landau 2023}), but there are many
other resources for R included on the
\href{https://cran.r-project.org/web/views/ReproducibleResearch.html}{CRAN
Task View for Reproducible Research}. There are many advantages to
working with pre-existing frameworks for the savvy R programmer
including the ability to quickly generate a project scaffold, to
efficiently manage changes to the project, and to buy in to a common
framework that is supported by a community of developers.

On the other hand, these frameworks can be a bit daunting for the novice
R programmer. At the most basic level, a project can implement the seven
principles outlined above by creating a directory structure and a set of
files manually.

\begin{example}[]\protect\hypertarget{exm-fr-basic-project}{}\label{exm-fr-basic-project}

Minimal Project Framework

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    analysis/}
\ExtensionTok{}\NormalTok{    derived/}
\ExtensionTok{}\NormalTok{    original/}
\ExtensionTok{}\NormalTok{ output/}
\ExtensionTok{}\NormalTok{    figures/}
\ExtensionTok{}\NormalTok{    reports/}
\ExtensionTok{}\NormalTok{    results/}
\ExtensionTok{}\NormalTok{    tables/}
\ExtensionTok{}\NormalTok{ code/}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{ \_main.R}
\ExtensionTok{}\NormalTok{ README}
\end{Highlighting}
\end{Shaded}

\end{example}

In Example~\ref{exm-fr-basic-project}, I provide a minimal framework
that aligns with the reproducible research principles. Let me now make
the connections between the principles and this project structure.

The \emph{project/} directory is composed of three main sections:
\emph{data/}, \emph{output/}, and \emph{code/} corresponding to the
input, output, and code, respectively.

The \emph{data/} section is divided into three subsections:

\begin{itemize}
\tightlist
\item
  \emph{analysis/} for storing data used to perform analysis
\item
  \emph{derived/} for housing data produced in curation and
  transformation steps
\item
  \emph{original/} for keeping the original `read-only' data
\end{itemize}

The \emph{output/} section contains four subsections:

\begin{itemize}
\tightlist
\item
  \emph{figures/} for visualizations produced as part of the project
\item
  \emph{reports/} for the resulting reports (\emph{e.g.} article,
  presentation, blog post, \emph{etc.})
\item
  \emph{results/} for statistical results from the analysis
\item
  \emph{tables/} for summary tables
\end{itemize}

The \emph{code/} directory houses the code, with the \emph{\_main.R}
file at the root of the project orchestrating the execution of the
scripts that conduct the project steps (\emph{1-acquire-data.qmd},
\emph{2-curate-dataset.qmd}, \emph{3-transform-datasets.qmd}, and
\emph{4-analyze-datasets.qmd}).

Lastly, the \emph{README} file provides a description of the project and
instructions on how to reproduce the research.

The project structure in Example~\ref{exm-fr-basic-project} meets the
minimal structural requirements for reproducible research but can be
augmented in more sophisticated ways to support more functionality, as
we will see in Chapter~\ref{sec-collaboration}. One enhancement that I
highly recommend is the use of literate programming, in the form of
Quarto documents, to serve as the main project scripts. This facilitates
the combination of executable code and prose documentation for each
project step in a single, modular file.

\hypertarget{summary-3}{%
\section*{Summary}\label{summary-3}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

The aim of this chapter is to provide the key conceptual and practical
points to guide the development of a viable research project. Good
research is purposive, inquisitive, informed, methodological, and
communicable. It is not, however, always a linear process. Exploring
your area(s) of interest and connecting with existing work will help
couch and refine your research. But practical considerations, such as
the existence of viable data, technical skills, and/ or time constrains,
sometimes pose challenges and require a researcher to rethink and/ or
redirect the research in sometimes small and other times more
significant ways. The process of formulating a research question and
developing a viable research plan is key to supporting viable,
successful, and insightful research. To ensure that the effort to derive
insight from data is of most value to the researcher and the research
community, the research should strive to be methodological and
communicable adopting best practices for reproducible research.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{figures/fr-diki.drawio.png}

}

\caption{\label{fig-fr-visual-summary}Framing research: visual summary}

\end{figure}

This chapter concludes the Foundations part of this textbook. At this
stage our overview of fundamental characteristics of research are in
place to move a project towards implementation, as seen in
Figure~\ref{fig-fr-visual-summary}. From this point forward we will
integrate your conceptual knowledge and emerging R programming skills as
we cover common scenarios encountered when conducting reproducible
research with real-world data.

The next part, Preparation, aims to cover R coding strategies to
acquire, curate, and transform data in preparation for analysis. These
are the first steps in putting a research blueprint into action and by
no coincidence the first components in the Data to Insight Hierarchy.
Without further ado, let's get started!

\hypertarget{activities-2}{%
\section*{Activities}\label{activities-2}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Add description of outcomes \ldots{}
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://qtalr.github.io/qtalrkit/articles/recipe-4.html}{Project
management}\\
\textbf{How}: Read Recipe 4 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: \faIcon{wrench} To learn how to use \ldots{} reproducible
research projects.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/qtalr/lab-4}{Project
management}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 4.\\
\textbf{Why}: \faIcon{wrench} To \ldots{} a reproducible research
project.

\end{tcolorbox}

\hypertarget{questions-3}{%
\section*{Questions}\label{questions-3}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  What are the key characteristics of good research as described in this
  chapter?
\item
  What are some strategies researchers can use to identify potential
  research areas and problems to investigate?
\item
  For each strategy, describe how it contributes to research that is
  purposive, inquisitive, and informed.
\item
  Why is framing a clear, focused research question or hypothesis
  important? Briefly explain how the research question guides the
  overall research process.
\item
  What is the difference between the unit of analysis and the unit of
  observation? How do these concepts relate to the research question?
\item
  What does it mean to operationalize a variable? Why is this important?
\item
  The process of developing a research blueprint involves both
  conceptual planning and practical implementation steps. Explain how
  going through this process not only aids the individual researcher,
  but also the research community.
\item
  Describe the main aspects of developing a research plan.
\item
  Explain why it is important for research to be methodological and
  reproducible. What are some challenges researchers may face in
  achieving this?
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{wrench} Technical exercises}

\begin{itemize}
\tightlist
\item
  Matching research questions with data sources
\item
  Matching research questions with research plans
\item
  Preregistering a research project (?)
\item
  Propose a quantitative research topic (or question if possible).
  Support your topic with supporting literature. (?)
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\part{Preparation}

At this point we begin our journey to implement the research blueprint.
As such, the content will be more focused on the practical steps to
bring a plan to fruition integrating our conceptual understanding of the
research process from the previous chapters with our emerging
programming skills developed in lessons, recipes, and labs.

This part, Preparation, will address data acquistion, curation, and
transformation steps. The goal of data preparation is to create a
dataset which is ready for analysis. In each of these three upcoming
chapters, I will outline some of the main characteristics to consider in
each of these research steps and provide authentic examples of working
with R to implement these steps. In
\protect\hyperlink{sec-acquire-data}{Chapter 5} this includes the most
common strategies for acquiring data: downloads and APIs. In
\protect\hyperlink{sec-curate-data}{Chapter 6} we turn to organize data
into rectangular, or `tidy', format. Depending on the data or dataset
acquired for the research project, the steps necessary to shape our data
into a base dataset will vary, as we will see. In
\protect\hyperlink{sec-transform-data}{Chapter 7} we will work to
manipulate curated datasets to create datasets which are aligned with
the research aim and research question. This often includes normalizing
values, recoding variables, and generating new variables as well as and
sourcing and merging information from other datasets with the dataset to
be submitted for analysis.

Each of these chapters will cover the necessary documentation to trace
our steps and provide a record of the data preparation process.
Documentation serves to inform the analysis and interpretation of the
results and also forms the cornerstone of reproducible research.

\hypertarget{sec-acquire-data}{%
\chapter{Acquire data}\label{sec-acquire-data}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
The scariest moment is always just before you start.

-- Stephen King
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify common strategies for acquiring corpus data.
\item
  Describe how to organize and document data acquisition to support
  reproducibility.
\item
  Recall R programming concepts and strategies relevant to acquiring
  data.
\end{itemize}

\end{tcolorbox}

As we start down the path to executing our research blueprint, our first
step is to acquire the primary data that will be employed in the
project. This chapter covers three widely-used strategies for acquiring
corpus data: downloads and APIs (Application Programming Interfaces). We
will encounter various file formats and folder structures in the process
and we will address how to effectively organize our data for subsequent
processing. Crucial to our efforts is the process of documenting our
data. We will learn to provide data origin information to ensure key
characteristics of the data and its source are documented.

Along the way we will explore R coding concepts including control
statements and custom functions relevant to the task of acquiring data.
By the end of this chapter, you will not only be adept at acquiring data
from diverse sources but also capable of documenting it comprehensively,
enabling you to replicate the process in the future.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Control
Statements, Custom Functions, Vectorization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To recognize the logic behind code that can make dynamic
choices and to recall how functions serve to produce efficient,
reusable, and more legible code.

\end{tcolorbox}

\hypertarget{downloads}{%
\section{Downloads}\label{downloads}}

The most common and straightforward method for acquiring corpus data is
through direct downloads. In a nutshell, this method involves navigating
to a website, locating the data, and downloading it to your computing
environment. In some cases access to the data requires manual
intervention and in others the process can be implemented
programmatically. The data may be contained in a single file or multiple
files. The files may be compressed or uncompressed. The data may be
hierarchically organized or not. Each resource will have its own unique
characteristics that will influence the process of acquiring the data.
In this section we will work through a few examples to demonstrate the
general process of acquiring data through downloads.

\hypertarget{manual}{%
\subsection{Manual}\label{manual}}

In contrast to the other data acquisition methods we will cover in this
chapter, \textbf{manual downloads} require human intervention. This
means that manual downloads are non-reproducible in a strict sense and
require that we keep track of and document our procedure. It is a very
common for research projects to acquire data through manual downloads as
many data resources require some legwork before they are accessible for
downloading. These can be resources that require institutional or
private licensing and fees (\href{https://www.ldc.upenn.edu/}{Language
Data Consortium}, \href{http://ice-corpora.net/ice/}{International
Corpus of English}, \href{https://www.corpusdata.org/}{BYU Corpora},
\emph{etc.}), require authorization/ registration
(\href{https://archive.mpi.nl/tla/}{The Language Archive},
\href{https://www.webcorpora.org/}{COW Corpora}, \emph{etc.}), and/ or
are only accessible via resource search interfaces
(\href{https://cesa.arizona.edu/}{Corpus of Spanish in Southern
Arizona}, \href{http://cedel2.learnercorpora.com/}{Corpus Escrito del
Espaol como L2 (CEDEL2)}, \emph{etc.}).

Let's take a look at how to acquire data from a resource that requires
manual intervention. The resource we will use is the
\href{http://cedel2.learnercorpora.com/}{Corpus Escrito del Espaol como
L2 (CEDEL2)} (\protect\hyperlink{ref-Lozano2009}{Lozano 2009}), a corpus
of Spanish learner writing. It includes L2 writing from students with a
variety of L1 backgrounds. For comparative puposes it also includes
native writing for Spanish, English, and several other languages.

The CEDEL2 corpus is a freely available resource, but to access the data
you must first use a search interface to select the relevant
characteristics of the data of interest. Following the search/ download
link you can find a search interface that allows the user to select the
subcorpus and filter the results by a set of attributes, seen in
Figure~\ref{fig-ad-cedel2-search}.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures/ad-cedel2-search.png}

}

\caption{\label{fig-ad-cedel2-search}Search and download interface for
the CEDEL2 Corpus}

\end{figure}

For this example let's assume that we want to acquire data to use in a
study comparing the use of the Spanish preterite and imperfect past
tense aspect in written texts by English L1 learners of Spanish to
native Spanish speakers. To acquire data for such a project, we will
first select the subcorpus ``Learners of L2 Spanish''. We will set the
results to provide full texts and filter the results to ``L1 English -
L2 Spanish''. Additionally, we will set the medium to ``Written''. This
will provide us with a set of texts for the L2 learners that we can use
for our study. The search parameters and results are shown in
Figure~\ref{fig-ad-cedel2-results}.

\begin{figure}[H]

{\centering \includegraphics[width=1\textwidth,height=\textheight]{figures/ad-cedel2-results.png}

}

\caption{\label{fig-ad-cedel2-results}Search results for the CEDEL2
Corpus}

\end{figure}

The `Download' link now appears for this search criteria. Following this
link will provide the user a form to fill out. This particular resource
allows for access to different formats to download (Texts only, Texts
with metadata, CSV (Excel), CSV (Others)). I will select the `CSV
(Others)' option so that the data is structured for easier processing
downstream when we work to curate the data in our next processing step.
Then I will choose to save the CSV in the \emph{data/original/}
directory of my project and create a sub-directory named \emph{cedel2/},
as seen in Example~\ref{exm-ad-cedel2-learners-download}.

\begin{example}[]\protect\hypertarget{exm-ad-cedel2-learners-download}{}\label{exm-ad-cedel2-learners-download}

Download CEDEL2 L2 Spanish Learners data

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ cedel2/}
       \ExtensionTok{}\NormalTok{ cedel2{-}l1{-}english{-}learners.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

Note that the file is named \emph{cedel2-l1-english-learners.csv} to
reflect the search criteria used to acquire the data. In combination
with other data documentation, this will help us to maintain
transparency.

Now, after downloading the L2 learner and the native speaker data into
the appropriate directory, we move on to the next processing step,
right? Not so fast! Imagine we are working on a project with a
collaborator. How will they know where the data came from? What if we
need to come back to this data in the future? How will we know what
characteristics we used to filter the data? The directory and filenames
may not be enough. To address these questions we need to document the
origin of the data, and in the case of data acquired through manual
downloads, we need to document the procedures we took to acquire the
data to the best of our ability.

As discussed in Section~\ref{sec-ud-data-origin}, all acquired data
should be accompanied by a data origin file. The majority of this
information can typically be identified on the resource's website and/
or the resource's documentation. In the case of the CEDEL2 corpus, the
corpus homepage provides most of the information we need.

Structurally, data documentation files should be stored close to the
data they describe. So for our data origin file this means adding it to
the \emph{data/original/} directory. Naming the file in a transparent
way is also important. I've named the file \emph{cedel2\_do.csv} to
reflect the name of the corpus, the meaning of the file as data origin
with *\_do\emph{, and the file extension }.csv* to reflect the file
format. CSV files reflect tabular content. It is not required that data
origin files are tabular, but it makes it easier to read and display
them in literate programming documents.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

There are many ways to create and edit CSV files. You can use a
spreadsheet program like MS Excel or Google Sheets, a text editor like
Notepad or TextEdit, or a code editor like RStudio or VS Code. The
\texttt{qtalrkit} package provides a convenient function,
\texttt{create\_data\_origin()} to create a CSV file with the data
origin boilerplate structure. This CSV file then can be edited to add
the relevant information in any of the above mentioned programs.

Using a spreadsheet program is the easiest method for editing tabular
data. The key is to save the file as a CSV file, and not as an Excel
file, to maintain our adherence to the principle of using open formats
for reproducible research.

\end{tcolorbox}

In Table~\ref{tbl-ad-cedel2-do}, I've created a data origin file for the
CEDEL2 corpus.

\hypertarget{tbl-ad-cedel2-do}{}
\begin{table}
\caption{\label{tbl-ad-cedel2-do}Data origin file for the CEDEL2 corpus }\tabularnewline

\centering
\begin{tabular}{ll}
\toprule
attribute & description\\
\midrule
Resource name & CEDEL2: Corpus Escrito del Espaol como L2.\\
Data source & http://cedel2.learnercorpora.com/, https://doi.org/10.1177/02676583211050522\\
Data sampling frame & Corpus that contains samples of the language produced from learners of Spanish as a second language. For comparative purposes, it also contains a native control subcorpus of the language produced by native speakers of Spanish from different varieties (peninsular Spanish and all varieties of Latin American Spanish), so it can be used as a native corpus in its own right.\\
Data collection date(s) & 2006-2020.\\
Data format & CSV file. Each row corresponds to a writing sample. Each column is an attribute of the writing sample.\\
\addlinespace
Data schema & A CSV file for L2 learners and a CSV file for native speakers.\\
License & CC BY-NC-ND 3.0 ES\\
Attribution & Lozano, C. (2022). CEDEL2: Design, compilation and web interface of an online corpus for L2 Spanish acquisition research. Second Language Research, 38(4), 965-983. https://doi.org/10.1177/02676583211050522.\\
\bottomrule
\end{tabular}
\end{table}

Given this is a manual download we also need to document the procedure
used to retrieve the data in prose. The script in the \emph{code/}
directory that is typically used to acquire the data is not used to
programmatically retrieve data in this case. However, to keep things
predictable we will use this file to document the download procedure.
I've created a Quarto file named \emph{1\_acquire\_data.qmd} in the
\emph{code/} directory of my project.

A glimpse at the directory structure of the project at this point is
seen in Example~\ref{exm-ad-cedel2-structure}.

\begin{example}[]\protect\hypertarget{exm-ad-cedel2-structure}{}\label{exm-ad-cedel2-structure}

Project structure for the CEDEL2 corpus data acquisition

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{}\NormalTok{ code/}
\ExtensionTok{}\NormalTok{    1\_acquire\_data.qmd}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    analysis/}
\ExtensionTok{}\NormalTok{    derived/}
\ExtensionTok{}\NormalTok{    original/}
\ExtensionTok{}\NormalTok{        cedel2\_do.csv}
\ExtensionTok{}\NormalTok{        cedel2/}
\KeywordTok{|}           \ExtensionTok{}\NormalTok{ cedel2{-}l1{-}english{-}learners.csv}
\KeywordTok{|}           \ExtensionTok{}\NormalTok{ cedel2{-}native{-}spanish{-}speakers.csv}
\ExtensionTok{}\NormalTok{ output/}
\ExtensionTok{}\NormalTok{    figures/}
\ExtensionTok{}\NormalTok{    reports/}
\ExtensionTok{}\NormalTok{    results/}
\ExtensionTok{}\NormalTok{    tables/}
\ExtensionTok{}\NormalTok{ README.md}
\ExtensionTok{}\NormalTok{ \_main.R}
\end{Highlighting}
\end{Shaded}

\end{example}

In the \emph{1\_acquire\_data.qmd} file I've added example sections to
display the data origin CSV file as a table and to document the data
download procedures, as seen in
File~\ref{lst-ad-cedel2-acquire-data-qmd}.

\begin{codelisting}

\caption{\texttt{1-acquire-data.qmd}: Acquire data file}

\hypertarget{lst-ad-cedel2-acquire-data-qmd}{%
\label{lst-ad-cedel2-acquire-data-qmd}}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-}{-}}
\AnnotationTok{title:}\CommentTok{ "Acquire data"}
\AnnotationTok{format:}\CommentTok{ html}
\CommentTok{{-}{-}{-}}

\FunctionTok{\#\# Overview}
\NormalTok{The goal of this script is to acquire and document data for this project from the CEDEL2 corpus. The acquired data will be stored in the }\InformationTok{\textasciigrave{}data/original/cedel2/\textasciigrave{}}\NormalTok{ directory.}

\FunctionTok{\#\# Data origin}
\NormalTok{To document the origin of the data we created a file named }\InformationTok{\textasciigrave{}cedel2\_do.csv\textasciigrave{}}\NormalTok{ in the }\InformationTok{\textasciigrave{}data/original/\textasciigrave{}}\NormalTok{ directory. This file contains the following information: }

\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: tbl{-}cedel2{-}data{-}origin}
\CommentTok{\#| tbl{-}cap: "Data origin file for the CEDEL2 corpus"}
\CommentTok{\#| echo: false}

\CommentTok{\# Display data origin file}
\NormalTok{readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/original/cedel2\_do.csv"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}\NormalTok{ knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{()}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\FunctionTok{\#\# Download procedures  }
\NormalTok{The process to acquire data from the CEDEL2 corpus involved the following steps:}

\NormalTok{L2 Spanish Learners:  }
\SpecialStringTok{1. }\NormalTok{Navigate to the }\CommentTok{[}\OtherTok{CEDEL2 Corpus}\CommentTok{](http://cedel2.learnercorpora.com/search)}\NormalTok{ search interface}
\SpecialStringTok{2. }\NormalTok{Select the subcorpus "Learners of L2 Spanish"}
\SpecialStringTok{3. }\NormalTok{Set the results to provide full texts}
\SpecialStringTok{4. }\NormalTok{Filter the results to "L1 English {-} L2 Spanish"}
\SpecialStringTok{5. }\NormalTok{Set the medium to "Written"}
\SpecialStringTok{6. }\NormalTok{Download the data in CSV format}
\SpecialStringTok{7. }\NormalTok{Save the CSV file to the }\InformationTok{\textasciigrave{}data/original/cedel2/\textasciigrave{}}\NormalTok{ directory as }\InformationTok{\textasciigrave{}cedel2{-}l1{-}english{-}learners.csv\textasciigrave{}}

\NormalTok{Spanish natives: }
\end{Highlighting}
\end{Shaded}

\end{codelisting}

The output from \emph{1\_acquire\_data.qmd} will contain a table
displaying the data origin file and a prose section documenting the data
acquisition process. This will provide a transparent record of the data
acquisition process for future reference.

Manually downloading other resources will inevitably include unique
processes for obtaining the data, but in the end the data should be
archived in the research structure in the \emph{data/original/}
directory and documented in the appropriate places. The acquired data is
treated as `read-only', meaning it is not modified in any way. This
gives us a transparent starting point for subsequent steps in the data
preparation process.

\hypertarget{programmatic}{%
\subsection{Programmatic}\label{programmatic}}

There are many resources that provide corpus data that is directly
accessible for which programmatic downloads can be applied. A
\textbf{programmatic download} is a download in which the process can be
automated through code. Thus, this is a reproducible process. The data
can be acquired by anyone with access to the necessary code.

In this case, and subsquent data acquisition procedures in this chapter,
we use the \emph{1\_acquire\_data.qmd} Quarto file to its full potential
intermingling prose, code, and code comments to execute and document the
download procedure. In File~\ref{lst-ad-swda-acquire-data-qmd}, I've
added example sections to display example boilerplate structure for a
programmatic data acquisition and documentation.

\begin{codelisting}

\caption{\texttt{1-acquire-data.qmd}: Acquire data file}

\hypertarget{lst-ad-swda-acquire-data-qmd}{%
\label{lst-ad-swda-acquire-data-qmd}}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-}{-}}
\AnnotationTok{title:}\CommentTok{ "Acquire data"}
\AnnotationTok{format:}\CommentTok{ html}
\CommentTok{{-}{-}{-}}

\FunctionTok{\#\# Overview}

\NormalTok{The goal of this script is to ...}

\FunctionTok{\#\# Data origin}

\NormalTok{To document the origin of the data we created a file named ...}

\FunctionTok{\#\# Download procedures}

\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\#| label: setup}

\CommentTok{\# Load libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}\{r\}}
\CommentTok{\# .. additional code here to acquire data ...}
\InformationTok{\textasciigrave{}\textasciigrave{}\textasciigrave{}}
\NormalTok{... and so on}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

To illustrate how this works to conduct a programmatic download, we will
work with the Switchboard Dialog Act Corpus (SWDA)
(\protect\hyperlink{ref-SWDA2008}{University of Colorado Boulder 2008}).
The version that we will use is found on the Linguistic Data Consortium
under the \href{https://catalog.ldc.upenn.edu/LDC97S62}{Switchboard-1
Release 2 Corpus}. The corpus and related documentation are linked on
the catalog page \url{https://catalog.ldc.upenn.edu/docs/LDC97S62/}.

From the documentation we learn that the corpus contains transcripts for
1155 5-minute two-way telephone conversations among English speakers for
all areas of the United States. The speakers were given a topic to
discuss and the conversations were recorded. The corpus metadata and
annotations for sociolinguistic and discourse features.

The SWDA was referred to in Section~\ref{sec-aa-infer} to support our
toy hypothesis that men and women differ in the frequency of the use of
questions in spontaneous conversations. This corpus, as you can image,
could support a wide range of interesting reseach questions. Let's
assume we are following research conducted by Tottie
(\protect\hyperlink{ref-Tottie2011}{2011}) to explore the use of filled
pauses such as ``um'' and ``uh'' and traditional sociolinguistic
variables such as sex, age, and education in spontaneous speech by
American English speakers.

With this goal in mind, let's get started writing the code to download
and organize the data in our project directory. First we need to
identify the URL (Uniform Resource Locator) for the data that we want to
download. More often than not this file will be some type of compressed
archive file with an extension such as \emph{.zip} (Zipped file),
\emph{.tar} (Tarball file), or \emph{tar.gz} (Gzipped tarball file),
which is the case for the SWDA corpus. Compressed files make downloading
multiple files easy by grouping files and directories into one file.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

You may be wondering what the difference betwen \emph{.zip},
\emph{.tar}, and \emph{.tar.gz} files are. The \emph{.zip} file format
is the most common. It groups file and directories into one file
(archives) and compresses them to reduce the size of the file in one
step when the file is created.

The \emph{.tar} file format is used archive files and folders, it does
not perform compression. Gzipping peforms the compression to the
\emph{.tar} file resulting in a file with the \emph{.tar.gz} extension.
Notably the \emph{.gz} compression is highly efficient for large files.
Take the \emph{swda.tar.gz} file for example. It has a compressed file
size of 4.6 MB, but when uncompressed it is 16.9 MB. This is a 73\%
reduction in file size.

\end{tcolorbox}

In R we can use the \texttt{download.file()} function from base R. The
\texttt{download.file()} function minimally requires two arguments:
\texttt{url} and \texttt{destfile}. These correspond to the file to
download and the location where it is to be saved to disk. To break out
the process a bit, I will assign the URL and destination file path to
variables and then use the \texttt{download.file()} function to download
the file.

\begin{example}[]\protect\hypertarget{exm-ad-swda-download-file}{}\label{exm-ad-swda-download-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}} 
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Relative path to project/data/original directory}
\NormalTok{file\_path }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda.tar.gz"}

\CommentTok{\# Download SWDA corpus compressed file}
\FunctionTok{download.file}\NormalTok{(}\AttributeTok{url =}\NormalTok{ file\_url, }\AttributeTok{destfile =}\NormalTok{ file\_path)}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

Note that the \texttt{file\_path} variable in
Example~\ref{exm-ad-swda-download-file} is a relative path to the
\emph{data/original/} directory. This is because the
\emph{1\_acquire\_data.qmd} file that we are using for this code is
located in the \emph{code/} directory and the \emph{data/} directory is
a sibling directory to the \emph{code/} directory.

It is also possible to use an absolute path to the \emph{data/original/}
directory. I will have more to say about the advantages and
disadvantages of relative and absolute paths in reproducible research in
Chapter~\ref{sec-collaboration}.

\end{tcolorbox}

As we can see looking at the directory structure, in
Example~\ref{exm-ad-swda-download-location}, the \emph{swda.tar.zip}
file has been added to the \emph{data/original/} directory.

\begin{example}[]\protect\hypertarget{exm-ad-swda-download-location}{}\label{exm-ad-swda-download-location}

Downloaded SWDA corpus compressed file

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ swda.tar.zip}
\end{Highlighting}
\end{Shaded}

\end{example}

Once an compressed file is downloaded, however, the file needs to be
`decompressed' to reveal the directory structure and files. To
decompress this \emph{.tar.gz} file we use the \texttt{untar()} function
with the arguments \texttt{tarfile} pointing to the \emph{.tar.gz} file
and \texttt{exdir} specifying the directory where we want the files to
be extracted to. Again, I will assign the arguments to variables. Then
we can decompress the file using the \texttt{untar()} function.

\begin{example}[]\protect\hypertarget{exm-ad-swda-decompress-file}{}\label{exm-ad-swda-decompress-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Relative path to the compressed file}
\NormalTok{tar\_file }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda.tar.gz"}

\CommentTok{\# Relative path to the directory to extract to}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Decompress .zip file and extract to our target directory}
\FunctionTok{untar}\NormalTok{(tar\_file, extract\_to\_dir)}
\end{Highlighting}
\end{Shaded}

\end{example}

The directory structure of \emph{data/} in
Example~\ref{exm-ad-swda-decompress-location} now shows the
\emph{swda.tar.gz} file and the \emph{swda} directory that contains the
decompressed directories and files.

\begin{example}[]\protect\hypertarget{exm-ad-swda-decompress-location}{}\label{exm-ad-swda-decompress-location}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ swda/}
    \ExtensionTok{}\NormalTok{    README}
    \ExtensionTok{}\NormalTok{    doc/}
    \ExtensionTok{}\NormalTok{    sw00utt/}
    \ExtensionTok{}\NormalTok{    sw01utt/}
    \ExtensionTok{}\NormalTok{    sw02utt/}
    \ExtensionTok{}\NormalTok{    sw03utt/}
    \ExtensionTok{}\NormalTok{    sw04utt/}
    \ExtensionTok{}\NormalTok{    sw05utt/}
    \ExtensionTok{}\NormalTok{    sw06utt/}
    \ExtensionTok{}\NormalTok{    sw07utt/}
    \ExtensionTok{}\NormalTok{    sw08utt/}
    \ExtensionTok{}\NormalTok{    sw09utt/}
    \ExtensionTok{}\NormalTok{    sw10utt/}
    \ExtensionTok{}\NormalTok{    sw11utt/}
    \ExtensionTok{}\NormalTok{    sw12utt/}
    \ExtensionTok{}\NormalTok{    sw13utt/}
    \ExtensionTok{}\NormalTok{ swda.tar.gz}
\end{Highlighting}
\end{Shaded}

\end{example}

At this point we have acquired the data programmatically and with this
code as part of our workflow anyone could run this code and reproduce
the same results. The code as it is, however, is not ideally efficient.
Firstly the \emph{swda.tar.gz} file is not strictly needed after we
decompress it and it occupies disk space if we keep it. And second, each
time we run this code the file will be downloaded from the remote
server. This leads to unnecessary data transfer and server traffic and
will overwrite the data if it already exists in our project directory
which could be problematic if the data changes on the remote server.
Let's tackle each of these issues in turn.

To avoid writing the \emph{swda.tar.gz} file to disk (long-term) we can
use the \texttt{tempfile()} function to open a temporary holding space
for the file in the computing environment. This space can then be used
to store the file, decompress it, and then the temporary file will
automatically be deleted. We assign the temporary space to an R object
we will name \texttt{temp\_file} with the \texttt{tempfile()} function.
This object can now be used as the value of the argument
\texttt{destfile} in the \texttt{download.file()} function.

\begin{example}[]\protect\hypertarget{exm-ad-swda-temp-file}{}\label{exm-ad-swda-temp-file}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}} 
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Create a temporary file space for our .tar.gz file}
\NormalTok{temp\_file }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}

\CommentTok{\# Download SWDA corpus compressed file}
\FunctionTok{download.file}\NormalTok{(file\_url, temp\_file)}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

In Example~\ref{exm-ad-swda-temp-file}, I've used the values stored in
the objects \texttt{file\_url} and \texttt{temp\_file} in the
\texttt{download.file()} function without specifying the argument names
--only providing the names of the objects. R will assume that values of
a function map to the ordering of the arguments. If your values do not
map to ordering of the arguments you are required to specify the
argument name and the value. To view the ordering of objects hit
\texttt{tab} after entering the function name or consult the function
documentation by prefixing the function name with \texttt{?} and hitting
\texttt{enter}.

\end{tcolorbox}

At this point our downloaded file is stored temporarily on disk and can
be accessed and decompressed to our target directory using
\emph{temp\_file} as the value for the argument \texttt{tarfile} from
the \texttt{untar()} function. I've assigned our target directory path
to \texttt{extract\_to\_dir} and used it as the value for the argument
\texttt{exdir}.

\begin{example}[]\protect\hypertarget{exm-ad-swda-untar-temp}{}\label{exm-ad-swda-untar-temp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Assign our target directory to \textasciigrave{}extract\_to\_dir\textasciigrave{}}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Decompress .tar.gz file and extract to our target directory}
\FunctionTok{untar}\NormalTok{(}\AttributeTok{tarfile =}\NormalTok{ temp\_file, }\AttributeTok{exdir =}\NormalTok{ target\_dir)}
\end{Highlighting}
\end{Shaded}

\end{example}

Our directory structure in Example~\ref{exm-ad-swda-untar-temp} is the
same as in Example~\ref{exm-ad-swda-decompress-location}, minus the
\emph{swda.tar.gz} file.

The second issue I raised concerns the fact that running this code as
part of our project will repeat the download each time. Since we would
like to be good citizens and avoid unnecessary traffic on the web and
avoid potential issues in overwriting data, it would be nice if our code
checked to see if we already have the data on disk and if it exists,
then skip the download, if not then download it.

The desired functionality we've described can be implemented using the
\texttt{if()} function. The \texttt{if()} function is one of a class of
functions known as control statements. \textbf{Control statments} allow
us to control the flow of our code by evaluating logical statements and
processing subsequent code based on the logical value it is passed as an
argument.

So in this case we want to evaluate whether the data directory exists on
disk. If it does then skip the download, if not, proceed with the
download. In combination with \texttt{else} which provides the `if not'
part of the statement, we have the following logical flow in
Example~\ref{exm-ad-if-dir-exists}.

\begin{example}[]\protect\hypertarget{exm-ad-if-dir-exists}{}\label{exm-ad-if-dir-exists}

~

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (DIRECTORY\_EXISTS) \{}
  \CommentTok{\# Do nothing}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \CommentTok{\# Download data}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

We can simplify this statement by using the \texttt{!} operator which
negates the logical value of the statement it precedes. So if the
directory exists, \texttt{!DIRECTORY\_EXISTS} will return \texttt{FALSE}
and if the directory does not exist, \texttt{!DIRECTORY\_EXISTS} will
return \texttt{TRUE}. In other words, if the directory does not exist,
download the data. This is shown in
Example~\ref{exm-ad-if-dir-exists-simplified}.

\begin{example}[]\protect\hypertarget{exm-ad-if-dir-exists-simplified}{}\label{exm-ad-if-dir-exists-simplified}

~

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\NormalTok{DIRECTORY\_EXISTS) \{}
  \CommentTok{\# Download data}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Now, to determine if a directory exists in our project directory we will
turn to the \texttt{fs} package (\protect\hyperlink{ref-R-fs}{Hester,
Wickham, and Csrdi 2023}). The \texttt{fs} package provides a set of
functions for interacting with the file system, including
\texttt{dir\_exists()}. \texttt{dir\_exists()} takes a path to a
directory as an argument and returns the logical value, \texttt{TRUE},
if that directory exists, and \texttt{FALSE} if it does not.

We can use this function to evaluate whether the directory exists and
then use the \texttt{if()} function to process the subsequent code based
on the logical flow we set out in
Example~\ref{exm-ad-if-dir-exists-simplified}. Applied to our project,
the code will look like Example~\ref{exm-ad-swda-if-dir-exists}.

\begin{example}[]\protect\hypertarget{exm-ad-swda-if-dir-exists}{}\label{exm-ad-swda-if-dir-exists}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the \textasciigrave{}fs\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(fs)}

\CommentTok{\# URL to SWDA corpus compressed file}
\NormalTok{file\_url }\OtherTok{\textless{}{-}} 
  \StringTok{"https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1\_dialogact\_annot.tar.gz"}

\CommentTok{\# Create a temporary file space for our .tar.gz file}
\NormalTok{temp\_file }\OtherTok{\textless{}{-}} \FunctionTok{tempfile}\NormalTok{()}

\CommentTok{\# Assign our target directory to \textasciigrave{}extract\_to\_dir\textasciigrave{}}
\NormalTok{extract\_to\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/original/swda/"}

\CommentTok{\# Check if our target directory exists}
\CommentTok{\# If it does not exist, download the file and extract it}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{dir\_exists}\NormalTok{(extract\_to\_dir)) \{}
  \CommentTok{\# Download SWDA corpus compressed file}
  \FunctionTok{download.file}\NormalTok{(file\_url, temp\_file)}
  
  \CommentTok{\# Decompress .tar.gz file and extract to our target directory}
  \FunctionTok{untar}\NormalTok{(}\AttributeTok{tarfile =}\NormalTok{ temp\_file, }\AttributeTok{exdir =}\NormalTok{ extract\_to\_dir)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

The code in Example~\ref{exm-ad-swda-if-dir-exists} is added to the
\emph{1\_acquire\_data.qmd} file we introduced in
File~\ref{lst-ad-swda-acquire-data-qmd}. When this file is run, the SWDA
corpus data will be downloaded and extracted to our project directory.
If the data already exists, the download will be skipped, just as we
wanted.

Before we move on, we need to make sure to create and add the
appropriate information to the data origin file. To make this easier,
the \texttt{qtalrkit} package includes a function,
\texttt{create\_data\_origin()}, to create a data origin file template
in CSV format. This function takes the path for the desired file. In the
SWDA Corpus case, this might be something like:
\emph{../data/original/swda\_do.csv}. The function only needs to be run
once and does not need to be part of the reproducible workflow.

Running the code in Example~\ref{exm-ad-swda-create-do} at the console
will create the file. Open it in your preferred text or spreadsheet
editor to add the appropriate information.

\begin{example}[]\protect\hypertarget{exm-ad-swda-create-do}{}\label{exm-ad-swda-create-do}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the \textasciigrave{}qtalrkit\textasciigrave{} package}
\FunctionTok{library}\NormalTok{(qtalrkit)}

\CommentTok{\# Create a data origin file template}
\FunctionTok{create\_data\_origin}\NormalTok{(}\StringTok{"../data/original/swda\_do.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Our complete project structure for the SWDA corpus data acquisition is
shown in Example~\ref{exm-ad-swda-structure}.

\begin{example}[]\protect\hypertarget{exm-ad-swda-structure}{}\label{exm-ad-swda-structure}

Project structure for the SWDA corpus data acquisition

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{}\NormalTok{ code/}
\ExtensionTok{}\NormalTok{    1\_acquire\_data.qmd}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    analysis/}
\ExtensionTok{}\NormalTok{    derived/}
\ExtensionTok{}\NormalTok{    original/}
\ExtensionTok{}\NormalTok{        swda\_do.csv}
\ExtensionTok{}\NormalTok{        swda/}
\ExtensionTok{}\NormalTok{           README}
\ExtensionTok{}\NormalTok{           doc/}
\ExtensionTok{}\NormalTok{           sw00utt/}
\ExtensionTok{}\NormalTok{           sw01utt/}
\ExtensionTok{}\NormalTok{           sw02utt/}
\ExtensionTok{}\NormalTok{           sw03utt/}
\ExtensionTok{}\NormalTok{           sw04utt/}
\ExtensionTok{}\NormalTok{           sw05utt/}
\ExtensionTok{}\NormalTok{           sw06utt/}
\ExtensionTok{}\NormalTok{           sw07utt/}
\ExtensionTok{}\NormalTok{           sw08utt/}
\ExtensionTok{}\NormalTok{           sw09utt/}
\ExtensionTok{}\NormalTok{           sw10utt/}
\ExtensionTok{}\NormalTok{           sw11utt/}
\ExtensionTok{}\NormalTok{           sw12utt/}
\ExtensionTok{}\NormalTok{           sw13utt/}
\ExtensionTok{}\NormalTok{ output/}
\ExtensionTok{}\NormalTok{    figures/}
\ExtensionTok{}\NormalTok{    reports/}
\ExtensionTok{}\NormalTok{    results/}
\ExtensionTok{}\NormalTok{    tables/}
\ExtensionTok{}\NormalTok{ README.md}
\ExtensionTok{}\NormalTok{ \_main.R}
\end{Highlighting}
\end{Shaded}

\end{example}

Great, we've successfully acquired and documented the SWDA Corpus data.
We've leveraged R to automate the download and extraction of the data,
depending on the existence of the data in our project directory. But you
may be asking yourself, ``Can't I just navigate to the corpus page and
download the data manually myself?'' The simple answer is, ``Yes, you
can.'' The more nuanced answer is, ``Yes, but consider the trade-offs.''

The following scenarios highlight the some advantages to automating the
process. If you are acquiring data from multiple files, it can become
tedious to document the manual process for each file such that it is
reproducible. It's possible, but it's error prone. Now, if you are
collaborating with others, you will want to share this data with them.
It is very common to find data that has limited restrictions for use in
academic projects, but the most common limitation is redistribution.
This means that you can use the data for your own research, but you
cannot share it with others. If you plan on publishing your project to a
repository, like GitHub, to share the data as part of your reproducible
project, you would be violating the terms of use for the data. By
including the programmatic download in your project, you can ensure that
your collaborators can easily and effectively acquire the data
themselves and that you are not violating the terms of use.

\hypertarget{sec-apis}{%
\section{APIs}\label{sec-apis}}

A convenient alternative method for acquiring data in R is through
package interfaces to web services. These interfaces are built using R
code to make connections with resources on the web through
\textbf{Application Programming Interfaces} (APIs). Websites such as
Project Gutenberg, Twitter, Facebook, and many others provide APIs to
allow access to their data under certain conditions, some more limiting
for data collection than others. Programmers (like you!) in the R
community take up the task of wrapping calls to an API with R code to
make accessing that data from R convenient, and of course reproducible.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

Many, many web services provide API access. These APIs span all kinds of
data, from text to images to video to audio. Visit the
\href{https://publicapis.io/}{Public APIs website} to explore the
diversity of APIs available.

ROpenSci maintains a curated list of R packages that provide access to
data from web services. Visit the
\href{https://ropensci.org/packages/data-access/}{ROpenSci website} to
explore the packages available.

\end{tcolorbox}

In addition to popular public APIs, there are also APIs that provide
access to repositories and databases which are of particular interest to
linguists. For example, \href{http://wordbank.stanford.edu/}{Wordbank}
provides access to a large collection of child language corpora through
the \texttt{wordbankr} package
(\protect\hyperlink{ref-R-wordbankr}{Braginsky 2022}), and
\href{https://glottolog.org/}{Glottolog},
\href{https://wals.info/}{World Atlas of Language Structures} (WALS),
and \href{https://phoible.org/}{PHOIBLE} provide access to large
collections of language metadata that can be accessed through the
\texttt{lingtypology} package
(\protect\hyperlink{ref-R-lingtypology}{Moroz 2023}).

Let's work with an R package that provides access to the
\href{https://talkbank.org/}{TalkBank} database. The TalkBank project
\faIcon{wrench} {[}CITATION{]} contains a large collection of spoken
language corpora from various contexts: conversation, child language,
multilinguals, \emph{etc}. Resource information, web interfaces, and
links to download data in various formats can be found by perusing
individual resources linked from the main page. However, the
\texttt{TBDBr} package (\protect\hyperlink{ref-R-TBDBr}{Kowalski and
Cavanaugh 2022}) provides convenient access to data using R once a data
resource is identified.

The CABNC (\protect\hyperlink{ref-Albert2015}{Albert, de Ruiter, and de
Ruiter 2015}) contains the
\href{http://www.natcorp.ox.ac.uk/docs/URG/BNCdes.html\#body.1_div.1_div.5_div.1}{demographically
sampled portion} of the spoken portion of the British National Corpus
(BNC) (\protect\hyperlink{ref-Leech1992}{Leech 1992}).

Useful for a study aiming to research spoken British English, either in
isoloation or in comparison to American English (SWDA).

First, we need to install and load the \texttt{TBDBr} package. The
\texttt{install.package()} and \texttt{library()} functions work just
great for this. An alternative is to use the \texttt{pacman} package
(\protect\hyperlink{ref-R-pacman}{Rinker and Kurkiewicz 2019}) to
install and load the package in one step with the function
\texttt{p\_load()}, as shown in
Example~\ref{exm-ad-install-load-pacman}.

\begin{example}[]\protect\hypertarget{exm-ad-install-load-pacman}{}\label{exm-ad-install-load-pacman}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install and load the TBDBr package}
\NormalTok{pacman}\SpecialCharTok{::}\FunctionTok{p\_load}\NormalTok{(TBDBr)}
\end{Highlighting}
\end{Shaded}

\end{example}

The \texttt{TBDBr} package provides a set of common \texttt{get*()}
functions for acquiring data from the TalkBank corpus resources. These
include:

\begin{itemize}
\tightlist
\item
  \texttt{getParticipants()}
\item
  \texttt{getTranscripts()}
\item
  \texttt{getTokens()}
\item
  \texttt{getTokenTypes()}
\item
  \texttt{getUtterances()}
\end{itemize}

For each of these function the first argument is \texttt{corpusName},
which is the name of the corpus resource as it appears in the TalkBank
database. The second argument is \texttt{corpora}, which takes a
character vector describing the path to the data. For the CABNC, these
arguments are \texttt{"ca"} and \texttt{c("ca",\ "CABNC")} respectively.
To determine these values, TBDBr provides the \texttt{getLegalValues()}
interactive function which allows you to interactively select the
repository name, corpus name, and transcript name (if necessary).

Another important aspect of these function is that they return data
frame objects. Since we are accessing data that is in a structured
database, this makes sense. However, we should always check the
documentation for the object type that is returned by function to be
aware of how to work with the data.

Let's start by retrieving the utterance data for the CABNC and preview
the data frame it returns using \texttt{glimpse()}.

\begin{example}[]\protect\hypertarget{exm-ad-get-utterances}{}\label{exm-ad-get-utterances}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set corpus\_name and corpus\_path}
\NormalTok{corpus\_name }\OtherTok{\textless{}{-}} \StringTok{"ca"}
\NormalTok{corpus\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\StringTok{"CABNC"}\NormalTok{)}

\CommentTok{\# Get utterance data}
\NormalTok{utterances }\OtherTok{\textless{}{-}} 
  \FunctionTok{getUtterances}\NormalTok{(}
    \AttributeTok{corpusName =}\NormalTok{ corpus\_name, }
    \AttributeTok{corpora =}\NormalTok{ corpus\_path)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "Fetching data, please wait..."
> [1] "Success!"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the data}
\FunctionTok{glimpse}\NormalTok{(utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <list> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000",~
> $ path      <list> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC~
> $ utt_num   <list> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
> $ who       <list> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS00~
> $ role      <list> "Unidentified", "Unidentified", "Unidentified", "Unidentifi~
> $ postcodes <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NU~
> $ gems      <list> <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NULL>, <NU~
> $ utterance <list> "You enjoyed yourself in America", "Eh", "did you", "Oh I c~
> $ startTime <list> "0.208", "2.656", "2.896", "3.328", "5.088", "6.208", "8.32~
> $ endTime   <list> "2.672", "2.896", "3.328", "5.264", "6.016", "8.496", "9.31~
\end{verbatim}

\end{example}

Inspecting the output from Example~\ref{exm-ad-get-utterances}, we see
that the data frame contains 235901 observations and 10 variables.

The summary provided by \texttt{glimpse()} also provides other useful
information. First, we see the data type of each variable.
Interestingly, the data type for each variable in the data frame is
\texttt{list}. Being that a list is two-dimensional data type, like a
data frame, we have list-type data in each value. This is known as a
\textbf{nested structure}. We will see, and create, nested structures
later, but for now it will suffice to say that we would like to `unnest'
these lists and reveal the list-contained vector types at the data frame
level.

To do this we will pass the \texttt{utterances} data frame to the
\texttt{unnest()} function from the \texttt{tidyr} package
(\protect\hyperlink{ref-R-tidyr}{Wickham, Vaughan, and Girlich 2023}).
\texttt{unnest()} takes a data frame and a vector of variable names to
unnest, \texttt{cols\ =\ c()}. To unnest all variables, we will use the
\texttt{everything()} function from \texttt{dplyr}
(\protect\hyperlink{ref-R-dplyr}{Wickham, Franois, et al. 2023}) to
select all variables. We will use the result to overwrite the
\texttt{utterances} object with the unnested data frame.

\begin{example}[]\protect\hypertarget{exm-ad-unnest}{}\label{exm-ad-unnest}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unnest the data frame}
\NormalTok{utterances }\OtherTok{\textless{}{-}} 
\NormalTok{  utterances }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{())}

\CommentTok{\# Preview the data}
\FunctionTok{glimpse}\NormalTok{(utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", ~
> $ path      <chr> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC/~
> $ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
> $ who       <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS002~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ postcodes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ gems      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ utterance <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh I co~
> $ startTime <chr> "0.208", "2.656", "2.896", "3.328", "5.088", "6.208", "8.32"~
> $ endTime   <chr> "2.672", "2.896", "3.328", "5.264", "6.016", "8.496", "9.312~
\end{verbatim}

\end{example}

The output from Example~\ref{exm-ad-unnest} shows that the variables are
now one-dimensional vector types.

Returning to the information about our data frame from
\texttt{glimpse()}, the second thing to notice is we get a short preview
of the values for each variable. There are a couple things we can gleen
from this. One is that we can confirm or clarify the meaning of the
variable names by looking at the values. The other thing to consider is
whether the values show any patterns that may be worthy of more
scrutiny. For example, various variables appear to contain the same
values for each observation. For a variable like \texttt{filename}, this
is expected as the first values likely correspond to the same file.
However, for the variables \texttt{postcodes} and \texttt{gems} the
values are `NA'. This suggests that these variables may not contain any
useful information and we may want to remove them later.

For now, however, we want to acquire and store the data in its original
form (or as closely as possible). So now, we have acquired the
utterances data and have it in our R session as a data frame. To store
this data in a file, we will first need to consider the file format.
Data frames are tabular, so that gives us a few options. Since we are
working in R, we could store this data as an R object, in the form of an
RDS file. An RDS file is a binary file that can be read back into R as
an R object. This is a good option if we want to store the data for use
in R, but not if we want to share the data with others or use it in
other software. Another option is to store the data as a spreadsheet
file, such as XSLX (MS Excel). This may make viewing and editing the
contents more convenient, but it depends on the software available to
you and others. A third, more viable option, is to store the data as a
CSV file. CSV files are plain text files that can be read and written by
most software. This makes CSV files one of the most popular for sharing
tablular data. For this reason, we will store the data as a CSV file.

The \texttt{readr} package (\protect\hyperlink{ref-R-readr}{Wickham,
Hester, and Bryan 2023}) provides the \texttt{write\_csv()} function for
writing data frames to CSV files. The first argument is the data frame
to write, and the second argument is the path to the file to write.
Note, however, that the directories in the path we specify need to
exist. If they do not, we will get an error. In this case, I would like
to write the file \emph{utterances.csv} to the
\emph{../data/original/cabnc/} directory. The original project structure
does not contain a \emph{cabnc/} directory, so I need to create one. To
do this, I will use \texttt{dir\_create()} from the \texttt{fs} package
(\protect\hyperlink{ref-R-fs}{Hester, Wickham, and Csrdi 2023}).

\begin{example}[]\protect\hypertarget{exm-ad-write-csv}{}\label{exm-ad-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the target directory}
\FunctionTok{dir\_create}\NormalTok{(}\StringTok{"../data/original/cabnc/"}\NormalTok{)}

\CommentTok{\# Write the data frame to a CSV file}
\FunctionTok{write\_csv}\NormalTok{(utterances, }\StringTok{"../data/original/cabnc/utterances.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Chaining the steps covered in Examples \ref{exm-ad-get-utterances},
\ref{exm-ad-unnest}, and \ref{exm-ad-write-csv}, we have a succinct and
legible code to acquire, adjust, and write utterances from the CABNC in
Example~\ref{exm-ad-get-unnest-write-csv}.

\begin{example}[]\protect\hypertarget{exm-ad-get-unnest-write-csv}{}\label{exm-ad-get-unnest-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set corpus\_name and corpus\_path}
\NormalTok{corpus\_name }\OtherTok{\textless{}{-}} \StringTok{"ca"}
\NormalTok{corpus\_path }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"ca"}\NormalTok{, }\StringTok{"CABNC"}\NormalTok{)}

\CommentTok{\# Create the target directory}
\FunctionTok{dir\_create}\NormalTok{(}\StringTok{"../data/original/cabnc/"}\NormalTok{)}

\CommentTok{\# Get utterance data}
\FunctionTok{getUtterances}\NormalTok{(}
  \AttributeTok{corpusName =}\NormalTok{ corpus\_name,}
  \AttributeTok{corpora =}\NormalTok{ corpus\_path}
\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{write\_csv}\NormalTok{(}\StringTok{"../data/original/cabnc/utterances.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

If our goal is just to acquire utterances, then we are done acquiring
data and we move on to the next step. However, if we want to acquire
other datasets from the CABNC, say participants, tokens, \emph{etc.},
then we can either repeat the steps in
Example~\ref{exm-ad-get-unnest-write-csv} for each data type, or we can
write a function to do this for us. A function serves us to make our
code more legible and reusable for the CABNC, and since the TalkBank
data is structured similarly across corpora, we can also use the
function to acquire data from other corpora, if need be.

To write a function, we need to consider the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the name of the function?
\item
  What arguments does the function take?
\item
  What functionality does the function provide?
\item
  Does the function have optional arguments?
\item
  How does the function return the results?
\end{enumerate}

Taking each in turn, the name of the function should be descriptive of
what the function does. In this case, we are acquiring and writing data
from Talkbank corpora. A possible name is
\texttt{get\_talkbank\_data()}. The required arguments of the the
\texttt{get*()} functions will definitely figure in our function. In
addition, we will need to specify the path to the directory to write the
data.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-1}{}\label{exm-ad-get-talkbank-data-1}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}
  \CommentTok{\# ...}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

The next thing to consider is what functionality the function provides.
In this case, we want to acquire and write data from Talkbank corpora.
We can start by leveraging the code steps in
Example~\ref{exm-ad-get-unnest-write-csv}, making some adjustments to
the code replacing the hard-coded values with the function arguments and
adding code to create the target file name based on the
\texttt{target\_dir} argument.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-2}{}\label{exm-ad-get-talkbank-data-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}
  
  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}
  
  \CommentTok{\# Acquire data and write to file}
  \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Before we address the obvious feature missing, which is the fact that
this function in Example~\ref{exm-ad-get-talkbank-data-2} only acquires
and writes data for utterances, let's consider some functionality which
would make this function more user-friendly.

What if the data is already acquired? Do we want to overwrite it, or
should the function skip the process for files that already exist? By
skipping the process, we can save time and computing resources. If the
files are periodically updated, then we might want to overwrite existing
files. To achieve this functionality we will use an \texttt{if()}
statement to check if the file exists. If it does, then we will skip the
process. If it does not, then we will acquire and write the data.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-3}{}\label{exm-ad-get-talkbank-data-3}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir) \{}
  
  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}
  
  \CommentTok{\# If the file does not exist, then...}
  \CommentTok{\# Acquire data and write to file}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file\_exists}\NormalTok{(utterances\_file)) \{}
    \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

We can also add functionality to
Example~\ref{exm-ad-get-talkbank-data-3} to force overwrite existing
files, if need be. To do this, we will add an optional argument to the
function, \texttt{force}, which will be a logical value. We will set the
default to \texttt{force\ =\ FALSE} to preserve the existing
functionality. If \texttt{force\ =\ TRUE}, then we will overwrite
existing files. Then we add another condition to the \texttt{if()}
statement to check if \texttt{force\ =\ TRUE}. If it is, then we will
overwrite existing files.

\begin{example}[]\protect\hypertarget{exm-ad-get-talkbank-data-4}{}\label{exm-ad-get-talkbank-data-4}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_talkbank\_data }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(corpus\_name, corpus\_path, target\_dir, }\AttributeTok{force =} \ConstantTok{FALSE}\NormalTok{) \{}
  
  \CommentTok{\# Create the target directory}
  \FunctionTok{dir\_create}\NormalTok{(target\_dir)}

  \CommentTok{\# Set up file path name}
\NormalTok{  utterances\_file  }\OtherTok{\textless{}{-}} \FunctionTok{path}\NormalTok{(target\_dir, }\StringTok{"utterances.csv"}\NormalTok{)}
  
  \CommentTok{\# If the file does not exist, then...}
  \CommentTok{\# Acquire data and write to file}
  \ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{file\_exists}\NormalTok{(utterances\_file) }\SpecialCharTok{|}\NormalTok{ force) \{}
    \FunctionTok{getUtterances}\NormalTok{(}\AttributeTok{corpusName =}\NormalTok{ corpus\_name, }\AttributeTok{corpora =}\NormalTok{ corpus\_path) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
      \FunctionTok{write\_csv}\NormalTok{(utterances\_file)}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

From this point, we add the functionality to acquire and write the other
data available from Talkbank corpora, such as participants, tokens,
\emph{etc.} This involves adding additional file path names and
\texttt{if()} statements to check if the files exist surrounding the
processing steps to Example~\ref{exm-ad-get-talkbank-data-4}. It may be
helpful to perform other input checks, print messages, \emph{etc.} for
functions that we plan to share with others. I will leave these
enhancements as an exercise for the reader.

Before we leave the topic of functions, let's consider where to put
functions after we write them. Here are a few options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In the same script as the code that uses the function.
\item
  In a separate script, such as \emph{functions.R}.
\item
  In a package, which is loaded by the script that uses the function.
\end{enumerate}

The general heuristic for choosing where to put functions is to put them
in the same script as the code that uses them if the function is only
used in that script. If the function is used in multiple scripts or the
function or number of functions clutters the readability of the code,
then put it in a separate script. If the function is used in multiple
projects, then put it in an R package.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

If you are interested in learning more about writing functions, check
out the \href{https://r4ds.had.co.nz/functions.html}{Writing Functions
chapter} in the \href{https://r4ds.had.co.nz/}{R for Data Science} book.

If you find yourself writing functions that are useful for multiple
projects, you may want to consider creating an R package. R packages are
a great way to share your code with others. If you are interested in
learning more about creating R packages, check out the
\href{https://r-pkgs.org/}{R Packages book} by Hadley Wickham and Jenny
Bryan.

\end{tcolorbox}

In this case, we will put the function in a separate file,
\emph{functions.R}, in the same directory as the other code files as in
Example~\ref{exm-ad-functions-r}.

\begin{example}[]\protect\hypertarget{exm-ad-functions-r}{}\label{exm-ad-functions-r}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{code/}
\ExtensionTok{}\NormalTok{    1\_acquire\_data.qmd}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{    functions.R}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

Note that that the \emph{functions.R} file is an R script, not a Quarto
document. Therefore code blocks that are used in \emph{.qmd} files are
not used, only the R code and code comments.

\end{tcolorbox}

To include this, or other functions in in the R session of the code file
that uses them, use the \texttt{source()} function, as seen in
Example~\ref{exm-ad-source-functions}.

\begin{example}[]\protect\hypertarget{exm-ad-source-functions}{}\label{exm-ad-source-functions}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Source functions}
\FunctionTok{source}\NormalTok{(}\StringTok{"functions.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Given the utility of this function to my projects and potentially
others', I've included the \texttt{get\_talkbank\_data()} function in
the \texttt{qtalrkit} package. You can view the source code by calling
the function without parentheses \texttt{()}, or on the
\texttt{qtalrkit} GitHub repository.

After running the \texttt{get\_talkbank\_data()} function, we can see
that the data has been acquired and written to the
\emph{data/original/cabnc/} directory.

\begin{example}[]\protect\hypertarget{exm-ad-functions-r}{}\label{exm-ad-functions-r}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis}
\ExtensionTok{}\NormalTok{ derived}
\ExtensionTok{}\NormalTok{ original}
    \ExtensionTok{}\NormalTok{ cabnc}
        \ExtensionTok{}\NormalTok{ participants.csv}
        \ExtensionTok{}\NormalTok{ token\_types.csv}
        \ExtensionTok{}\NormalTok{ tokens.csv}
        \ExtensionTok{}\NormalTok{ transcripts.csv}
        \ExtensionTok{}\NormalTok{ utterances.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

Add comments to your code in \emph{1-acquire-data.Rmd} and create and
complete the data origin documentation file for this resource, and the
acquisition is complete.

\hypertarget{summary-4}{%
\section*{Summary}\label{summary-4}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we have covered a lot of ground. On the surface we have
discussed a few methods for acquiring corpus data for use in text
analysis. In the process we have delved into various aspects of the R
programming language. Some key concepts include writing custom
functions, control statements, and applying functions iteratively. We
have also considered topics that are more general in nature and concern
interacting with data found on the internet.

Each of these methods should be approached in a way that is transparent
to the researcher and to would-be collaborators and the general research
community. For this reason the documentation of the steps taken to
acquire data are key both in the code and in human-facing documentation.

At this point you have both a bird's eye view of the data available on
the web and strategies on how to access a great majority of it. It is
now time to turn to the next step in our data analysis project: data
curation. In the next chapter, I will cover how to wrangle your raw data
into a tidy dataset.

\hypertarget{activities-3}{%
\section*{Activities}\label{activities-3}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Add description of outcomes
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} update
\end{itemize}

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_6.html}{Collecting
and documenting data}\\
\textbf{How}: Read Recipe 6 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To understand common strategies to collect, organize, and
document data using effective, concise, and reproducible code. You will
see how control statements, writing custom functions, and leveraging
iteration works to these goals. These programming strategies are often
useful for acquiring data but, as we will see, they are powerful
concepts that can be used throughout a reproducible research project.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} update
\end{itemize}

\textbf{What}: \href{https://github.com/lin380/lab_6}{Control
statements, custom functions, and iteration}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 6.\\
\textbf{Why}: To gain experience working with coding strategies such as
control statements, custom functions, and iteration, practice working
with direct downloads and API interfaces to acquire data, and implement
organizational strategies for organizing data in reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-4}{%
\section*{Questions}\label{questions-4}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} create conceptual and technical questions
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Conceptual questions}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  For many resources, information to describe the data origin is found
  on the resource's website. Visit the XXX resource and complete the
  data origin information.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{Technical exercises}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\hypertarget{sec-curate-datasets}{%
\chapter{Curate datasets}\label{sec-curate-datasets}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
The hardest bit of information to extract is the first piece.

--- Robert Ferrigno
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Describe the importance of data curation in text analysis
\item
  Recognize the different types of data formats
\item
  Associate the types data formats with the appropriate R programming
  techniques to curate the data
\end{itemize}

\end{tcolorbox}

In this chapter we will now look at the next step in a text analysis
project: data curation. That is, the process of converting the original
data we acquire to a tidy dataset. Acquired data can come in a wide
variety of formats. These formats tend to signal the richness of the
metadata that is included in the file content. In this chapter we will
consider three general types of content formats: (1) unstructured data,
(2) structured data, and (3) semi-structured data. Regardless of the
file type and the structure of the data, it will be necessary to
consider how to curate a dataset that such that the structure reflects
the basic the unit of analysis that we wish to investigate. The
resulting dataset will form the base from which we will work to further
transform the dataset such that it aligns with the unit(s) of
observation required for the analysis method(s) that we will implement.
Once the dataset is curated, we will create a data dictionary that
describes the dataset and the variables that are included in the dataset
for transparency and reproducibility.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Pattern Matching
and Manipulate Datasets}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: To familiarize yourself with the basics of using the
pattern matching syntax Regular Expressions and the \texttt{dplyr}
package to manipulate datasets.

\end{tcolorbox}

\hypertarget{unstructured}{%
\section{Unstructured}\label{unstructured}}

The bulk of text ever created is of the unstructured variety.
Unstructured data is data that has not been organized to make the
information contained within machine-readable. Remember that text in
itself is not information. Only when given explicit context does text
become informative. The explicit contextual information that is included
with data is called metadata. Metadata can be linguistic or
non-linguistic in nature. So for unstructured data there is little to no
metadata directly associated with the data. This information needs to be
added or derived for the purposes of the research, either through manual
inspection or (semi-)automatic processes. For now, however, our job is
just to get the unstructured data into a structured format with a
minimal set of metadata that we can derive from the resource.

\hypertarget{reading-data}{%
\subsection{Reading data}\label{reading-data}}

Let's consider some of the common file formats which contain
unstructured data, such as TXT, PDF, and DOCX, and how to read these
formats into an R session.

It is very common for unstructured data resources to include data that
is in TXT files. These are the simplest of file formats which contain no
formatting or explicit metadata. Many times TXT files have the
\emph{.txt} extension, but it is not required. There are many ways to
read TXT files into R and many packages that can be used to do so. For
example, using the \texttt{readr} package, we can choose to read the
entire file into a single vector of character strings with
\texttt{read\_file()} or read the file by lines with
\texttt{read\_lines()} in which each line is a character string in a
vector.

Less commonly used in prepared data resources, PDF and DOCX files are
also formats for unstructured data. These formats are often more complex
than TXT files as they contain formatting and embedded metadata.
However, these attributes are primarily for visual presentation and not
for machine-readability. Therefore, we need to use packages and
functions that can extract the text content from these files and
potentially some of the metadata. For example, using the
\texttt{readtext} package (\protect\hyperlink{ref-R-readtext}{Benoit and
Obeng 2023}), we can read the text content from PDF and DOCX files into
a single vector of character strings with \texttt{readtext()}.

Whether in TXT, PDF, or DOCX format, the resulting data structure will
require further processing to convert the data into a tidy dataset. For
example, we may need to split the data into multiple columns or rows, or
extract metadata from the text content.

\hypertarget{orientation}{%
\subsection{Orientation}\label{orientation}}

As an example of curating an unstructured source of corpus data, let's
take a look at the \href{https://www.statmt.org/europarl/}{Europarl
Parallel Corpus} (\protect\hyperlink{ref-Koehn2005}{Koehn 2005}). This
corpus contains parallel texts (source and translated documents) from
the European Parliamentary proceedings between 1996-2011 for some 21
European languages.

Let's assuming we selected this corpus data because we are interested in
researching Spanish to English translations. After consulting the corpus
website, downloading the compressed file, and inspecting the
decompressed structure, we have the the file structure seen in
Example~\ref{exm-cd-europarl-file-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-file-structure}{}\label{exm-cd-europarl-file-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{}\NormalTok{ code/}
\ExtensionTok{}\NormalTok{    1{-}acquire{-}data.qmd}
\ExtensionTok{}\NormalTok{    2{-}curate{-}data.qmd}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    analysis/}
\ExtensionTok{}\NormalTok{    derived/}
\ExtensionTok{}\NormalTok{    original/}
\ExtensionTok{}\NormalTok{        europarl\_do.csv}
\ExtensionTok{}\NormalTok{        europarl/}
\ExtensionTok{}\NormalTok{            europarl{-}v7.es{-}en.en}
\ExtensionTok{}\NormalTok{            europarl{-}v7.es{-}en.es}
\ExtensionTok{}\NormalTok{ output/}
\ExtensionTok{}\NormalTok{    figures/}
\ExtensionTok{}\NormalTok{    reports/}
\ExtensionTok{}\NormalTok{    results/}
\ExtensionTok{}\NormalTok{    tables/}
\ExtensionTok{}\NormalTok{ README.md}
\ExtensionTok{}\NormalTok{ \_main.R}
\end{Highlighting}
\end{Shaded}

\end{example}

The \emph{europarl\_do.csv} file contains the data origin information
documented as part of the acquisition process. The contents are seen in
Table~\ref{tbl-cd-europarl-data-origin}.

\hypertarget{tbl-cd-europarl-data-origin}{}
\begin{table}
\caption{\label{tbl-cd-europarl-data-origin}Data origin: Europarl Corpus }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
attribute & description\\
\hline
Resource name & Europarl Parallel Corpus\\
\hline
Data source & <https://www.statmt.org/europarl/>\\
\hline
Data sampling frame & Spanish transcripts from the European Parliament proceedings\\
\hline
Data collection date(s) & 1996-2011\\
\hline
Data format & TXT files with '.es' for source (Spanish) and '.en' for target (English) files.\\
\hline
Data schema & Line-by-line unannotated parallel text\\
\hline
License & See: <https://www.europarl.europa.eu/legal-notice/en/>\\
\hline
Attribution & Please cite the paper: Koehn, P. 2005. 'Europarl: A Parallel Corpus for Statistical Machine Translation.' MT Summit X, 12-16.\\
\hline
\end{tabular}
\end{table}

Now let's get familiar with the corpus directory structure and the
files. In Example~\ref{exm-cd-europarl-file-structure}, we see that
there are two corpus files, \emph{europarl-v7.es-en.es} and
\emph{europarl-v7.es-en.en}, that contain the source and target language
texts, respectively. The file names indicate that the files contain
Spanish-English parallel texts. The \emph{.es} and \emph{.en} extensions
indicate the language of the text.

Looking at the beginning of the \emph{.es} and \emph{.en} files, in
File~\ref{lst-cd-europarl-es} and File~\ref{lst-cd-europarl-en}, we see
that the files contain a series of lines in either the source or target
language.

\begin{codelisting}

\caption{\texttt{data/original/europarl/europarl-v7.es-en.es}: Spanish
source text}

\hypertarget{lst-cd-europarl-es}{%
\label{lst-cd-europarl-es}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Reanudacin del perodo de sesiones}
\NormalTok{Declaro reanudado el perodo de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Seoras mi deseo de que hayan tenido unas buenas vacaciones.}
\NormalTok{Como todos han podido comprobar, el gran "efecto del ao 2000" no se ha producido. En cambio, los ciudadanos de varios de nuestros pases han sido vctimas de catstrofes naturales verdaderamente terribles.}
\NormalTok{Sus Seoras han solicitado un debate sobre el tema para los prximos das, en el curso de este perodo de sesiones.}
\NormalTok{A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las vctimas de las tormentas, en los distintos pases de la Unin Europea afectados.}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\begin{codelisting}

\caption{\texttt{data/original/europarl/europarl-v7.es-en.en}: English
target text}

\hypertarget{lst-cd-europarl-en}{%
\label{lst-cd-europarl-en}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Resumption of the session}
\NormalTok{I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.}
\NormalTok{Although, as you will have seen, the dreaded \textquotesingle{}millennium bug\textquotesingle{} failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.}
\NormalTok{You have requested a debate on this subject in the course of the next few days, during this part{-}session.}
\NormalTok{In the meantime, I should like to observe a minute\textquotesingle{} s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

We can clearly appreciate that the data is unstructured. That is, there
is no explicit metadata associated with the data. The data is just a
series of lines. The only information that we can surmise from structure
of the data is that the texts are line-aligned and that the data in each
file corresponds to source and target languages.

Now, before embarking on a data curation process, it is recommendable to
define the structure of the data that we want to create. I call this the
``idealized structure'' of the data. For a curated dataset there are two
considerations. First, we want to maintain the original structure of the
data as much as possible. This provides a link to the original data and
gives us a default dataset structure from which we can derive other
structures that fit our analysis needs. Second, we want to add structure
to the data that makes it easier to work with. This is the `tidy' part
of the data curation process. The aim is to imbue the dataset with as
much of the metadata the data resource makes available.

Given what we know about the data, we can define the idealized structure
of the data as seen in Table~\ref{tbl-cd-europarl-structure-example}.

\hypertarget{tbl-cd-europarl-structure-example}{}
\begin{table}[!h]
\caption{\label{tbl-cd-europarl-structure-example}Idealized structure for the europarl Corpus dataset. }\tabularnewline

\centering
\begin{tabular}{rlrl}
\toprule
doc\_id & type & line\_id & line\\
\midrule
1 & Source & 1 & ...line from source language\\
2 & Source & 2 & ...\\
3 & Source & 3 & ...\\
4 & Target & 1 & ...line from target language\\
5 & Target & 2 & ...\\
\addlinespace
6 & Target & 3 & ...\\
\bottomrule
\end{tabular}
\end{table}

The dataset structure in Table~\ref{tbl-cd-europarl-structure-example}
has four columns. The first column, \texttt{doc\_id} is a unique
identifier for each line in the corpus. \texttt{type}, indicates whether
the line is from the source or target language. The third column,
\texttt{line\_id}, is a unique identifier for each line for each
\texttt{type}. The last column, \texttt{line}, contains the text of the
line, and maintains the structure of the original data. The observations
are lines.

Our task now is to develop code that will read the original data and
render the idealized structure as a curated dataset we will write to the
\emph{data/derived/} directory. The code we develop will be added to the
\emph{2-curate-data.qmd} file. And finally, the dataset will be
documented with a data dictionary file.

\hypertarget{tidy-the-data}{%
\subsection{Tidy the data}\label{tidy-the-data}}

To create the idealized dataset structure in
Table~\ref{tbl-cd-europarl-structure-example}, lets's start by reading
the files into R. We will use the \texttt{readtext()} function from the
\texttt{readtext} package. \texttt{readtext()} is a versatile function
that can read many different types of text files (\emph{e.g.}
\emph{.txt}, \emph{.csv}, \emph{xml}, \emph{etc.}). It can also read
multiple files at once using wildcard matching. We will use it to read
the \emph{.es} and \emph{.en} files by passing the path to the directory
where the files are located, and then use the \texttt{*} wildcard to
read all the files in the directory.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(readtext)}

\CommentTok{\# Read Europarl files .es and .en}
\NormalTok{europarl\_docs\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{readtext}\NormalTok{(}\StringTok{"../data/original/europarl/*"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# read in files}
  \FunctionTok{tibble}\NormalTok{() }\CommentTok{\# convert to tibble}
\end{Highlighting}
\end{Shaded}

\begin{example}[]\protect\hypertarget{exm-cd-europarl-readtext}{}\label{exm-cd-europarl-readtext}

~

\end{example}

In Example~\ref{exm-cd-europarl-readtext}, the \texttt{readtext()}
function reads all the files in the \emph{../data/original/europarl/}
directory and returns a tibble.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

The \texttt{readtext()} function can read many different types of file
formats, from structured to unstructured. However, it depends in large
part on the extension of the file to recognize what algorithm to use
when reading a file. In this particular case, the Europarl files do not
have a typical extension (they have \texttt{.en} and \texttt{.es}). The
\texttt{readtext()} function will treat them as plain text
(\texttt{.txt}), but it will throw a warning message. To suppress the
warning message you can add the \texttt{verbosity\ =\ 0} argument or set
\texttt{readtext\_options(verbosity\ =\ 0)}.

\end{tcolorbox}

Let's inspect the \texttt{europarl\_docs\_tbl} object with the
\texttt{str()} function, in Example~\ref{exm-cd-europarl-readtext-str}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-readtext-str}{}\label{exm-cd-europarl-readtext-str}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview data}
\FunctionTok{str}\NormalTok{(europarl\_docs\_tbl) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> tibble [2 x 2] (S3: tbl_df/tbl/data.frame)
>  $ doc_id: chr [1:2] "europarl-v7.es-en.en" "europarl-v7.es-en.es"
>  $ text  : chr [1:2] "Resumption of the session\nI declare resumed the session of the European Parliament adjourned on Friday 17 Dece"| __truncated__ "Reanudacin del perodo de sesiones\nDeclaro reanudado el perodo de sesiones del Parlamento Europeo, interrump"| __truncated__
\end{verbatim}

\end{example}

We see that the output from Example~\ref{exm-cd-europarl-readtext-str}
is a tibble with two columns, \texttt{doc\_id} and \texttt{text}. The
\texttt{doc\_id} column contains the name of the file from which the
text was read. The \texttt{text} column contains the text of the file.
There are two observations, one for each file.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

Note that the \texttt{str()} function from base R is similar to
\texttt{glimpse()}. However, \texttt{glimpse()} will attempt to show you
as much data as possible. In this case since our column \texttt{text} is
a very long character vector it will take a long time to render. I've
chosen the \texttt{str()} function as it will automatically truncate the
data.

\end{tcolorbox}

The fact that we only have one row for each file means that all the text
in each file is contained in one cell! We want to break these cells up
into rows for each line, as they appear in the original data. You may be
wondering why the \texttt{readtext()} function did not do this for us,
after all the original data was already separated into lines. The reason
is that the \texttt{readtext()} function chose to read the files as
plain text, and plain text does not have any structure. The line breaks,
however, are still there, they are just represented as a special
character, \texttt{\textbackslash{}n}. Comparing
Example~\ref{exm-cd-europarl-text-lines} and
Example~\ref{exm-cd-europarl-text-lines-raw}, we can see that the text
is a single long character vector, and that the line breaks are
represented as \texttt{\textbackslash{}n}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines}{}\label{exm-cd-europarl-text-lines}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview first 50 characters}
\NormalTok{europarl\_docs\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_trunc}\NormalTok{(}\DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_view}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] | Resumption of the session
>     | I declare resumed the...
> [2] | Reanudacin del perodo de sesiones
>     | Declaro rea...
\end{verbatim}

\end{example}

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-raw}{}\label{exm-cd-europarl-text-lines-raw}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview first 50 raw characters}
\NormalTok{europarl\_docs\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(text) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_trunc}\NormalTok{(}\DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{str\_view}\NormalTok{(}\AttributeTok{use\_escapes =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] | Resumption of the session\nI declare resumed the...
> [2] | Reanudaci\u00f3n del per\u00edodo de sesiones\nDeclaro rea...
\end{verbatim}

\end{example}

In Example~\ref{exm-cd-europarl-text-lines}, we can see a truncated
version of the text as it is in a printed format. The
\texttt{str\_trunc()} function from the \texttt{stringr} package
(\protect\hyperlink{ref-R-stringr}{Wickham 2022}) truncates the text to
the first 50 characters. The \texttt{str\_view()} function from the same
package allows us to see the text in a viewer pane.

In Example~\ref{exm-cd-europarl-text-lines-raw}, we can see the raw
text, that is, the text as it is stored in the computer. The
\texttt{use\_escapes\ =\ TRUE} argument tells the \texttt{str\_view()}
function to show the special characters as they are stored in the
computer. This includes the \texttt{\textbackslash{}n} line-feed
character, which represents a line break.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

The \texttt{str\_view()} with the argument
\texttt{use\_escapes\ =\ TRUE} also allows you to see other special
characters, such as \texttt{\textbackslash{}t} (tab) and
\texttt{\textbackslash{}r} (carriage return) as well as Unicode
characters, such as \texttt{\textbackslash{}u00f3n} (),
\texttt{\textbackslash{}u00ed} (), \emph{etc.}. These characters are
not visible in the printed version of the text, but they are there in
the raw text.

\end{tcolorbox}

We can see that the text is a single long character vector, and that the
line breaks are represented as \texttt{\textbackslash{}n}. So our goal
is to split the text for each file into lines creating a new row for
each line created.

To do this we will use another function from the \texttt{stringr}
package, \texttt{str\_split()}, whose function is to split a character
vector into smaller character vectors based on some splitting criteria.
In Example~\ref{exm-cd-europarl-text-lines-tbl}, we use the
\texttt{str\_split()} function to split the text into lines based on the
\texttt{\textbackslash{}n} character and assign it to the new column
\texttt{lines} using \texttt{mutate()}. Since we will not need the
\texttt{text} column anymore, we can use \texttt{select()} to drop it.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-tbl}{}\label{exm-cd-europarl-text-lines-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Split text into lines}
\NormalTok{europarl\_lines\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_docs\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lines =} \FunctionTok{str\_split}\NormalTok{(text, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{text)}

\CommentTok{\# Preview}
\NormalTok{europarl\_lines\_tbl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 2
>   doc_id               lines            
>   <chr>                <list>           
> 1 europarl-v7.es-en.en <chr [1,965,734]>
> 2 europarl-v7.es-en.es <chr [1,965,734]>
\end{verbatim}

\end{example}

Previewing the output in Example~\ref{exm-cd-europarl-text-lines-tbl},
we can see that the \texttt{lines} column contains a list of character
vectors. Why is this so? \texttt{str\_split()} takes a character vector
and splits it, as we know. It is a vectorized function, meaning that it
can take a vector of character vectors and split each one into
subsegments. Since any given input character vector can have a different
number of subsegments, the number of subsegments in each file could be
different. The ideal object for such data is a list. So
\texttt{str\_split()} returns a list of character vectors.

The list of character vectors, lines in our case, is still associated
with the respective \texttt{doc\_id} value. However, we want to create a
new row for each line in the list. To do this, we will use the
\texttt{unnest()} function from the \texttt{tidyr} package
(\protect\hyperlink{ref-R-tidyr}{Wickham, Vaughan, and Girlich 2023}).
The \texttt{unnest()} function takes a list column and creates a new row
for each element in the list. We use the \texttt{unnest()} function to
create a new row for each line in the \texttt{lines} column. We can see
the output from Example~\ref{exm-cd-europarl-text-lines-tbl-unnest}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-tbl-unnest}{}\label{exm-cd-europarl-text-lines-tbl-unnest}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new row for each line}
\NormalTok{europarl\_lines\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_lines\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest}\NormalTok{(lines)}

\CommentTok{\# Preview}
\NormalTok{europarl\_lines\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 2
>   doc_id               lines                                                    
>   <chr>                <chr>                                                    
> 1 europarl-v7.es-en.en Resumption of the session                                
> 2 europarl-v7.es-en.en I declare resumed the session of the European Parliament~
> 3 europarl-v7.es-en.en Although, as you will have seen, the dreaded 'millennium~
> 4 europarl-v7.es-en.en You have requested a debate on this subject in the cours~
> 5 europarl-v7.es-en.en In the meantime, I should like to observe a minute' s si~
\end{verbatim}

\end{example}

Remember that the data in the Europarl corpus is aligned, meaning that
each line in the source file is aligned with a line in the target file.
So we need to make sure that the number of lines in each file is the
same. We can do this by grouping the data by \texttt{doc\_id} and then
counting the number of lines in each file. We can do this using the
\texttt{group\_by()} and \texttt{count()} functions from the
\texttt{dplyr} package (\protect\hyperlink{ref-R-dplyr}{Wickham,
Franois, et al. 2023}). We can see the output in
Example~\ref{exm-cd-europarl-text-lines-tbl-count}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-tbl-count}{}\label{exm-cd-europarl-text-lines-tbl-count}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Count the number of lines in each file}
\NormalTok{europarl\_lines\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(doc\_id) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 2
> # Groups:   doc_id [2]
>   doc_id                     n
>   <chr>                  <int>
> 1 europarl-v7.es-en.en 1965734
> 2 europarl-v7.es-en.es 1965734
\end{verbatim}

\end{example}

The output of Example~\ref{exm-cd-europarl-text-lines-tbl-count} shows
that the number of lines in each file is the same. This is good. If the
number of lines in each file was different, we would need to figure out
why and fix it.

We now have our \texttt{lines} column and the associated observations
for our idealized dataset, in
Table~\ref{tbl-cd-europarl-structure-example}. Let's now leverage the
existing \texttt{doc\_id} to create the \texttt{type} column. The goal
is to assign the value of \texttt{type} according to the
\texttt{doc\_id} value. Specifically, when \texttt{doc\_id} is
`europarl-v7.es-en.es' type should be `Source' and when \texttt{doc\_id}
is `europarl-v7.es-en.en' type should be `Target'.

We can do this using the \texttt{case\_when()} function from the
\texttt{dplyr} package (\protect\hyperlink{ref-R-dplyr}{Wickham,
Franois, et al. 2023}). The \texttt{case\_when()} function takes a
series of conditions and assigns a value based on the first condition
that is met. In this case, we will use \texttt{mutate()} to create the
\texttt{type} column and then use the \texttt{doc\_id} column to create
the conditions and the \texttt{type} column to assign the values from
\texttt{case\_when()}. We will store the output in a new object called
\texttt{europarl\_lines\_type\_tbl}, as seen in
Example~\ref{exm-cd-europarl-text-lines-type-tbl}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-type-tbl}{}\label{exm-cd-europarl-text-lines-type-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create \textasciigrave{}type\textasciigrave{} column}
\NormalTok{europarl\_lines\_type\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_lines\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    doc\_id }\SpecialCharTok{==} \StringTok{"europarl{-}v7.es{-}en.es"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Source"}\NormalTok{,}
\NormalTok{    doc\_id }\SpecialCharTok{==} \StringTok{"europarl{-}v7.es{-}en.en"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Target"}
\NormalTok{  ))}

\CommentTok{\# Preview dataset}
\FunctionTok{glimpse}\NormalTok{(europarl\_lines\_type\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,931,468
> Columns: 3
> $ doc_id <chr> "europarl-v7.es-en.en", "europarl-v7.es-en.en", "europarl-v7.es~
> $ lines  <chr> "Resumption of the session", "I declare resumed the session of ~
> $ type   <chr> "Target", "Target", "Target", "Target", "Target", "Target", "Ta~
\end{verbatim}

\end{example}

The preview output from
Example~\ref{exm-cd-europarl-text-lines-type-tbl} shows us that we now
have three columns, \texttt{doc\_id}, \texttt{lines}, and \texttt{type}.
The \texttt{type} column has been created and the values have been
assigned according to the \texttt{doc\_id} values.

We can now overwrite the \texttt{doc\_id} column with a unique
identifier for each line. A numeric identifier makes sense. We can do
this by using the \texttt{mutate()} function to assign \texttt{doc\_id}
a sequential number with \texttt{row\_number()}, which increments by 1
for each row. We will store the output in a new object called
\texttt{europarl\_lines\_type\_id\_tbl}, as seen in
Example~\ref{exm-cd-europarl-text-lines-type-id-tbl}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-type-id-tbl}{}\label{exm-cd-europarl-text-lines-type-id-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create new \textasciigrave{}doc\_id\textasciigrave{} column}
\NormalTok{europarl\_lines\_type\_id\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_lines\_type\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{row\_number}\NormalTok{())}

\CommentTok{\# Preview dataset}
\FunctionTok{glimpse}\NormalTok{(europarl\_lines\_type\_id\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,931,468
> Columns: 3
> $ doc_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, ~
> $ lines  <chr> "Resumption of the session", "I declare resumed the session of ~
> $ type   <chr> "Target", "Target", "Target", "Target", "Target", "Target", "Ta~
\end{verbatim}

\end{example}

The preview output from
Example~\ref{exm-cd-europarl-text-lines-type-id-tbl} shows us that we
now have the same three columns, \texttt{doc\_id}, \texttt{lines}, and
\texttt{type}. However, the values in the \texttt{doc\_id} column now
reflect a unique identifier for each line.

The last step to get to our envisioned dataset structure is to add the
\texttt{line\_id} column which will be calculated by grouping the data
by \texttt{type} and then assigning a row number to each of the lines in
each group. We use the \texttt{group\_by()} function to perform the
grouping as seen in
Example~\ref{exm-cd-europarl-text-lines-type-id-line-id-tbl}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-text-lines-type-id-line-id-tbl}{}\label{exm-cd-europarl-text-lines-type-id-line-id-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create \textasciigrave{}line\_id\textasciigrave{} column}
\NormalTok{europarl\_lines\_type\_id\_line\_id\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_lines\_type\_id\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{line\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview dataset}
\FunctionTok{glimpse}\NormalTok{(europarl\_lines\_type\_id\_line\_id\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,931,468
> Columns: 4
> $ doc_id  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~
> $ lines   <chr> "Resumption of the session", "I declare resumed the session of~
> $ type    <chr> "Target", "Target", "Target", "Target", "Target", "Target", "T~
> $ line_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,~
\end{verbatim}

\end{example}

The preview output from
Example~\ref{exm-cd-europarl-text-lines-type-id-line-id-tbl} shows us
that we now have the desired four columns, \texttt{doc\_id},
\texttt{lines}, \texttt{type}, and \texttt{line\_id}.

Before we get to writing the dataset to disk, let's organized it in a
way that the columns are in the order we want and the rows are sorted in
a way that makes sense.

Reordering the columns involves using the \texttt{select()} function and
list the order of the columns. To sort by columns, we will use
\texttt{arrange()} and specify the columns to order by. We store the
results in a more legible object, \texttt{europarl\_curated\_tbl}, as
seen in Example~\ref{exm-cd-europarl-curated-tbl}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-curated-tbl}{}\label{exm-cd-europarl-curated-tbl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reorder columns and sort rows}
\NormalTok{europarl\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_lines\_type\_id\_line\_id\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(doc\_id, type, line\_id, lines) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(line\_id, type, doc\_id)}

\CommentTok{\# Preview dataset}
\FunctionTok{glimpse}\NormalTok{(europarl\_curated\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 3,931,468
> Columns: 4
> $ doc_id  <int> 1965735, 1, 1965736, 2, 1965737, 3, 1965738, 4, 1965739, 5, 19~
> $ type    <chr> "Source", "Target", "Source", "Target", "Source", "Target", "S~
> $ line_id <int> 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, ~
> $ lines   <chr> "Reanudacin del perodo de sesiones", "Resumption of the sess~
\end{verbatim}

\end{example}

The preview output from Example~\ref{exm-cd-europarl-curated-tbl} shows
us that we now have the desired four columns, \texttt{doc\_id},
\texttt{type}, \texttt{line\_id}, and \texttt{lines}. The rows are
sorted by \texttt{line\_id}, \texttt{type}, and \texttt{doc\_id}. This
gives us a dataset that reads left to right from document to line
oriented attributes and top to bottom by source-target line pairs.

\hypertarget{write-dataset}{%
\subsection{Write dataset}\label{write-dataset}}

At this point we have the curated dataset
(\texttt{europarl\_curated\_tbl}) in a tidy format. This dataset,
however, is only in the current R session. We will want to write this
dataset to disk so that in the next step of the text analysis workflow
(transformation) we will be able to start work on this dataset and make
changes as needed to fit our analysis needs.

We will leverage the project directory structure which has distinct
directories for \texttt{original/} and \texttt{derived/} data(sets),
seen in Example~\ref{exm-cd-europarl-write-directory}.

\begin{example}[]\protect\hypertarget{exm-cd-europarl-write-directory}{}\label{exm-cd-europarl-write-directory}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ europarl\_do.csv}
    \ExtensionTok{}\NormalTok{ europarl/}
        \ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.es}
        \ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.en}
\end{Highlighting}
\end{Shaded}

\end{example}

Since this is a tabular, tidy dataset we have various options for the
file type to write. Many of these formats are software-specific, such as
\texttt{*.xlsx} for Microsoft Excel, \texttt{*.sav} for SPSS,
\texttt{*.dta} for Stata, and \texttt{*.rds} for R. We will use the
\texttt{*.csv} format since it is a common format that can be read by
many software packages. We will use the \texttt{write\_csv()} function
from the \texttt{readr} package to write the dataset to disk.

Now the question is where to save our CSV file. Since our
\texttt{europarl\_curated\_tbl} dataset is derived by our work, we will
added it to the \texttt{derived/} directory. I'll create a
\texttt{europarl/} directory with \texttt{dir\_create()} just to keep
things organized.

\begin{example}[]\protect\hypertarget{exm-cd-unstructured-write-europarl}{}\label{exm-cd-unstructured-write-europarl}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the europarl/ directory}
\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/europarl/"}\NormalTok{)}

\CommentTok{\# Write the curated dataset to disk}
\FunctionTok{write\_csv}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ europarl\_curated\_tbl,}
  \AttributeTok{file =} \StringTok{"../data/derived/europarl/europarl\_curated.csv"}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\end{example}

After running the code in
Example~\ref{exm-cd-unstructured-write-europarl}, the directory
structure under the \texttt{derived/} directory should look like
Example~\ref{exm-cd-unstructured-write-europarl-directory}.

\begin{example}[]\protect\hypertarget{exm-cd-unstructured-write-europarl-directory}{}\label{exm-cd-unstructured-write-europarl-directory}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{    europarl/}
\ExtensionTok{}\NormalTok{        europarl\_curated.csv}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{europarl}
        \ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.en}
        \ExtensionTok{}\NormalTok{ europarl{-}v7.es{-}en.es}
\end{Highlighting}
\end{Shaded}

\end{example}

The final step, as always, is to document the dataset. For datasets the
documentation is a data dictionary, as discussed in
Section~\ref{sec-ud-data-dictionaries}. As with data origin files, you
can use spreadsheet software to create and/ or edit the data dictionary.

In the \texttt{qtalrkit} package we have a function,
\texttt{create\_data\_dictionary()} that will generate the scaffolding
for a data dictionary. The function takes two arguments, \texttt{data}
and \texttt{file\_path}. It reads the dataset columns and provides a
template for the data dictionary.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

The \texttt{create\_data\_dictionary()} function provides a rudimentary
data dictionary template by default. However, you can take advantage of
OpenAI's text generation models to generate a more detailed data
dictionary for you to edit. To do this create
\href{https://platform.openai.com/signup}{an OpenAI account} and
\href{https://platform.openai.com/account/api-keys}{an API key} and add
this key to your R environment
(\texttt{Sys.setenv(OPENAI\_API\_KEY\ =\ "sk..."}). Then you can specify
the model you would like to use in the function with the
\texttt{model\ =} argument. For example,
\texttt{model\ =\ "gpt-3.5-turbo"} will use the GPT-3.5 Turbo model.

\end{tcolorbox}

\begin{example}[]\protect\hypertarget{exm-cd-unstructured-data-dictionary}{}\label{exm-cd-unstructured-data-dictionary}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the data dictionary}
\FunctionTok{create\_data\_dictionary}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ europarl\_curated\_tbl,}
\NormalTok{  file\_path }\OtherTok{\textless{}{-}} \StringTok{"../data/derived/europarl/europarl\_curated\_dd.csv"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

An example of the data dictionary for the
\texttt{europarl\_curated\_tbl} dataset is shown in
Table~\ref{tbl-cd-unstructured-data-dictionary-example}.

\hypertarget{tbl-cd-unstructured-data-dictionary-example}{}
\begin{table}
\caption{\label{tbl-cd-unstructured-data-dictionary-example}Data dictionary for the \texttt{europarl\_curated\_tbl} dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
doc\_id & Document ID & numeric & Unique identification number for each document\\
\hline
type & Document Type & categorical & Type of document; either 'Source' (Spanish) or 'Target' (English)\\
\hline
line\_id & Line ID & numeric & Unique identification number for each line in each document type\\
\hline
lines & Lines & categorical & Content of the lines in the document\\
\hline
\end{tabular}
\end{table}

\hypertarget{structured}{%
\section{Structured}\label{structured}}

Structured data already reflects the physical and semantic structure of
a tidy dataset. This means that the data is already in a tabular format
and the relationships between columns and rows are already well-defined.
Therefore the heavy lifting of curating the data is already done. There
are two remaining questions, however, that need to be taken into
account. One, logistical question, is what file format the dataset is in
and how to read it into R. And the second, more research-based, is
whether the data may benefit from some additional curation and
documentation to make it more amenable to analysis and more
understandable to others.

\hypertarget{reading-data-1}{%
\subsection{Reading data}\label{reading-data-1}}

Let's consider some common formats for structured data and how to read
them into R. First, we will consider R-native formats, such as package
datasets and RDS files. Then will consider non-native formats, such as
relational databases and datasets produced by other software. Finally,
we will consider software agnostic formats, such as CSV.

R and some R packages provide structured datasets that are available for
use directly within R. For example, the \texttt{languageR} package
(\protect\hyperlink{ref-R-languageR}{\textbf{R-languageR?}}) provides
the \texttt{dative} dataset, which is a dataset containing the
realization of the dative as NP or PP in the Switchboard corpus and the
Treebank Wall Street Journal collection. The \texttt{janeaustenr}
package (\protect\hyperlink{ref-R-janeaustenr}{Silge 2022}) provides the
\texttt{austen\_books} dataset, which is a dataset of Jane Austen's
novels. Package datasets are loaded into an R session using either the
\texttt{data()} function, if the package is loaded, or the \texttt{::}
operator, if the package is not loaded. For example,
\texttt{data(dative)} or \texttt{languageR::dative}.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

To explore the available datasets in a package, you can use the
\texttt{data(package\ =\ "package\_name")} function. For example,
\texttt{data(package\ =\ "languageR")} will list the datasets available
in the \texttt{languageR} package. You can also explore all the datasets
available in the loaded packages with the \texttt{data()} function using
no arguments. For example, \texttt{data()}.

\end{tcolorbox}

R also provides a native file format for storing R objects, the RDS
file. Any R object, including data frames, can be written from an R
session to disk by using the \texttt{write\_rds()} function from
\texttt{readr}. The \emph{.rds} files will be written to disk in a
binary format that is not human-readable, which is not ideal for
transparent data sharing. However, the files and the R objects can be
read back into an R session using the \texttt{read\_rds()} function with
all the attributes intact, such as vector types, factor levels,
\emph{etc.}.

R provides a suite of tools for importing data from non-native
structured sources such as databases and datasets from software such as
SPSS, SAS, and Stata. For instance, if you are working with data stored
in a relational database such as MySQL, PostgreSQL, or SQLite, you can
use the \texttt{DBI} package (\protect\hyperlink{ref-R-DBI}{R Special
Interest Group on Databases (R-SIG-DB), Wickham, and Mller 2022}) to
connect to the database and the \texttt{dbplyr} package
(\protect\hyperlink{ref-R-dbplyr}{Wickham, Girlich, and Ruiz 2023}) to
query the database using the SQL language. Files from SPSS
(\emph{.sav}), SAS (\emph{.sas7bdat}), and Stata (\emph{.dta}) can be
read into R using the \texttt{haven} package
(\protect\hyperlink{ref-R-haven}{Wickham, Miller, and Smith 2023}).

Software agnostic file formats include delimited files, such as CSV,
TSV, \emph{etc.}. These file formats lack the robust structural
attributes of the other formats, but balance this shortcoming by storing
structured data in more accessible, human-readable format. Delimited
files are plain text files which use a delimiter, such as a comma
(\texttt{,}), tab (\texttt{\textbackslash{}t}), or pipe
(\texttt{\textbar{}}), to separate the columns and rows. For example, a
CSV file is a delimited file where the columns and rows are separated by
commas, as seen in Example~\ref{exm-cd-csv-example}.

\begin{example}[]\protect\hypertarget{exm-cd-csv-example}{}\label{exm-cd-csv-example}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_1,column\_2,column\_3}
\NormalTok{row 1 value 1,row 1 value 2,row 1 value 3}
\NormalTok{row 2 value 1,row 2 value 2,row 2 value 3}
\end{Highlighting}
\end{Shaded}

\end{example}

Given the accessibility of delimited files, they are a common format for
sharing structured data in reproducible research. It is not surprising,
then, that this is the format which we have chosen for the derived
datasets in this book.

\hypertarget{orientation-1}{%
\subsection{Orientation}\label{orientation-1}}

With an understanding of the various structured formats, we can now turn
to considerations about how the original dataset is structured and how
that structure is to be used for a given research project. As an
example, we will work with the CABNC datasets acquired in
Chapter~\ref{sec-acquire-data}. The structure of the original dataset is
shown in Example~\ref{exm-cd-cabnc-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-structure}{}\label{exm-cd-cabnc-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ cabnc\_do.csv}
    \ExtensionTok{}\NormalTok{ cabnc/}
        \ExtensionTok{}\NormalTok{ participants.csv}
        \ExtensionTok{}\NormalTok{ token\_types.csv}
        \ExtensionTok{}\NormalTok{ tokens.csv}
        \ExtensionTok{}\NormalTok{ transcripts.csv}
        \ExtensionTok{}\NormalTok{ utterances.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

In addition to other important information, the data origin file
\emph{cabnc\_do.csv} shown in Table~\ref{tbl-cd-cabnc-do} informs us the
the datasets are related by a common variable.

\hypertarget{tbl-cd-cabnc-do}{}
\begin{table}
\caption{\label{tbl-cd-cabnc-do}Data origin: CABNC datasets }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
attribute & description\\
\hline
Resource name & CABNC.\\
\hline
Data source & <https://ca.talkbank.org/access/CABNC.html>, <doi:10.21415/T55Q5R>\\
\hline
Data sampling frame & Over 400 British English Speakers from across the UK stratified age, gender, social group, and region, and recording their language output over a set period of time.\\
\hline
Data collection date(s) & 1992.\\
\hline
Data format & CSV Files\\
\hline
Data schema & The recordings are linked by `filename` and the participants are linked by `who`.\\
\hline
License & CC BY NC SA 3.0\\
\hline
Attribution & Saul Albert, Laura E. de Ruiter, and J.P. de Ruiter (2015) CABNC: the Jeffersonian transcription of the Spoken British National Corpus. https://saulalbert.github.io/CABNC/.\\
\hline
\end{tabular}
\end{table}

The CABNC datasets are structured in a relational format, which means
that the data is stored in multiple tables that are related to each
other. The tables are related by a common column or set of columns,
which are called a keys. A key is used to join the tables together to
create a single dataset. There are two keys in the CABNC datasets,
\texttt{filename} and \texttt{who}. Each variable corresponds to
recording- and/ or participant-oriented datasets.

Now, let's envision a scenario in which we want to organize a dataset
that can be used in a study that aims to investigate the relationship
between speaker demographics and utterances. An ideal dataset would
contain information about speakers and their utterances. In their
original format, the CABNC datasets separate information about
utterances and speakers in separate tables, \texttt{cabnc\_utterances}
and \texttt{cabnc\_participants}, respectively. The idealized dataset,
then, will combine the variables from each of these tables into a single
dataset.

\hypertarget{tidy-the-dataset}{%
\subsection{Tidy the dataset}\label{tidy-the-dataset}}

With our idealized dataset in mind, let's start the process of curation
by reading the relevant datasets into an R session. Since we are working
with CSV files will will use the \texttt{read\_csv()} function, as seen
in Example~\ref{exm-cd-cabnc-read}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-read}{}\label{exm-cd-cabnc-read}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read the relevant datasets}
\NormalTok{cabnc\_utterances }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/cabnc/original/utterances.csv"}\NormalTok{)}
\NormalTok{cabnc\_participants }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/cabnc/original/participants.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

The next step is to inspect the structure of the datasets. We can use
the \texttt{glimpse()} function for this task.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-glimpse}{}\label{exm-cd-cabnc-glimpse}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the structure of the datasets}
\FunctionTok{glimpse}\NormalTok{(cabnc\_utterances)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 10
> $ filename  <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", ~
> $ path      <chr> "ca/CABNC/KB0/KB0RE000", "ca/CABNC/KB0/KB0RE000", "ca/CABNC/~
> $ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
> $ who       <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS002~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ postcodes <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ gems      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ utterance <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh I co~
> $ startTime <dbl> 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2~
> $ endTime   <dbl> 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(cabnc\_participants)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 6,190
> Columns: 13
> $ filename  <chr> "KB0RE004", "KB0RE004", "KB0RE004", "KB0RE006", "KB0RE006", ~
> $ path      <chr> "ca/CABNC/0missing/KB0RE004", "ca/CABNC/0missing/KB0RE004", ~
> $ who       <chr> "PS008", "PS009", "KB0PSUN", "PS007", "PS008", "PS009", "KB0~
> $ name      <chr> "John", "Gethyn", "Unknown_speaker", "Alan", "John", "Gethyn~
> $ role      <chr> "Unidentified", "Unidentified", "Unidentified", "Unidentifie~
> $ language  <chr> "eng", "eng", "eng", "eng", "eng", "eng", "eng", "eng", "eng~
> $ monthage  <dbl> 481, 481, 13, 949, 481, 481, 13, 637, 565, 13, 637, 565, 13,~
> $ age       <chr> "40;01.01", "40;01.01", "1;01.01", "79;01.01", "40;01.01", "~
> $ sex       <chr> "male", "male", "male", "male", "male", "male", "male", "mal~
> $ numwords  <dbl> 28, 360, 156, 1610, 791, 184, 294, 93, 3, 0, 128, 24, 0, 150~
> $ numutts   <dbl> 1, 9, 27, 7, 5, 7, 6, 5, 1, 0, 11, 6, 0, 110, 74, 96, 12, 1,~
> $ avgutt    <dbl> 28.00, 40.00, 5.78, 230.00, 158.20, 26.29, 49.00, 18.60, 3.0~
> $ medianutt <dbl> 28, 39, 5, 84, 64, 9, 3, 15, 3, 0, 9, 3, 0, 7, 6, 4, 3, 12, ~
\end{verbatim}

\end{example}

From visual inspection of the output of
Example~\ref{exm-cd-cabnc-glimpse} we can see that there are common
variables in both datasets. It is also possible to intersect the
datasets to see which variables are common using the
\texttt{intersect()} function to intersect the column names of each data
frame with the \texttt{names()} function, as seen in
Example~\ref{exm-cd-cabnc-intersect}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-intersect}{}\label{exm-cd-cabnc-intersect}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Find the common variables}
\NormalTok{common\_vars }\OtherTok{\textless{}{-}} 
  \FunctionTok{intersect}\NormalTok{(}
    \FunctionTok{names}\NormalTok{(cabnc\_utterances), }
    \FunctionTok{names}\NormalTok{(cabnc\_participants)}
\NormalTok{  )}

\CommentTok{\# Print the common variables}
\NormalTok{common\_vars}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "filename" "path"     "who"      "role"
\end{verbatim}

\end{example}

Using both visual inspection and column name intersection, we can make
sure that the variable names and the values are consistent across the
datasets. For example, if the variable \texttt{filename} in one dataset
is called \texttt{file\_name} in another dataset, then we will need to
rename the variable in one of the datasets so that the variable names
are consistent. Furthermore, if the values in the \texttt{filename}
variable are not consistent across the datasets, then we will need to
make the values consistent, if possible, before joining the datasets.

In this case, the variable names and values are consistent across the
datasets. Therefore, we can join the datasets together using the
\texttt{left\_join()} function from the \texttt{dplyr} package, as seen
in Example~\ref{exm-cd-cabnc-join}. This function will take the dataset
on the left (\texttt{x\ =}) and join it to the dataset on the right
(\texttt{y\ =}). The \texttt{by} argument specifies the variables to
join on. The choice of which dataset to put on the left usually depends
on which dataset has the most detailed information. In this case, the
\texttt{cabnc\_utterances} dataset has more detailed information about
the utterances so we will put this dataset on the left.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-join}{}\label{exm-cd-cabnc-join}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the datasets}
\NormalTok{cabnc\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{left\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ cabnc\_utterances, }
    \AttributeTok{y =}\NormalTok{ cabnc\_participants, }
    \AttributeTok{by =}\NormalTok{ common\_vars}
\NormalTok{  )}

\CommentTok{\# Preview the dimensions of the joined dataset}
\FunctionTok{dim}\NormalTok{(cabnc\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 235901     19
\end{verbatim}

\end{example}

The result of Example~\ref{exm-cd-cabnc-join} should produce a dataset
with the same number of observations as the \texttt{cabnc\_utterances}
dataset, seen in Example~\ref{exm-cd-cabnc-glimpse}, and a combined
number of unique variables from both datasets, that is 23 minus the four
common variables, seen in Example~\ref{exm-cd-cabnc-intersect}. The
output of \texttt{dim()} confirms this.

Now we will consider the variables that will be useful for future
analysis. Since we are creating a curated dataset, the goal will be to
retain as much information as possible from the original datasets. There
are cases, however, in which there may be variables that are not
informative and thus, will not prove useful for any analysis. These
removable variables tend to be of one of two types: variables which show
no variation across observations and variables where the information is
redundant.

Let's get a high-level summary of the variables in the dataset. We can
use the \texttt{skim()} function from the \texttt{skimr} package
(\protect\hyperlink{ref-R-skimr}{Waring et al. 2022}) to get a summary
of the variables in the dataset\footnote{Note I've modified the output
  of \texttt{skim()} for display purposes.}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-skim}{}\label{exm-cd-cabnc-skim}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(skimr)}

\CommentTok{\# Summarize the variables in the dataset}
\FunctionTok{skim}\NormalTok{(cabnc\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tbl-cd-cabnc-skim-run}Summary of variables in the CABNC
dataset}\begin{minipage}[t]{\linewidth}
\subcaption{\label{tbl-cd-cabnc-skim-run-1}\textbf{?(caption)}}

{\centering 

\hypertarget{tbl-cd-cabnc-skim-run-1}{}
\tabularnewline

\centering\begingroup\fontsize{8}{10}\selectfont

\begin{tabular}{l|r|r}
\hline
variable & complete\_rate & n\_unique\\
\hline
filename & 1 & 2020\\
\hline
path & 1 & 2020\\
\hline
who & 1 & 568\\
\hline
role & 1 & 1\\
\hline
utterance & 1 & 174414\\
\hline
name & 1 & 269\\
\hline
language & 1 & 1\\
\hline
age & 1 & 83\\
\hline
sex & 1 & 2\\
\hline
\end{tabular}
\endgroup{}

Categorical variables 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}
\subcaption{\label{tbl-cd-cabnc-skim-run-2}\textbf{?(caption)}}

{\centering 

\hypertarget{tbl-cd-cabnc-skim-run-2}{}
\tabularnewline

\centering\begingroup\fontsize{8}{10}\selectfont

\begin{tabular}{l|r|r}
\hline
variable & complete\_rate & mean\\
\hline
postcodes & 0 & NA\\
\hline
gems & 0 & NA\\
\hline
\end{tabular}
\endgroup{}

Logical variables 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}
\subcaption{\label{tbl-cd-cabnc-skim-run-3}\textbf{?(caption)}}

{\centering 

\hypertarget{tbl-cd-cabnc-skim-run-3}{}
\tabularnewline

\centering\begingroup\fontsize{8}{10}\selectfont

\begin{tabular}{l|r|r}
\hline
variable & complete\_rate & mean\\
\hline
utt\_num & 1.000 & 231.67\\
\hline
startTime & 0.841 & NA\\
\hline
endTime & 0.841 & NA\\
\hline
monthage & 1.000 & 448.61\\
\hline
numwords & 1.000 & 1387.47\\
\hline
numutts & 1.000 & 156.04\\
\hline
avgutt & 1.000 & 9.13\\
\hline
medianutt & 1.000 & 6.06\\
\hline
\end{tabular}
\endgroup{}

Numeric variables 

}

\end{minipage}%

\end{table}

\end{example}

We see from the \texttt{skim()} output in
Table~\ref{tbl-cd-cabnc-skim-run-2}, that the variables
\texttt{postcodes} and \texttt{gems} have no values. Therefore we can
remove these variables from the dataset. On the other hand, in
Table~\ref{tbl-cd-cabnc-skim-run-1}, the variables \texttt{role} and
\texttt{language} have the same value for every observation as the
output shows \texttt{n\_unique} is \texttt{1}. We can also remove these
variables from the dataset.

Another set of variables of potential interest are the
\texttt{startTime} and \texttt{endTime} variables, in
Table~\ref{tbl-cd-cabnc-skim-run-3}. These each have a completion rate
of less than 100\%, specifically 84\%. On first blush, it would seem
that we would go ahead and remove them. However, we should pause and
consider that these variables may still be of some use. At the curation
stage, however, it is best to err on the side of caution and leave them
in the dataset. We will decide later whether to keep or remove them as
we explore the dataset further in the research.

Another consideration is whether we have variables that are clearly
redundant. For example, the variables \texttt{age} and \texttt{monthage}
are both measures of age, stated in different measures, looking back at
Example~\ref{exm-cd-cabnc-glimpse}. As such, we can remove one of these
variables from the dataset. In this case, we will remove the
\texttt{age} variable as it is the least straightforward to interpret.

Another potentially redundant set of variables are \texttt{who} and
\texttt{name} --both of which are speaker identifiers. The \texttt{who}
variable is a unique identifier, but there may be some redundancy with
the \texttt{name} variable, that is there may be two speakers with the
same name. We can check this by looking at the number of unique values
in the \texttt{who} and \texttt{name} variables from the \texttt{skim()}
output in Table~\ref{tbl-cd-cabnc-skim-run-1}. \texttt{who} has 568
unique values and \texttt{name} has 269 unique values. This suggests
that there are multiple speakers with the same name.

Another way to explore this is to look at the number of unique values in
the \texttt{who} variable for each unique value in the \texttt{name}
variable. We can do this using the \texttt{group\_by()} and
\texttt{summarize()} functions from the \texttt{dplyr} package. For each
value of \texttt{name}, we will count the number of unique values in
\texttt{who} and then sort the results in descending order.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-who-name}{}\label{exm-cd-cabnc-who-name}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cabnc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{unique}\NormalTok{(who) }\SpecialCharTok{|\textgreater{}} \FunctionTok{length}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 269 x 2
>    name                          n
>    <chr>                     <int>
>  1 None                         59
>  2 Unknown_speaker              57
>  3 Group_of_unknown_speakers    10
>  4 Chris                         9
>  5 David                         9
>  6 Margaret                      8
>  7 Ann                           7
>  8 John                          7
>  9 Alan                          6
> 10 Jackie                        5
> # i 259 more rows
\end{verbatim}

\end{example}

It is good that we performed the check in
Example~\ref{exm-cd-cabnc-who-name} beforehand. In addition to speakers
with the same name, such as `Chris' and `David', we also have multiple
speakers with generic codes, such as `None' and `Unknown\_speaker'. It
is clear that \texttt{name} is redundant and we can safely remove it
from the dataset.

Another redundant variable is the \texttt{path} variable. This variable
is not more informative than the \texttt{filename}, for our purposes. We
will remove the \texttt{path} variable from the dataset too.

In all, we will remove the following variables from the dataset:
\texttt{postcodes}, \texttt{gems}, \texttt{role}, \texttt{language},
\texttt{age}, \texttt{name}, and \texttt{path}. To drop variables from a
data frame we can use the \texttt{select()} function in combination with
the \texttt{-} operator. The \texttt{-} operator tells the
\texttt{select()} function to drop the variables that follow it.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-drop-vars}{}\label{exm-cd-cabnc-drop-vars}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop variables}
\NormalTok{cabnc\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{postcodes, }\SpecialCharTok{{-}}\NormalTok{gems, }\SpecialCharTok{{-}}\NormalTok{role, }\SpecialCharTok{{-}}\NormalTok{language, }\SpecialCharTok{{-}}\NormalTok{age, }\SpecialCharTok{{-}}\NormalTok{name, }\SpecialCharTok{{-}}\NormalTok{path)}

\CommentTok{\# Preview the dataset}
\FunctionTok{glimpse}\NormalTok{(cabnc\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 12
> $ filename  <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", ~
> $ utt_num   <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
> $ who       <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "PS002~
> $ utterance <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh I co~
> $ startTime <dbl> 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480, 10.2~
> $ endTime   <dbl> 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34, 15.9~
> $ monthage  <dbl> 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, ~
> $ sex       <chr> "female", "male", "female", "male", "female", "male", "femal~
> $ numwords  <dbl> 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, ~
> $ numutts   <dbl> 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 2, 74, 6~
> $ avgutt    <dbl> 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6~
> $ medianutt <dbl> 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 1, 7, 5, 7, 5, 7, 5, ~
\end{verbatim}

\end{example}

Now we have a dataset with 12 informative variables which describe the
utterances for each recording. There are variables about the recordings,
such as the \texttt{filename}, about the speakers, such as the
\texttt{sex} and \texttt{who}, and about the utterances, such as the
\texttt{utterance}, \texttt{utt\_num}, \emph{etc.}. Let's organize the
columns to read left to right from most general to most specific. Again
we turn to the \texttt{select()} function, this time including the
variables in the order we want them to appear in the dataset. We will
take this opportunity to rename some of the variable names so that they
are more informative.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-rename-vars}{}\label{exm-cd-cabnc-rename-vars}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename variables}
\NormalTok{cabnc\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}
    \AttributeTok{doc\_id =}\NormalTok{ filename,}
\NormalTok{    utt\_num,}
    \AttributeTok{utt\_start =}\NormalTok{ startTime,}
    \AttributeTok{utt\_end =}\NormalTok{ endTime,}
\NormalTok{    utterance,}
    \AttributeTok{part\_id =}\NormalTok{ who,}
    \AttributeTok{part\_age =}\NormalTok{ monthage,}
    \AttributeTok{part\_sex =}\NormalTok{ sex,}
    \AttributeTok{num\_words =}\NormalTok{ numwords,}
    \AttributeTok{num\_utts =}\NormalTok{ numutts,}
    \AttributeTok{avg\_utt\_len =}\NormalTok{ avgutt,}
    \AttributeTok{median\_utt\_len =}\NormalTok{ medianutt}
\NormalTok{  )}

\CommentTok{\# Preview the dataset}
\FunctionTok{glimpse}\NormalTok{(cabnc\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 235,901
> Columns: 12
> $ doc_id         <chr> "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE000", "KB0RE0~
> $ utt_num        <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~
> $ utt_start      <dbl> 0.208, 2.656, 2.896, 3.328, 5.088, 6.208, 8.320, 8.480,~
> $ utt_end        <dbl> 2.67, 2.90, 3.33, 5.26, 6.02, 8.50, 9.31, 11.23, 14.34,~
> $ utterance      <chr> "You enjoyed yourself in America", "Eh", "did you", "Oh~
> $ part_id        <chr> "PS002", "PS006", "PS002", "PS006", "PS002", "PS006", "~
> $ part_age       <dbl> 721, 601, 721, 601, 721, 601, 721, 601, 721, 601, 721, ~
> $ part_sex       <chr> "female", "male", "female", "male", "female", "male", "~
> $ num_words      <dbl> 759, 399, 759, 399, 759, 399, 759, 399, 759, 399, 759, ~
> $ num_utts       <dbl> 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 64, 74, 2, ~
> $ avg_utt_len    <dbl> 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.26, 6.23, 10.~
> $ median_utt_len <dbl> 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 1, 7, 5, 7, 5, 7~
\end{verbatim}

\end{example}

The variable order is organized after running
Example~\ref{exm-cd-cabnc-rename-vars}. Now let's sort the rows by
\texttt{doc\_id} and \texttt{utt\_num} so that the utterances are in
order. The \texttt{arrange()} function takes a data frame and a list of
variables to sort by, in the order they are listed.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-sort-rows}{}\label{exm-cd-cabnc-sort-rows}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort rows}
\NormalTok{cabnc\_tbl }\OtherTok{\textless{}{-}}
\NormalTok{  cabnc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(doc\_id, utt\_num)}

\CommentTok{\# Preview the dataset}
\NormalTok{cabnc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 12
>    doc_id   utt_num utt_start utt_end utterance        part_id part_age part_sex
>    <chr>      <dbl>     <dbl>   <dbl> <chr>            <chr>      <dbl> <chr>   
>  1 KB0RE000       0     0.208    2.67 You enjoyed you~ PS002        721 female  
>  2 KB0RE000       1     2.66     2.90 Eh               PS006        601 male    
>  3 KB0RE000       2     2.90     3.33 did you          PS002        721 female  
>  4 KB0RE000       3     3.33     5.26 Oh I covered a ~ PS006        601 male    
>  5 KB0RE000       4     5.09     6.02 Oh very good ye~ PS002        721 female  
>  6 KB0RE000       5     6.21     8.50 Er saw Mary and~ PS006        601 male    
>  7 KB0RE000       6     8.32     9.31 Yes you did      PS002        721 female  
>  8 KB0RE000       7     8.48    11.2  in fact the who~ PS006        601 male    
>  9 KB0RE000       8    10.3     14.3  Oh very nice ve~ PS002        721 female  
> 10 KB0RE000       9    14.3     16.0  It is horrible ~ PS006        601 male    
> # i 4 more variables: num_words <dbl>, num_utts <dbl>, avg_utt_len <dbl>,
> #   median_utt_len <dbl>
\end{verbatim}

\end{example}

Applying the sorting in Example~\ref{exm-cd-cabnc-sort-rows}, we can see
that the utterances are now our desired order. We have now completed the
curation of the structured dataset aiming to provide a base for further
analysis into the recorded utterances of speakers from the CABNC corpus.

\hypertarget{write-dataset-1}{%
\subsection{Write dataset}\label{write-dataset-1}}

Let's now write this dataset to disk. Again, since this is a dataset we
have created, we will store the dataset in the \emph{data/derived/}
directory. To keep the CABNC separate from any other datasets that we
may need for our research project, I will create a subdirectory called
\emph{cabnc/} to store the dataset. Into this directory we can write a
CSV file with our \texttt{cabnc\_tbl} data frame with the
\texttt{write\_csv()} function, as seen in
Example~\ref{exm-cd-cabnc-write-csv}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-write-csv}{}\label{exm-cd-cabnc-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create cabnc directory}
\FunctionTok{dir\_create}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/cabnc"}\NormalTok{)}

\CommentTok{\# Write dataset to disk}
\NormalTok{cabnc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{write\_csv}\NormalTok{(}\AttributeTok{path =} \StringTok{"../data/derived/cabnc/cabnc\_curated.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

The final step is to create a data dictionary file for this dataset. We
can do this using the \texttt{create\_data\_dictionary()} function,
passing the data frame object name and a path to the directory where we
want to store the data dictionary. The
\texttt{create\_data\_dictionary()} function will create a CSV file with
a data dictionary template in the specified directory.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-create-data-dict}{}\label{exm-cd-cabnc-create-data-dict}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create data dictionary}
\FunctionTok{create\_data\_dictionary}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ cabnc\_tbl,}
  \AttributeTok{file\_path =} \StringTok{"../data/derived/cabnc\_curated\_dd.csv"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

After running the code in Example~\ref{exm-cd-cabnc-create-data-dict},
we can open the \emph{cabnc\_curated\_dd.csv} file in the
\emph{data/derived/cabnc/} directory and fill in the data dictionary
template. An example of an edited data dictionary is shown in
Table~\ref{tbl-cd-cabnc-dd-example}.

\hypertarget{tbl-cd-cabnc-dd-example}{}
\begin{table}
\caption{\label{tbl-cd-cabnc-dd-example}Data dictionary example for the curated CABNC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
doc\_id & Document ID & categorical & Unique identifier for each document\\
\hline
utt\_num & Utterance Number & ordinal & Sequential number assigned to each utterance in a document\\
\hline
utt\_start & Utterance Start Time & numeric & Timestamp indicating the start time of an utterance, measured in seconds\\
\hline
utt\_end & Utterance End Time & numeric & Timestamp indicating the end time of an utterance, measured in seconds\\
\hline
utterance & Utterance Text & categorical & Textual content of an utterance\\
\hline
part\_id & Participant ID & categorical & Unique identifier for each participant\\
\hline
part\_age & Participant Age & numeric & Age of the participant, measured in months\\
\hline
part\_sex & Participant Sex & categorical & Sex of the participant. (male/ female)\\
\hline
num\_words & Number of Words & numeric & Total number of words in utterances in the document\\
\hline
num\_utts & Number of Utterances & numeric & Total number of utterances in the document\\
\hline
avg\_utt\_len & Average Utterance Length & numeric & Average length of an utterance in the document, measured in words\\
\hline
median\_utt\_len & Median Utterance Length & numeric & Median length of an utterance in the document, measured in words\\
\hline
\end{tabular}
\end{table}

And the final directory structure for the \emph{data/} directory is
shown in Example~\ref{exm-cd-cabnc-final-dir}.

\begin{example}[]\protect\hypertarget{exm-cd-cabnc-final-dir}{}\label{exm-cd-cabnc-final-dir}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{    cabnc\_curated\_dd.csv}
\ExtensionTok{}\NormalTok{    cabnc/}
\ExtensionTok{}\NormalTok{        cabnc\_curated.csv}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ cabnc\_do.csv}
    \ExtensionTok{}\NormalTok{ cabnc/}
        \ExtensionTok{}\NormalTok{ participants.csv}
        \ExtensionTok{}\NormalTok{ token\_types.csv}
        \ExtensionTok{}\NormalTok{ tokens.csv}
        \ExtensionTok{}\NormalTok{ transcripts.csv}
        \ExtensionTok{}\NormalTok{ utterances.csv}
\end{Highlighting}
\end{Shaded}

\end{example}

\hypertarget{semi-structured}{%
\section{Semi-structured}\label{semi-structured}}

At this point we have discussed curating unstructured data and
structured datasets. Between these two extremes falls semi-structured
data. And as the name suggests, it is a hybrid between unstructured and
structured data. This means that there will be important structured
metadata included with unstructured elements. The file formats and
approaches to encoding the structured aspects of the data vary widely
from resource to resource and therefore often requires more detailed
attention to the structure of the data and often includes more
sophisticated programming strategies to curate the data to produce a
tidy dataset.

\hypertarget{reading-data-2}{%
\subsection{Reading data}\label{reading-data-2}}

The file formats associated with semi-structured data include a wide
range. These include file formats conducive to more structured-leaning
data, such as XML, HTML, and JSON, and file formats with more
unstructured-leaning data, such as annotated TXT files. Annotated TXT
files may in fact appear with the \emph{.txt} extension, but may also
appear with other, sometimes resource-specific, extensions, such as
\emph{.utt} for the Switchboard Dialogue Act Corpus or \emph{.cha} for
the CHILDES corpus annotation files, for example.

The more structured file formats use standard conventions and therefore
can be read into an R session with format-specific functions. Say, for
example, we are working with data in a JSON file format, as in
File~\ref{lst-cd-json-example}. We can read the data into an R session
with the \texttt{read\_json()} function from the \texttt{jsonlite}
package (\protect\hyperlink{ref-R-jsonlite}{Ooms 2023}). For XML and
HTML files, the \texttt{rvest} package provides the \texttt{read\_xml()}
and \texttt{read\_html()} functions.

\begin{codelisting}

\caption{\texttt{data.json}: Example JSON file}

\hypertarget{lst-cd-json-example}{%
\label{lst-cd-json-example}}%
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\{}
    \DataTypeTok{"data"}\FunctionTok{:} \OtherTok{[}
        \FunctionTok{\{}
            \DataTypeTok{"speaker"}\FunctionTok{:} \StringTok{"Alice"}\FunctionTok{,}
            \DataTypeTok{"age"}\FunctionTok{:} \DecValTok{27}\FunctionTok{,}
            \DataTypeTok{"sex"}\FunctionTok{:} \StringTok{"Female"}\FunctionTok{,}
            \DataTypeTok{"word\_form"}\FunctionTok{:} \StringTok{"running"}\FunctionTok{,}
            \DataTypeTok{"lemma\_form"}\FunctionTok{:} \StringTok{"run"}
        \FunctionTok{\}}\OtherTok{,}
        \FunctionTok{\{}
            \DataTypeTok{"speaker"}\FunctionTok{:} \StringTok{"Bob"}\FunctionTok{,}
            \DataTypeTok{"age"}\FunctionTok{:} \DecValTok{35}\FunctionTok{,}
            \DataTypeTok{"sex"}\FunctionTok{:} \StringTok{"Male"}\FunctionTok{,}
            \DataTypeTok{"word\_form"}\FunctionTok{:} \StringTok{"jumped"}\FunctionTok{,}
            \DataTypeTok{"lemma\_form"}\FunctionTok{:} \StringTok{"jump"}
        \FunctionTok{\}}\OtherTok{,}
        \FunctionTok{\{}
            \DataTypeTok{"speaker"}\FunctionTok{:} \StringTok{"Charlie"}\FunctionTok{,}
            \DataTypeTok{"age"}\FunctionTok{:} \DecValTok{42}\FunctionTok{,}
            \DataTypeTok{"sex"}\FunctionTok{:} \StringTok{"Male"}\FunctionTok{,}
            \DataTypeTok{"word\_form"}\FunctionTok{:} \StringTok{"singing"}\FunctionTok{,}
            \DataTypeTok{"lemma\_form"}\FunctionTok{:} \StringTok{"sing"}
        \FunctionTok{\}}\OtherTok{,}
        \FunctionTok{\{}
            \DataTypeTok{"speaker"}\FunctionTok{:} \StringTok{"Diane"}\FunctionTok{,}
            \DataTypeTok{"age"}\FunctionTok{:} \DecValTok{32}\FunctionTok{,}
            \DataTypeTok{"sex"}\FunctionTok{:} \StringTok{"Female"}\FunctionTok{,}
            \DataTypeTok{"word\_form"}\FunctionTok{:} \StringTok{"walked"}\FunctionTok{,}
            \DataTypeTok{"lemma\_form"}\FunctionTok{:} \StringTok{"walk"}
        \FunctionTok{\}}
    \OtherTok{]}
\FunctionTok{\}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

Semi-structured data in TXT files can be read either as an entire file
with \texttt{read\_file()} or line-by-line with \texttt{read\_lines()}.
The choice of which approach to take depends on the structure of the
data. If the data structure is line-based, then \texttt{read\_lines()}
often makes more sense than \texttt{read\_file()}. However, in some
cases, the data may be structured in a way that requires the entire file
to be read into an R session and then subsequently parsed.

\hypertarget{orientation-2}{%
\subsection{Orientation}\label{orientation-2}}

To provide an example of the curation process using semi-structured
data, we will work with the ENNTT corpus, introduced in
Section~\ref{sec-aa-predict}. Let's look at the directory structure for
the ENNTT corpus in Example~\ref{exm-cd-enntt-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-structure}{}\label{exm-cd-enntt-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{ original/}
    \ExtensionTok{}\NormalTok{ enntt\_do.csv}
    \ExtensionTok{}\NormalTok{ enntt/}
        \ExtensionTok{}\NormalTok{ natives.dat}
        \ExtensionTok{}\NormalTok{ natives.tok}
        \ExtensionTok{}\NormalTok{ nonnatives.dat}
        \ExtensionTok{}\NormalTok{ nonnatives.tok}
        \ExtensionTok{}\NormalTok{ translations.dat}
        \ExtensionTok{}\NormalTok{ translations.tok}
\end{Highlighting}
\end{Shaded}

\end{example}

We now inspect the data origin file for the ENNTT corpus,
\emph{enntt\_do.csv}, in Table~\ref{tbl-cd-enntt-do}.

\hypertarget{tbl-cd-enntt-do}{}
\begin{table}
\caption{\label{tbl-cd-enntt-do}Data origin file for the ENNTT corpus. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
attribute & description\\
\hline
Resource name & Europarl corpus of Native, Non-native and Translated Texts - ENNTT\\
\hline
Data source & https://github.com/senisioi/enntt-release\\
\hline
Data sampling frame & English, European Parliament texts, transcribed discourse, political genre\\
\hline
Data collection date(s) & Not specified in the repository\\
\hline
Data format & .tok, .dat\\
\hline
Data schema & *.tok files contain the actual text; *.dat files contain the annotations corresponding to each line in the *.tok files.\\
\hline
License & Not specified. Contact the authors for more information.\\
\hline
Attribution & Nisioi, S., Rabinovich, E., Dinu, L. P., \& Wintner, S. (2016). A corpus of native, non-native and translated texts. Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).\\
\hline
\end{tabular}
\end{table}

According to the data origin file, there are two important file types,
\emph{.dat} and \emph{.tok}. The \emph{.dat} files contain annotations
and the \emph{.tok} files contain the actual text. Let's inspect the
first couple of lines in the \emph{.dat} file for the native speakers,
\emph{nonnatives.dat}, in File~\ref{lst-cd-enntt-nonnatives-dat}.

\begin{codelisting}

\caption{\texttt{../data/original/enntt/nonnatives.dat}: Example
\emph{.dat} file for the non-native speakers.}

\hypertarget{lst-cd-enntt-nonnatives-dat}{%
\label{lst-cd-enntt-nonnatives-dat}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\textless{}LINE STATE="Poland" MEPID="96779" LANGUAGE="EN" NAME="Danuta Hbner," SEQ\_SPEAKER\_ID="184" SESSION\_ID="ep{-}05{-}11{-}17"/\textgreater{}}
\NormalTok{\textless{}LINE STATE="Poland" MEPID="96779" LANGUAGE="EN" NAME="Danuta Hbner," SEQ\_SPEAKER\_ID="184" SESSION\_ID="ep{-}05{-}11{-}17"/\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

We see that the \emph{.dat} file contains annotations for various
session and speaker attributes. The format of the annotations is
XML-like. XML is a form of markup language, such as YAML, JSON,
\emph{etc.} \textbf{Markup languages} are used to annotate text with
additional information about the structure, meaning, and/ or
presentation of text. In XML, structure is built up by nesting of nodes.
The nodes are named with tags, which are enclosed in angle brackets,
\texttt{\textless{}} and \texttt{\textgreater{}}. Nodes are opened with
\texttt{\textless{}TAG\textgreater{}} and closed with
\texttt{\textless{}/TAG\textgreater{}}. In Example~\ref{exm-cd-xml} we
see an example of a simple XML file structure.

\begin{example}[]\protect\hypertarget{exm-cd-xml}{}\label{exm-cd-xml}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{\textless{}?xml}\OtherTok{ version=}\StringTok{"1.0"}\OtherTok{ encoding=}\StringTok{"UTF{-}8"}\FunctionTok{?\textgreater{}}
\NormalTok{\textless{}}\KeywordTok{book}\OtherTok{ category=}\StringTok{"fiction"}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{title}\OtherTok{ lang=}\StringTok{"en"}\NormalTok{\textgreater{}The Catcher in the Rye\textless{}/}\KeywordTok{title}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{author}\NormalTok{\textgreater{}J.D. Salinger\textless{}/}\KeywordTok{author}\NormalTok{\textgreater{}}
\NormalTok{  \textless{}}\KeywordTok{year}\NormalTok{\textgreater{}1951\textless{}/}\KeywordTok{year}\NormalTok{\textgreater{}}
\NormalTok{\textless{}/}\KeywordTok{book}\NormalTok{\textgreater{}}
\end{Highlighting}
\end{Shaded}

\end{example}

In Example~\ref{exm-cd-xml} there are four nodes, three of which are
nested inside of the \texttt{\textless{}book\textgreater{}} node. The
\texttt{\textless{}book\textgreater{}} node in this example is the root
node. XML files require a root node. Nodes can also have attributes,
such as the \texttt{category} attribute in the
\texttt{\textless{}book\textgreater{}} node, but they are not required.
Furthermore, XML files also require a declaration, which is the first
line in Example~\ref{exm-cd-xml}. The declaration specifies the version
of XML used and the encoding.

So the \emph{.dat} file is not strict XML, but is similar in that it
contains nodes and attributes. An XML variant you a likely familiar
with, HTML, has more relaxed rules than XML. HTML is a markup language
used to annotate text with information about the organization and
presentation of text on the web that does not require a root node or a
declaration --much like our \emph{.dat} file. So suffice it to say that
the \emph{.dat} file can safely be treated as HTML.

And the \emph{.tok} file for the native speakers, \emph{nonnatives.tok},
in File~\ref{lst-cd-enntt-nonnatives-tok}, shows the actual text for
each line in the corpus.

\begin{codelisting}

\caption{\texttt{../data/original/enntt/nonnatives.tok}: Example
\emph{.tok} file for the non-native speakers.}

\hypertarget{lst-cd-enntt-nonnatives-tok}{%
\label{lst-cd-enntt-nonnatives-tok}}%
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{The Commission is following with interest the planned construction of a nuclear power plant in Akkuyu , Turkey and recognises the importance of ensuring that the construction of the new plant follows the highest internationally accepted nuclear safety standards . }
\NormalTok{According to our information , the decision on the selection of a bidder has not been taken yet .}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

In a study in which we are interested in contrasting the language of
natives and non-natives, we will want to combine the \emph{.dat} and
\emph{.tok} files for these groups of speakers and then join these
datasets into one.

The question is what attributes we want to include in the curated
dataset. Given the research focus, we will not need the
\texttt{LANGUAGE} or \texttt{NAME} attributes. We may want to modify the
attribute names so they are a bit more descriptive.

An idealized version of the dataset based on this criteria is shown in
Table~\ref{tbl-cd-enntt-ideal}.

\hypertarget{tbl-cd-enntt-ideal}{}
\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1000}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1000}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1000}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1000}}
  >{\centering\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.5000}}@{}}
\caption{\label{tbl-cd-enntt-ideal}Idealized version of the ENNTT
corpus.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
session\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
speaker\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
state
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
session\_seq
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
session\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
speaker\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
state
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
session\_seq
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ep-05-11-17 & 96779 & Poland & non-native & 184 & The Commission is
following with interest the planned construction of a nuclear power
plant in Akkuyu , Turkey and recognises the importance of ensuring that
the construction of the new plant follows the highest internationally
accepted nuclear safety standards . \\
\ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} & \ldots{} \\
\end{longtable}

\hypertarget{tidy-the-data-1}{%
\subsection{Tidy the data}\label{tidy-the-data-1}}

Now that we have a better understanding of the corpus data and our
target curated dataset structure, let's work to extract and organize the
data from the native and non-native files into one dataset.

The general approach we will take is, for native and then non-natives,
to read in the \emph{.dat} file as an HTML file and then extract the
line nodes and their attributes combining them into a data frame. Then
we'll read in the \emph{.tok} file as a text file and then combine the
two into a single data frame. After producing a native and non-native
data frame, we will combine them into one data frame to write and
document as our curated dataset.

Starting with the natives, we read in the \emph{.dat} file as an XML
file with the \texttt{read\_html()} function and then extract the line
nodes with the \texttt{html\_elements()} function as in
Example~\ref{exm-cd-enntt-read-xml}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-read-xml}{}\label{exm-cd-enntt-read-xml}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(rvest)}

\CommentTok{\# Read in *.dat* file as HTML}
\NormalTok{ns\_dat\_lines }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_html}\NormalTok{(}\StringTok{"../data/original/enntt/natives.dat"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{html\_elements}\NormalTok{(}\StringTok{"line"}\NormalTok{)}

\CommentTok{\# Inspect}
\FunctionTok{class}\NormalTok{(ns\_dat\_lines)}
\FunctionTok{typeof}\NormalTok{(ns\_dat\_lines)}
\FunctionTok{length}\NormalTok{(ns\_dat\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "xml_nodeset"
> [1] "list"
> [1] 116341
\end{verbatim}

\end{example}

When can see that the \texttt{ns\_dat\_lines} object is a special type
of list, \texttt{xml\_nodeset} which contains 116,341 line nodes. Let's
now jump out of sequence and read in the \emph{.tok} file as a text
file, in Example~\ref{exm-cd-enntt-read-lines}, again by lines using
\texttt{read\_lines()}, and compare the two to make sure that our
approach will work.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-read-lines}{}\label{exm-cd-enntt-read-lines}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in *.tok* file by lines}
\NormalTok{ns\_tok\_lines }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_lines}\NormalTok{(}\StringTok{"data/enntt/original/natives.tok"}\NormalTok{)}

\CommentTok{\# Inspect}
\FunctionTok{class}\NormalTok{(ns\_tok\_lines)}
\FunctionTok{typeof}\NormalTok{(ns\_tok\_lines)}
\FunctionTok{length}\NormalTok{(ns\_tok\_lines)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "character"
> [1] "character"
> [1] 116341
\end{verbatim}

\end{example}

We do, in fact, have the same number of lines in the \emph{.dat} and
\emph{.tok} files. So we can proceed with extracting the attributes from
the line nodes and combining them with the text from the \emph{.tok}
file.

Let's start by listing the attributes of the first line node in the
\texttt{ns\_dat\_lines} object. We use the \texttt{html\_attrs()}
function to get the attribute names and the values, as in
Example~\ref{exm-cd-enntt-list-attributes}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-list-attributes}{}\label{exm-cd-enntt-list-attributes}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# List attributes of first line node}
\NormalTok{ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{html\_attrs}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>             state             mepid          language              name 
>  "United Kingdom"            "2099"              "EN" "Evans, Robert J" 
>    seq_speaker_id        session_id 
>               "2"     "ep-00-01-17"
\end{verbatim}

\end{example}

No surprise here, these are the same attributes we saw in the
\emph{.dat} file preview in File~\ref{lst-cd-enntt-nonnatives-dat}. At
this point, it's good to make a plan on how to associate the attribute
names with the column names in our curated dataset.

\begin{itemize}
\tightlist
\item
  \texttt{session\_id} = \texttt{session\_id}
\item
  \texttt{speaker\_id} = \texttt{MEPID}
\item
  \texttt{state} = \texttt{state}
\item
  \texttt{session\_seq} = \texttt{seq\_speaker\_id}
\end{itemize}

We can do this one attribute at a time using the \texttt{html\_attr()}
function and then combine them into a data frame with the
\texttt{tibble()} function as in
Example~\ref{exm-cd-enntt-extract-attributes}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes}{}\label{exm-cd-enntt-extract-attributes}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract attributes from first line node}
\NormalTok{session\_id }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{state }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

\CommentTok{\# Combine into data frame}
\FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 2099       United Kingdom 2
\end{verbatim}

\end{example}

The results from Example~\ref{exm-cd-enntt-extract-attributes} show that
the attributes have been extracted and mapped to our idealized column
names, but this would be tedious to do for each line node. A function to
extract attributes and values from a line and add them to a data frame
would help simplify this process. The function in
Example~\ref{exm-cd-enntt-extract-attributes-function} does just that.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes-function}{}\label{exm-cd-enntt-extract-attributes-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Function to extract attributes from line node}
\NormalTok{extract\_dat\_attrs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(line\_node) \{}
\NormalTok{  session\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{  speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{  state }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{  session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

  \FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

It's a good idea to test out the function to verify that it works as
expected. We can do this by passing the various indices to the
\texttt{ns\_dat\_lines} object to the function as in
Example~\ref{exm-cd-enntt-test-extract-attributes-function}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-test-extract-attributes-function}{}\label{exm-cd-enntt-test-extract-attributes-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test function}
\FunctionTok{extract\_dat\_attrs}\NormalTok{(ns\_dat\_lines[[}\DecValTok{1}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 2099       United Kingdom 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{extract\_dat\_attrs}\NormalTok{(ns\_dat\_lines[[}\DecValTok{20}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-17 1309       United Kingdom 40
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{extract\_dat\_attrs}\NormalTok{(ns\_dat\_lines[[}\DecValTok{100}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 4
>   session_id  speaker_id state          session_seq
>   <chr>       <chr>      <chr>          <chr>      
> 1 ep-00-01-18 4549       United Kingdom 28
\end{verbatim}

\end{example}

Looks like the \texttt{extract\_dat\_attrs()} function is ready for
prime-time. Let's now apply it to all of the line nodes in the
\texttt{ns\_dat\_lines} object using the \texttt{map\_dfr()} function
from the \texttt{purrr} package as in
Example~\ref{exm-cd-enntt-extract-attributes-all}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-extract-attributes-all}{}\label{exm-cd-enntt-extract-attributes-all}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract attributes from all line nodes}
\NormalTok{ns\_dat\_attrs }\OtherTok{\textless{}{-}} 
\NormalTok{  ns\_dat\_lines }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{map\_dfr}\NormalTok{(extract\_dat\_attrs)}

\CommentTok{\# Inspect}
\FunctionTok{glimpse}\NormalTok{(ns\_dat\_attrs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 4
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
\end{verbatim}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

The \texttt{map*()} functions from the \texttt{purrr} package are a
family of functions that apply a function to each element of a vector,
list, or data frame. The \texttt{map\_dfr()} function is a variant of
the \texttt{map()} function that returns a data frame that is the result
of row-binding the results, hence \texttt{\_dfr}.

\end{tcolorbox}

We can see that the \texttt{ns\_dat\_attrs} object is a data frame with
116,341 rows and 4 columns, just has we expected. We can now combine the
\texttt{ns\_dat\_attrs} data frame with the \texttt{ns\_tok\_lines}
vector to create a single data frame with the attributes and the text.
This is done with the \texttt{mutate()} function assigning the
\texttt{ns\_tok\_lines} vector to a new column named \texttt{text} as in
Example~\ref{exm-cd-enntt-combine-attributes-text}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-attributes-text}{}\label{exm-cd-enntt-combine-attributes-text}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combine attributes and text}
\NormalTok{ns\_dat }\OtherTok{\textless{}{-}} 
\NormalTok{  ns\_dat\_attrs }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text =}\NormalTok{ ns\_tok\_lines)}

\CommentTok{\# Inspect}
\FunctionTok{glimpse}\NormalTok{(ns\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 5
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
> $ text        <chr> "You will be aware from the press and television that ther~
\end{verbatim}

\end{example}

This is the data for the native speakers. We can now repeat this process
for the non-native speakers, \emph{or} we can create a function to do it
for us. Let's explore the later option.

This is the data for the native speakers. We can now repeat this process
for the non-native speakers, or we can create a function to do it for
us. Let's explore the later option.

I will name this function \texttt{combine\_dat\_tok()} and it will take
two arguments: \texttt{dat\_file} and \texttt{tok\_file}. The
\texttt{dat\_file} argument will be the path to the \emph{.dat} file and
the \texttt{tok\_file} argument will be the path to the \emph{.tok}
file. The function first reads both the files into R as lines. Then it
extracts the attributes from the \emph{.dat} file using the
\texttt{extract\_dat\_attrs()} function we created earlier. Finally, it
combines the attributes with the text from the \emph{.tok} file and
returns a data frame.

Note that there is at least one tweak we need to make to our existing
code base if we plan on combining the native and non-native data. As our
function stands, it will return a data frame with the same column names
for both the native and non-native data. We will need to add a column to
which we will indicate what type of data it is (native or non-native).
This is where the \texttt{type} column we planned earlier comes in.
There are multiple ways to do this. We could add a column to the data
frame before we return it, or we could add a column to the data frame
after we return it. I will opt for the former and embed it inside of our
function \texttt{combine\_dat\_tok()}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-dat-tok-function}{}\label{exm-cd-enntt-combine-dat-tok-function}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(rvest) }\CommentTok{\# for reading/parsing HTML}
\FunctionTok{library}\NormalTok{(purrr) }\CommentTok{\# for mapping functions}
\FunctionTok{library}\NormalTok{(fs) }\CommentTok{\# for working with files}

\CommentTok{\# Function to combine *.dat* and *.tok* files}
\NormalTok{combine\_dat\_tok }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dat\_file, tok\_file) \{}
  \CommentTok{\# Read in files by lines}
\NormalTok{  dat }\OtherTok{\textless{}{-}}
    \FunctionTok{read\_html}\NormalTok{(dat\_file) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{"line"}\NormalTok{)}
\NormalTok{  tok }\OtherTok{\textless{}{-}} \FunctionTok{read\_lines}\NormalTok{(tok\_file)}

  \CommentTok{\# Create function to extract attributes}
\NormalTok{  extract\_dat\_attrs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(line\_node) \{}
\NormalTok{    session\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"session\_id"}\NormalTok{)}
\NormalTok{    speaker\_id }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"mepid"}\NormalTok{)}
\NormalTok{    state }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"state"}\NormalTok{)}
\NormalTok{    session\_seq }\OtherTok{\textless{}{-}}\NormalTok{ line\_node }\SpecialCharTok{|\textgreater{}} \FunctionTok{html\_attr}\NormalTok{(}\StringTok{"seq\_speaker\_id"}\NormalTok{)}

    \FunctionTok{tibble}\NormalTok{(session\_id, speaker\_id, state, session\_seq)}
\NormalTok{  \}}

  \CommentTok{\# Apply function to all line nodes}
\NormalTok{  dat\_attrs }\OtherTok{\textless{}{-}}
\NormalTok{    dat }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{map\_dfr}\NormalTok{(extract\_dat\_attrs)}

  \CommentTok{\# Combine attrs and text into data frame}
\NormalTok{  dataset }\OtherTok{\textless{}{-}}
\NormalTok{    dat\_attrs }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{bind\_cols}\NormalTok{(}\AttributeTok{text =}\NormalTok{ tok)}

  \CommentTok{\# Add type column with value of file name (w/o extension)}
\NormalTok{  dataset }\OtherTok{\textless{}{-}} 
\NormalTok{    dataset }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \FunctionTok{path\_file}\NormalTok{(dat\_file) }\SpecialCharTok{|\textgreater{}} \FunctionTok{path\_ext\_remove}\NormalTok{())}

  \CommentTok{\# Return data frame}
  \FunctionTok{return}\NormalTok{(dataset)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\end{example}

Apply the \texttt{combine\_dat\_tok()} function to read in the data for
the native speakers and the non-native speakers in a few lines of code,
as in Example~\ref{exm-cd-enntt-combine-dat-tok}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-combine-dat-tok}{}\label{exm-cd-enntt-combine-dat-tok}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Native speakers}
\NormalTok{ns\_dat }\OtherTok{\textless{}{-}}
  \FunctionTok{combine\_dat\_tok}\NormalTok{(}
    \AttributeTok{dat\_file =} \StringTok{"../data/original/enntt/natives.dat"}\NormalTok{,}
    \AttributeTok{tok\_file =} \StringTok{"../data/original/enntt/natives.tok"}
\NormalTok{  )}
\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(ns\_dat)}

\CommentTok{\# Non{-}native speakers}
\NormalTok{nns\_dat }\OtherTok{\textless{}{-}}
  \FunctionTok{combine\_dat\_tok}\NormalTok{(}
    \AttributeTok{dat\_file =} \StringTok{"../data/original/enntt/nonnatives.dat"}\NormalTok{,}
    \AttributeTok{tok\_file =} \StringTok{"../data/original/enntt/nonnatives.tok"}
\NormalTok{  )}
\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(nns\_dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 116,341
> Columns: 6
> $ session_id  <chr> "ep-00-01-17", "ep-00-01-17", "ep-00-01-17", "ep-00-01-17"~
> $ speaker_id  <chr> "2099", "2099", "2099", "4548", "4548", "4541", "4541", "4~
> $ state       <chr> "United Kingdom", "United Kingdom", "United Kingdom", "Uni~
> $ session_seq <chr> "2", "2", "2", "4", "4", "12", "12", "12", "12", "12", "12~
> $ text        <chr> "You will be aware from the press and television that ther~
> $ type        <chr> "natives", "natives", "natives", "natives", "natives", "na~
\end{verbatim}

\begin{verbatim}
> Rows: 29,734
> Columns: 6
> $ session_id  <chr> "ep-00-01-18", "ep-00-01-18", "ep-00-01-18", "ep-00-01-18"~
> $ speaker_id  <chr> "653", "653", "653", "653", "653", "653", "653", "653", "6~
> $ state       <chr> "Belgium", "Belgium", "Belgium", "Belgium", "Belgium", "Be~
> $ session_seq <chr> "157", "157", "157", "157", "157", "157", "157", "157", "1~
> $ text        <chr> "The Commission is following with interest the planned con~
> $ type        <chr> "nonnatives", "nonnatives", "nonnatives", "nonnatives", "n~
\end{verbatim}

\end{example}

The last step to curate the dataset is to combine the native and
non-native data into a single data frame. We can do this using the
\texttt{bind\_rows()} function from the \texttt{dplyr} package, as
essentially we are just stacking the rows from each data frame on top of
each other.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combine native and non{-}native data}
\NormalTok{enntt\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_rows}\NormalTok{(ns\_dat, nns\_dat)}
\end{Highlighting}
\end{Shaded}

The \texttt{ennnt\_tbl} data frame is the curated dataset. We can
perform some data checks to make everything looks good and then proceed
to writing and documenting the dataset.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

What data checks could we perform to ensure that the data is in the
format we expect? What are some of the things we should be looking for?

\end{tcolorbox}

\hypertarget{write-dataset-2}{%
\subsection{Write dataset}\label{write-dataset-2}}

As we have done for the other curated datasets, we will write our
curated dataset to a \emph{.csv} file. We will use the
\texttt{write\_csv()} function and name the file appropriately, as in
Example~\ref{exm-cd-enntt-write-csv}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-write-csv}{}\label{exm-cd-enntt-write-csv}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create directory}
\FunctionTok{dir\_create}\NormalTok{(}\StringTok{"../data/derived/enntt"}\NormalTok{)}

\CommentTok{\# Write dataset to *.csv* file}
\FunctionTok{write\_csv}\NormalTok{(enntt\_tbl, }\StringTok{"../data/derived/enntt/enntt\_curated.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

Document the dataset in a data dictionary using the
\texttt{create\_data\_dictionary()} function from the \texttt{qtalrkit}
package, as in Example~\ref{exm-cd-enntt-create-data-dictionary}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-create-data-dictionary}{}\label{exm-cd-enntt-create-data-dictionary}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(qtalrkit)}

\CommentTok{\# Create data dictionary}
\FunctionTok{create\_data\_dictionary}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ enntt\_tbl,}
  \AttributeTok{file\_path =} \StringTok{"../data/derived/enntt\_curated\_dd.csv"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

After editing the data dictionary file to include the appropriate
information, we will have something similar to
Table~\ref{tbl-cd-enntt-data-dictionary}.

\hypertarget{tbl-cd-enntt-data-dictionary}{}
\begin{table}
\caption{\label{tbl-cd-enntt-data-dictionary}Data dictionary for the curated ENNTT dataset }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
session\_id & Session ID & categorical & Unique identifier for each session\\
\hline
speaker\_id & Speaker ID & categorical & Unique identifier for each speaker\\
\hline
state & State & categorical & Name of the state or country the session is linked to\\
\hline
session\_seq & Session Sequence & ordinal & Sequence number in the session\\
\hline
text & Text & categorical & Text transcript of the session\\
\hline
type & Type & categorical & The type of the speaker, whether native or nonnative\\
\hline
\end{tabular}
\end{table}

The final step is to clean up and comment the code added to the
\emph{2-curate-datasets.qmd} file. The final directory structure is seen
in Example~\ref{exm-cd-enntt-final-directory-structure}.

\begin{example}[]\protect\hypertarget{exm-cd-enntt-final-directory-structure}{}\label{exm-cd-enntt-final-directory-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{project/}
\ExtensionTok{}\NormalTok{ code/}
\ExtensionTok{}\NormalTok{    1{-}acquire{-}data.qmd}
\ExtensionTok{}\NormalTok{    2{-}curate{-}data.qmd}
\ExtensionTok{}\NormalTok{    ...}
\ExtensionTok{}\NormalTok{ data/}
\ExtensionTok{}\NormalTok{    analysis/}
\ExtensionTok{}\NormalTok{    derived/}
\ExtensionTok{}\NormalTok{        enntt\_curated\_dd.csv}
\ExtensionTok{}\NormalTok{       enntt/}
\ExtensionTok{}\NormalTok{           enntt\_curated.csv}
\ExtensionTok{}\NormalTok{    original/}
\ExtensionTok{}\NormalTok{        enntt\_do.csv}
\ExtensionTok{}\NormalTok{        enntt/}
\ExtensionTok{}\NormalTok{            natives.dat}
\ExtensionTok{}\NormalTok{            natives.tok}
\ExtensionTok{}\NormalTok{            nonnatives.dat}
\ExtensionTok{}\NormalTok{            nonnatives.tok}
\ExtensionTok{}\NormalTok{            translations.dat}
\ExtensionTok{}\NormalTok{            translations.tok}
\ExtensionTok{}\NormalTok{ output/}
\ExtensionTok{}\NormalTok{    figures/}
\ExtensionTok{}\NormalTok{    reports/}
\ExtensionTok{}\NormalTok{    results/}
\ExtensionTok{}\NormalTok{    tables/}
\ExtensionTok{}\NormalTok{ README.md}
\ExtensionTok{}\NormalTok{ \_main.R}
\end{Highlighting}
\end{Shaded}

\end{example}

\hypertarget{summary-5}{%
\section*{Summary}\label{summary-5}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we looked at the process of structuring data into a
dataset. This included a discussion on three main types of data
--unstructured, structured, and semi-structured. The level of structure
of the original data(set) will vary from resource to resource and by the
same token so will the file format used to support the level of metadata
included. The results from data curation results in a dataset that is
saved separate from the original data to maintain modularity between
what the data(set) looked like before we intervene and afterwards. Since
there can be multiple analysis approaches applied the original data in a
research project, this curated dataset serves as the point of departure
for each of the subsequent datasets derived from the transformational
steps. In addition to the code we use to derived the curated dataset's
structure, we also include a data dictionary which documents the curated
dataset.

\hypertarget{activities-4}{%
\section*{Activities}\label{activities-4}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Add description of outcomes
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_7.html}{Organizing
and documenting datasets}\\
\textbf{How}: Read Recipe 7 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To rehearse methods for deriving tidying datasets to use a
the base for further project-specific purposes. We will explore how
regular expressions are helpful in developing strategies for matching,
extracting, and/ or replacing patterns in character sequences and how to
change the dimensions of a dataset to either expand or collapse columns
or rows.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/lin380/lab_7}{Pattern Matching
and Manipulate Datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 7.\\
\textbf{Why}: To gain experience working with coding strategies
reshaping data using tidyverse functions and regular expressions, to
practice reading/ writing data from/ to disk, and to implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-5}{%
\section*{Questions}\label{questions-5}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Conceptual questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Technical questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-transform-datasets}{%
\chapter{Transform datasets}\label{sec-transform-datasets}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

\begin{quote}
Nothing is lost. Everything is transformed.

--- Michael Ende, The Neverending Story
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\faIcon{wrench} to update the learning outcomes

\begin{itemize}
\tightlist
\item
  Understand the role of data transformation in a text analysis project.
\item
  Identify the main types of transformations used to prepare datasets
  for analysis.
\item
  Recognize the importance of planning and documenting the
  transformation process.
\end{itemize}

\end{tcolorbox}

In this chapter, we will focus on transforming a curated dataset to
refine and possibly expand its relational characteristics to align with
our research. I will approach the transformation process by breaking it
down into five subcategories: text normalization, variable recoding,
text tokenization, variable generation, and observation/ variable
merging. These categories are not sequential but may occur in any order
based on the researcher's evaluation of the dataset characteristics and
the desired outcome.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Swirl lesson}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Reshape dataset
rows, Reshape dataset columns}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: \ldots{}

\end{tcolorbox}

\hypertarget{sec-td-normalization}{%
\section{Normalization}\label{sec-td-normalization}}

The process of normalizing datasets in essence is to sanitize the values
of variable or set of variables such that there are no artifacts that
will contaminate subsequent processing. It may be the case that
non-linguistic metadata may require normalization but more often than
not linguistic information is the most common target for normalization
as text often includes artifacts from the acquisition process which will
not be desired in the analysis.

\hypertarget{sec-td-normalization-orientation}{%
\subsection{Orientation}\label{sec-td-normalization-orientation}}

To explore some of the strategies of normalization, we will look at the
Europarl Corpus. As we are working working towards transforming a
curated dataset, we will start by getting oriented to the dataset.
Following the reproducible research principles, I will assume that the
curated dataset and its associated data dictionary are in the
\emph{data/derived/} directory, as seen in
Example~\ref{exm-td-europarl-curated-structure}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-curated-structure}{}\label{exm-td-europarl-curated-structure}

~

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{data/}
\ExtensionTok{}\NormalTok{ analysis/}
\ExtensionTok{}\NormalTok{ derived/}
\ExtensionTok{}\NormalTok{    europarl\_curated\_dd.csv}
\ExtensionTok{}\NormalTok{    europarl/}
\ExtensionTok{}\NormalTok{        europarl\_curated.csv}
\ExtensionTok{}\NormalTok{ original/}
\end{Highlighting}
\end{Shaded}

\end{example}

The contents of the data dictionary for this dataset appears in
Table~\ref{tbl-td-europarl-dd}.

\hypertarget{tbl-td-europarl-dd}{}
\begin{table}
\caption{\label{tbl-td-europarl-dd}Data dictionary for the Europarl Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
doc\_id & Document ID & numeric & Unique identification number for each document\\
\hline
type & Document Type & categorical & Type of document; either 'Source' (Spanish) or 'Target' (English)\\
\hline
line\_id & Line ID & numeric & Unique identification number for each line in each document type\\
\hline
lines & Lines & categorical & Content of the lines in the document\\
\hline
\end{tabular}
\end{table}

This dataset contains transcribed source language (Spanish) and
translated target language (English) from the proceedings of the
European Parliament. The unit of observation is the \texttt{lines}
variable whose values are are lines of dialog.

Let's read in the dataset CSV file with \texttt{read\_csv()} and inspect
the first lines of the dataset with \texttt{slice\_head()} in
Example~\ref{exm-tb-europarl-preview}.

\begin{example}[]\protect\hypertarget{exm-tb-europarl-preview}{}\label{exm-tb-europarl-preview}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the dataset}
\NormalTok{europarl\_curated\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\AttributeTok{file =} \StringTok{"../data/derived/europarl\_curated.csv"}\NormalTok{)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>     doc_id type   line_id lines                                                 
>      <dbl> <chr>    <dbl> <chr>                                                 
>  1 1965735 Source       1 "Reanudacin del perodo de sesiones"                 
>  2       1 Target       1 "Resumption of the session"                           
>  3 1965736 Source       2 "Declaro reanudado el perodo de sesiones del Parlame~
>  4       2 Target       2 "I declare resumed the session of the European Parlia~
>  5 1965737 Source       3 "Como todos han podido comprobar, el gran \"efecto de~
>  6       3 Target       3 "Although, as you will have seen, the dreaded 'millen~
>  7 1965738 Source       4 "Sus Seoras han solicitado un debate sobre el tema ~
>  8       4 Target       4 "You have requested a debate on this subject in the c~
>  9 1965739 Source       5 "A la espera de que se produzca, de acuerdo con mucho~
> 10       5 Target       5 "In the meantime, I should like to observe a minute' ~
\end{verbatim}

\end{example}

Simply looking at the first 10 lines of this dataset gives us a clearer
sense of the dataset structure, but, in terms of normalization
procedures we might apply, it is likely not sufficient. We want to get a
sense of any potential inconsistencies in the dataset, in particular in
the \texttt{lines} variable. Since this is a large dataset with 3931468
observations, we will need to explore the dataset in manageable chunks.
The \texttt{slice\_sample()} function will allow us to randomly sample a
subset of the dataset of a certain number of observations specified by
the \texttt{n\ =} argument, as seen in
Example~\ref{exm-td-europarl-sample-1}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-sample-1}{}\label{exm-td-europarl-sample-1}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Randomly sample 5 observations}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 4
>    doc_id type   line_id lines                                                  
>     <dbl> <chr>    <dbl> <chr>                                                  
> 1 1979130 Source   13396 Nosotros no apostamos aqu, en este Parlamento, por lo~
> 2 3322228 Source 1356494 (SK) Seor Presidente, me he abstenido de votar porque~
> 3  140652 Target  140652 It is these last two, however, together with Finland, ~
> 4 2145661 Source  179927 Son las mujeres de esos mismos pases: Egipto, Somalia~
> 5 2269815 Source  304081 Permtanme aadir que, en nuestro ltimo llamamiento p~
\end{verbatim}

\end{example}

We should run the code in Example~\ref{exm-td-europarl-sample-1}
multiple times to get a sense of the variation in the dataset.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip}

R functions which return samples (\emph{e.g.} \texttt{slice\_sample()})
are generated using pseudo-random number generators. These generators
are initialized with a seed value. You can control the seed value R uses
to generate the random numbers by using the \texttt{set.seed()} function
and setting the seed value to a number of your choice. It is important
to note that setting a seed only affects the subsequent random number
generation.

Using \texttt{set.seed()} is useful when you want to ensure that the
same random numbers are generated each time you run the code. This is
particularly helpful when you want to reproduce results in a report or
other document.

\end{tcolorbox}

In the case of the Europarl corpus dataset, it may be useful to see the
source and target lines in the same sample. Do do this, we can first
sample from the \texttt{line\_id} variable and then filter the
\texttt{europarl\_curated\_tbl} wit the \texttt{filter()} function and
the \texttt{\%in\%} operator to select the lines that match the sampled
\texttt{line\_id} values, as seen in
Example~\ref{exm-td-europarl-sample-2}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-sample-2}{}\label{exm-td-europarl-sample-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Randomly sample 5 line\_id values}
\NormalTok{line\_id\_sample\_vec }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{distinct}\NormalTok{(line\_id) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(line\_id)}

\CommentTok{\# Select the lines that match the sampled line\_id values}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(line\_id }\SpecialCharTok{\%in\%}\NormalTok{ line\_id\_sample\_vec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>     doc_id type   line_id lines                                                 
>      <dbl> <chr>    <dbl> <chr>                                                 
>  1 1992525 Source   26791 La sociedad del conocimiento es algo ms, y tambin p~
>  2   26791 Target   26791 The knowledge society is more than that, and it may a~
>  3 2247038 Source  281304 Especialmente en la industria de alta tecnologa pued~
>  4  281304 Target  281304 Particularly in the technical and hi-tech industry, t~
>  5 2325587 Source  359853 - (IT) Seora Presidenta, Comisario, Seoras, quiero~
>  6  359853 Target  359853 . (IT) Madam President, Commissioner, ladies and gent~
>  7 2573895 Source  608161 En consecuencia, nuestro Grupo ha votado en contra.   
>  8  608161 Target  608161 Consequently, our group voted against.                
>  9 2581569 Source  615835 Al margen de ello, es todo el club de los seis, esos ~
> 10  615835 Target  615835 Beyond that, it is the whole club of six, these count~
\end{verbatim}

\end{example}

After running the code in Example~\ref{exm-td-europarl-sample-1} and
Example~\ref{exm-td-europarl-sample-2} multiple times, I identified a
number of artifacts that we will want to consider addressing. These are
included in Table~\ref{tbl-td-europarl-normalization}.

\hypertarget{tbl-td-europarl-normalization}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3300}}@{}}
\caption{\label{tbl-td-europarl-normalization}Characteristics of the
Europarl Corpus dataset that may require normalization.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Concern
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Examples
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Concern
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Non-speech annotations & \texttt{(Abucheos)}, \texttt{(A4-0247/98)},
\texttt{(The\ sitting\ was\ opened\ at\ 09:00)} & Not of interest for
our analysis \\
Inconsistent whitespace & \texttt{5\ \%\ ,}, ~~~~~,
\texttt{Palacio\textquotesingle{}\ s} & May be problematic for
tokenization \\
Non-sentence punctuation & \texttt{-} & May be problematic for
tokenization \\
Abbreviations & \texttt{Mr.}, \texttt{Sr.}, \texttt{Mme.}, \texttt{Mr},
\texttt{Sr}, \texttt{Mme}, \texttt{Mister}, \texttt{Seor},
\texttt{Madam} & May be problematic for tokenization \\
Text case & \texttt{The}, \texttt{the}, \texttt{White}, \texttt{white} &
May be problematic for tokenization \\
\end{longtable}

The first three items in Table~\ref{tbl-td-europarl-normalization} are
relatively straightforward. Non-speech annotations most likely are not
relevant for our research. Inconsistent whitespace and non-sentence
punctuation may be problematic for tokenization that depends on
whitespace and punctuation regularities.

The other two considerations are more contingent on our research aims.
The existence of various forms for the same word, abbreviated and
unabreviated, introduces variability may not be of interest for our
analysis. Secondly, common conventions for capitalization in prose can
introduce unwanted variability. If we leave the text as is, the tokens
\texttt{The} and \texttt{the} will be treated as distinct. If we convert
the text to lowercase, the tokens \texttt{White} and \texttt{white} will
be treated as the same, even if \texttt{White} corresponds to a proper
noun (\emph{e.g.} \texttt{White\ House}).

These observations provide us a roadmap for the normalization process.
For demonstration, let's focus only on a couple of these cases: removing
parlimentary session descriptions and extra whitespace.

\hypertarget{sec-td-normalization-application}{%
\subsection{Application}\label{sec-td-normalization-application}}

Identifying our normalization goals is an important first step. The next
step is to identify the procedures that will accomplish these goals. The
majority of text normalization procedures can be accomplished with the
\texttt{stringr} package (\protect\hyperlink{ref-R-stringr}{Wickham
2022}). This package provides a number of functions for manipulating
text. The workhorse functions we will use for our tasks are the
\texttt{str\_remove()} and \texttt{str\_replace()} functions. As the
these functions give us the ability to remove or replace text. Our task
is to identify the patterns we want to remove or replace.

Before we modify any lines, let's try craft a search pattern to identify
the text of interest. This is done to avoid over- or under-generalizing
the search pattern. If we are too general, we may end up removing or
replacing text that we want to keep. If we are too specific, we may not
remove or replace all the text we want to remove or replace.

Let's start by identifying non-parlimentary speech. Two functions from
the \texttt{stringr} package come in handy here: \texttt{str\_detect()}
and \texttt{str\_extract()}. \texttt{str\_detect()} detects a pattern in
a character vector and returns a logical vector, \texttt{TRUE} if the
pattern is detected and \texttt{FALSE} if it is not.
\texttt{str\_extract()} extracts the text in a character vector that
matches a pattern.

\texttt{str\_detect()} pairs well with the \texttt{filter()} function to
return observations that match a pattern in a character vector.
\texttt{str\_extract()} pairs well with the \texttt{mutate()} function
to create a new variable which contains character vector that match a
pattern.

Let's start with the \texttt{str\_detect()} function. We will use this
function to identify the lines that contain the parliamentary session
descriptions. From the examples above, we can see that these instances
are wrapped with parentheses \texttt{(} and \texttt{)}. The text within
the parentheses can vary, so we need a Regular Expression to do the
heavy lifting. To start out we can match any one or multiple characters
with \texttt{.+}. But it is important to recognize the \texttt{+} (and
also the \texttt{*}) operators are `greedy', meaning that if there are
multiple matches, the longest match will be returned. In this case, we
want to match the shortest match. To do this we can use the \texttt{?}
operator to make the \texttt{+} operator `lazy'. This will match the
shortest match.

Our test code appears in
Example~\ref{exm-td-europarl-search-non-speech}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-non-speech}{}\label{exm-td-europarl-search-non-speech}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify non{-}speech lines}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>     doc_id type   line_id lines                                                 
>      <dbl> <chr>    <dbl> <chr>                                                 
>  1 3225772 Source 1260038 (PT) Seor Presidente, quisiera plantear dos pregunta~
>  2 3715842 Source 1750108 (El Parlamento decide la devolucin a la Comisin)    
>  3 1961715 Target 1961715 (Parliament adopted the resolution)                   
>  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEMM); binding~
>  5   51632 Target   51632 Question No 8 by (H-0376/00):                         
>  6 2482671 Source  516937 La Comisin propone proporcionar a las Agencias nacio~
>  7 1059628 Target 1059628 (The President cut off the speaker)                   
>  8 1507254 Target 1507254 in writing. - (LT) I welcomed this document, because ~
>  9 2765325 Source  799591 (Aplausos)                                            
> 10 2668536 Source  702802  Las preguntas que, por falta de tiempo, no han rec~
\end{verbatim}

\end{example}

The results from Example~\ref{exm-td-europarl-search-non-speech} show
that we have identified the lines that contain at least one of the
parliamentary session description annotations. A more targeted search to
identify specific instances of the parliamentary session descriptions
can be accomplished adding the \texttt{str\_extract()} function as seen
in Example~\ref{exm-td-europarl-search-non-speech-2}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-non-speech-2}{}\label{exm-td-europarl-search-non-speech-2}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract non{-}speech fragments}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{non\_speech =} \FunctionTok{str\_extract}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 5
>     doc_id type   line_id lines                                       non_speech
>      <dbl> <chr>    <dbl> <chr>                                       <chr>     
>  1 3225772 Source 1260038 (PT) Seor Presidente, quisiera plantear d~ (PT)      
>  2 3715842 Source 1750108 (El Parlamento decide la devolucin a la C~ (El Parla~
>  3 1961715 Target 1961715 (Parliament adopted the resolution)         (Parliame~
>  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM~ (para. 53)
>  5   51632 Target   51632 Question No 8 by (H-0376/00):               (H-0376/0~
>  6 2482671 Source  516937 La Comisin propone proporcionar a las Age~ (correspo~
>  7 1059628 Target 1059628 (The President cut off the speaker)         (The Pres~
>  8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen~ (LT)      
>  9 2765325 Source  799591 (Aplausos)                                  (Aplausos)
> 10 2668536 Source  702802  Las preguntas que, por falta de tiempo,~ (Vase el~
\end{verbatim}

\end{example}

The results from Example~\ref{exm-td-europarl-search-non-speech-2} show
that we have identified the lines that contain parliamentary session
description annotations and extracted this text --or have we? What if a
given line contains more than one parliamentary session description
annotation? It turns out that \texttt{str\_extract()} only returns the
first match. To return all matches we can use the
\texttt{str\_extract\_all()} function. Let's try again, in
Example~\ref{exm-td-europarl-search-non-speech-3}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-non-speech-3}{}\label{exm-td-europarl-search-non-speech-3}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract non{-}speech fragments}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{non\_speech =} \FunctionTok{str\_extract\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 5
>     doc_id type   line_id lines                                       non_speech
>      <dbl> <chr>    <dbl> <chr>                                       <list>    
>  1 3225772 Source 1260038 (PT) Seor Presidente, quisiera plantear d~ <chr [1]> 
>  2 3715842 Source 1750108 (El Parlamento decide la devolucin a la C~ <chr [1]> 
>  3 1961715 Target 1961715 (Parliament adopted the resolution)         <chr [1]> 
>  4 1429470 Target 1429470 27, originally Greens/EFA amendment in FEM~ <chr [1]> 
>  5   51632 Target   51632 Question No 8 by (H-0376/00):               <chr [1]> 
>  6 2482671 Source  516937 La Comisin propone proporcionar a las Age~ <chr [2]> 
>  7 1059628 Target 1059628 (The President cut off the speaker)         <chr [1]> 
>  8 1507254 Target 1507254 in writing. - (LT) I welcomed this documen~ <chr [1]> 
>  9 2765325 Source  799591 (Aplausos)                                  <chr [1]> 
> 10 2668536 Source  702802  Las preguntas que, por falta de tiempo,~ <chr [1]>
\end{verbatim}

\end{example}

OK, that might not be what you expected. The
\texttt{str\_extract\_all()} function returns a list of character
vectors. This is because for any given line in \texttt{lines} there may
be a different number of matches. To maintain the data frame as
rectangular, a list is returned for each value of \texttt{non\_speech}.
We could expand the list into a data frame with the \texttt{unnest()}
function from the \texttt{tidyr} package if our goal were to work with
these matches. But that is not our aim. Rather, we want to know if we
have multiple matches per line. Note that the information provided for
the \texttt{non\_speech} column by the tibble object tells use that we
have some lines with muliple matches, as we can see in line 6 of our
small sample. So good thing we checked!

Let's now remove these parliamentary session description annotations
from each line in the \texttt{lines} column. We turn to
\texttt{str\_remove\_all()}, a variant of \texttt{str\_remove()}, that,
as you expect, will remove multiple matches in a single line. We will
use the \texttt{mutate()} function to overwrite the \texttt{lines}
column with the modified text. The code is seen in
Example~\ref{exm-td-europarl-remove-non-speech}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-remove-non-speech}{}\label{exm-td-europarl-remove-non-speech}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove non{-}speech fragments}
\NormalTok{europarl\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lines =} \FunctionTok{str\_remove\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(.+?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{example}

I recommend spot checking the results of this normalization step by
running the code in Example~\ref{exm-td-europarl-search-non-speech}
again, if nothing appears we've done our job.

When you are content with the results, drop the observations that have
no text in the \texttt{lines} column given the entire line was
non-speech. This can be done with the \texttt{is.na()} function and the
\texttt{filter()} function as seen in
Example~\ref{exm-td-europarl-drop-empty-lines}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-drop-empty-lines}{}\label{exm-td-europarl-drop-empty-lines}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop empty lines}
\NormalTok{europarl\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(lines))}
\end{Highlighting}
\end{Shaded}

\end{example}

The second item of business to address is the extra whitespace we
observed in Table~\ref{tbl-td-europarl-normalization}. If we consider
the extra whitespace cases, we can categorize them into two types:
multiple spaces that should be a single space (\emph{e.g.} ~~~~~) and
single spaces that occur within a word (\emph{e.g.} \texttt{5\ \%\ ,} or
\texttt{Palacio\textquotesingle{}\ s}).

To deal with muliple spaces, we can turn to the
\texttt{str\_replace\_all()} function. This function will replace a
pattern with a replacement string for every pattern match. In this case,
we want to replace multiple spaces with a single space. We can use the
\texttt{\textbackslash{}\textbackslash{}s+} pattern to match one or more
spaces and then replace it with
\texttt{\textbackslash{}\textbackslash{}s} or a single whitespace
character \texttt{"\ "}.

Before we apply this normalization step, let's assess how many instances
of multiple spaces we have in the dataset. We can use the
\texttt{str\_count()} function to count the number of matches for a
pattern. The pattern we want needs to be a bit more precise than
\texttt{\textbackslash{}\textbackslash{}s+}, because this matches one or
more. We want to match \emph{two} or more. Using the regular expression
operator \texttt{\{,\}} we can specify our pattern to be
\texttt{\textbackslash{}\textbackslash{}s\{2,\}}, \emph{i.e.} two or
more continguous whitespaces. Let's count and sum all these matches. The
code is seen in Example~\ref{exm-td-europarl-count-whitespace}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-count-whitespace}{}\label{exm-td-europarl-count-whitespace}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Count multiple spaces}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{multiple\_spaces =} \FunctionTok{str\_count}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s\{2,\}"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{total\_multiple\_spaces =} \FunctionTok{sum}\NormalTok{(multiple\_spaces, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 1 x 1
>   total_multiple_spaces
>                   <int>
> 1                130628
\end{verbatim}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

You may be wondering what the extra parameter \texttt{na.rm\ =\ TRUE} is
doing in the \texttt{sum()} function. This parameter tells R to ignore
values that are \texttt{NA} (not available). This is important because
if we don't ignore \texttt{NA} values, the \texttt{sum()} function will
return \texttt{NA} if there are any \texttt{NA} values in the vector.
The code in Example~\ref{exm-td-europarl-count-whitespace} will return
\texttt{NA} when the \texttt{\textbackslash{}\textbackslash{}s\{2,\}}
doesn't match for a given line. This is because the
\texttt{str\_count()} function returns \texttt{NA} when there are no
matches. If we don't ignore these \texttt{NA} values, the \texttt{sum()}
function will return \texttt{NA} for the entire dataset.

\end{tcolorbox}

The results from Example~\ref{exm-td-europarl-count-whitespace} show
that we have over 130k instances of multiple spaces. Let's replace these
with a single space. The code is seen in
Example~\ref{exm-td-europarl-remove-multiple-whitespace}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-remove-multiple-whitespace}{}\label{exm-td-europarl-remove-multiple-whitespace}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove multiple spaces}
\NormalTok{europarl\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lines =} \FunctionTok{str\_replace\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s\{2,\}"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{example}

To check our work, we can run the code in
Example~\ref{exm-td-europarl-count-whitespace} again. We should see that
there are no more instances of multiple spaces.

Now let's turn to the single spaces that occur within a word. We can use
the \texttt{str\_replace\_all()} function again to replace these single
spaces with no space --but is that what we want? Probably not, we want
\emph{most} of the single spaces to remain, otherwise we would have one
very, very long string on each line.

Instead, we want to narrow our scope and focus in on whitespace that
occurs in particular contexts. One context we can focus on is the single
quote \texttt{\textquotesingle{}}, as in
\texttt{Palacio\textquotesingle{}\ s}. In this use, the single quote is
an apostrophe in a possessive form, but we might want to see if other
forms, such as contractions, that also may have this extra whitespace.
Let's check using a regular expression that matches a character string
\texttt{\textbackslash{}\textbackslash{}w+} with single quote
\texttt{\textquotesingle{}} at the end followed by whitespace
\texttt{\textbackslash{}\textbackslash{}s}. To give some more context I
will add a character string \texttt{\textbackslash{}\textbackslash{}w+}
after the whitespace. Using the \texttt{str\_extract\_all()} function we
can extract all the matches for this pattern. It returns a list, so we
\texttt{unnest()} the list. Then we can pull the vector of matches and
list the unique matches to get a sense of the context we are working
with. The code is seen in
Example~\ref{exm-td-europarl-search-apostrophe}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-search-apostrophe}{}\label{exm-td-europarl-search-apostrophe}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Match apostrophe followed by whitespace}
\NormalTok{europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{quote\_whitespace =} \FunctionTok{str\_extract\_all}\NormalTok{(lines, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest}\NormalTok{(quote\_whitespace) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(quote\_whitespace) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unique}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>  [1] "Frontiers' regulation" "capital' s"            "People' s"            
>  [4] "communitarization' of" "years' standing"       "citizens' initiative" 
>  [7] "EU' s"                 "expulsion' of"         "Members' assistants"  
> [10] "Europe' s"             "Commission' s"         "Parliament' s"        
> [13] "Mugabe' s"
\end{verbatim}

\end{example}

In Example~\ref{exm-td-europarl-search-apostrophe} we can see that we
have many singular possessive forms we want to ammend and other forms
that we may want to keep --in particular when the single quote is part
of a quote or a plural or irregular singular possessive. From various
runs of this code, it looks safe to remove whitespace between the single
quote and the \texttt{s} in the possessive form We need to be careful
not to remove whitespace in other contexts where the single quote is
followed by \texttt{s}. To do this we can use the word boundary pattern
\texttt{\textbackslash{}\textbackslash{}b} after the \texttt{s} in our
pattern to ensure that the \texttt{s} is not part of a following word.
The code is seen in Example~\ref{exm-td-europarl-remove-apostrophe}.

\begin{example}[]\protect\hypertarget{exm-td-europarl-remove-apostrophe}{}\label{exm-td-europarl-remove-apostrophe}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove whitespace after apostrophe}
\NormalTok{europarl\_transformed\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  europarl\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lines =} \FunctionTok{str\_replace\_all}\NormalTok{(lines, }\StringTok{"\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{ss}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{b"}\NormalTok{, }\StringTok{"\textquotesingle{}s"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{example}

To check our work, we can run the code in
Example~\ref{exm-td-europarl-search-apostrophe} again. We should see
that there are no more instances of whitespace after the single quote in
possessive forms. Once we are satisfied with our work, we can continue
with subsequent transformation procedures using the
\texttt{europarl\_transformed\_tbl} data, or save it to a CSV file,
create a data dictionary, and move on to the next transformation
procedure.

Normalization goals will vary from dataset to dataset but the procedures
often follow a similar line of attack to those outlined in this section.
There are cases, however, in which normalization procedures are more
easily accomplished after subsequent transformation steps or need to be
post-poned to further the goals of other transformation steps. For
example, standardizing abbreviated forms may be more easily accomplished
after tokenization when each token is a word. Another example is the
case of case conversion. Even if we are not directly interested in the
case differences between words, certain generation procedures, Named
Entity Recognition (NER) for example, may use case information to
identify the names of people, locations, organizations, \emph{etc.}. In
these cases, it may be better to leave the case as is until after the
generation step.

\hypertarget{sec-td-recoding}{%
\section{Recoding}\label{sec-td-recoding}}

Normalizing text can be seen as an extension of dataset curation to some
extent in that the structure of the dataset is maintained. In the
Europarl case, we saw this to be true. In the case of recoding, and
other transformational steps, the aim will be to modify the dataset
structure either by rows, columns, or both. Recoding processes can be
characterized by the creation of structural changes which are derived
from values in variables effectively recasting values as new variables
to enable more direct access in our analyses.

\hypertarget{sec-td-recoding-orientation}{%
\subsection{Orientation}\label{sec-td-recoding-orientation}}

The Switchboard dialog Act Corpus corpus contains a number of variables
describing conversations between speakers of American English. A subset
of this dataset provides a good example of the recoding process. The
data dictionary for this dataset appears in
Table~\ref{tbl-td-swda-curated-dd}.

\hypertarget{tbl-td-swda-curated-dd}{}
\begin{table}
\caption{\label{tbl-td-swda-curated-dd}Data dictionary for the Switchboard dialog Act Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & description & variable\_type\\
\hline
doc\_id & Document ID & Unique identifier for each document & numeric\\
\hline
speaker\_id & Speaker ID & Unique identifier for each speaker & numeric\\
\hline
sex & Sex & Gender of the speaker & character\\
\hline
education & Education & Level of education of the speaker (1 = no high school, 2 = high school degree, 3 = college degree) & numeric\\
\hline
birth\_year & Birth Year & Year of birth of the speaker & numeric\\
\hline
utt\_id & Utterance ID & Unique identifier for each utterance & numeric\\
\hline
utt\_text & Utterance Text & Text of the utterance & character\\
\hline
damsl\_tag & DAMSL Tag & Tag indicating the communicative function of the utterance & character\\
\hline
\end{tabular}
\end{table}

The data dictionary gives us a sense of the variables in the dataset.
Let's read in the dataset and preview the first 10 lines to get a sense
of the values in the dataset, as in
Example~\ref{exm-td-swda-dataset-read-show}.

\begin{example}[]\protect\hypertarget{exm-td-swda-dataset-read-show}{}\label{exm-td-swda-dataset-read-show}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the dataset}
\NormalTok{swda\_curated\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/derived/swda/swda\_curated.csv"}\NormalTok{)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id speaker_id sex    education birth_year utt_id utt_text       damsl_tag
>     <dbl>      <dbl> <chr>      <dbl>      <dbl>  <dbl> <chr>          <chr>    
>  1   4325       1632 Female         2       1962      1 Okay.  /       o        
>  2   4325       1632 Female         2       1962      2 {D So, }       qw       
>  3   4325       1519 Female         1       1971      3 [ [ I guess, + qy^d     
>  4   4325       1632 Female         2       1962      4 What kind of ~ +        
>  5   4325       1519 Female         1       1971      5 I think, ] + ~ +        
>  6   4325       1632 Female         2       1962      6 Does it say s~ qy       
>  7   4325       1519 Female         1       1971      7 I think it us~ sd       
>  8   4325       1519 Female         1       1971      8 You might try~ ad       
>  9   4325       1519 Female         1       1971      9 I don't know,~ h        
> 10   4325       1519 Female         1       1971     10 hold it down ~ ad
\end{verbatim}

\end{example}

Considering the data dictionary and the preview of the
\texttt{swda\_curated\_tbl} dataset, we observe a number of metadata
variables, such as \texttt{doc\_id}, \texttt{speaker\_id}, \texttt{sex},
\texttt{education}, \texttt{birth\_year}, \texttt{utt\_id},
\texttt{utt\_text}, and \texttt{damsl\_tag}.

Most of these variables and their values are readily interpretable.
However, the \texttt{damsl\_tag} variable and the annotation scheme that
appears interleaved with the dialog in \texttt{utt\_text} may require a
bit more explanation. If we consult the data origin file and/ or the
corpus website, we seee that the \texttt{damsl\_tag} is a
utterance-level annotation which indicates the dialog act type
(\emph{e.g.} statement, question, backchannel, \emph{etc.}). The
annotation interleaved with the dialog in \texttt{utt\_text} is a
\href{https://staff.fnwi.uva.nl/r.fernandezrovira/teaching/DM-materials/DFL-book.pdf}{disfluency
annotation scheme}. This scheme includes annotation for non-sentence
elements such as filled pauses (\emph{e.g.} \texttt{\{F\ uh\}}),
discourse markers (\emph{e.g.} \texttt{\{D\ well\}}), repetitions/
restarts (\emph{e.g.} \texttt{{[}I\ think\ +\ I\ believe{]}}), among
others.

Let's assume that we are interested in understanding the use of filled
pauses in the Switchboard dialog. Tottie
(\protect\hyperlink{ref-Tottie2011}{2011}) investigates the relationship
between speakers' use of filled pauses \texttt{uh} and \texttt{um} and
their socio-demographic background (sex, socio-economic status, and age)
in British English. An American English comparison would be insightful.
To do this, however, we will need to recode some of the variables in the
dataset.

In this case, we will use \texttt{education} as a proxy for
socio-economic status. As is, the values are numeric and are not
maximally transparent. We can recode these values to be more
interpretable. In the corpus documentation, the values for
\texttt{education} are described in Table~\ref{tbl-td-swda-education}.

\hypertarget{tbl-td-swda-education}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-td-swda-education}Values for the \texttt{education}
variable in the Switchboard dialog Act Corpus.}\tabularnewline
\toprule\noalign{}
Value & Description \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Value & Description \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Less Than High School \\
1 & Less Than College \\
2 & College \\
3 & More Than College \\
9 & Unknown \\
\end{longtable}

To derive a variable which reflects the age of each speaker, we will use
the \texttt{birth\_year} variable. This variable is a numeric value
which indicates the year of birth for each speaker. We can derive a new
variable \texttt{age} by subtracting the \texttt{birth\_year} from the
year of recordings, 1992.

Together \texttt{sex}, \texttt{education}, and \texttt{age} will provide
us with the socio-demographic information we need to investigate the
relationship between speakers' use of filled pauses and their
socio-demographic background. The last component we need to derive is
the use of filled pauses. To do this we will need to extract the filled
pauses from the \texttt{utt\_text} variable. We also need to consider
what we mean by `use'. In this case, we will operationalize the use of
filled pauses as the number of times a filled pause is used per
utterance.

\hypertarget{sec-td-recoding-application}{%
\subsection{Application}\label{sec-td-recoding-application}}

The plan to transform the \texttt{swda\_curated\_tbl} dataset is
established. Now we need to implement the plan. We will start by
recoding the \texttt{education} variable. Specifically, we want to map
the numeric values to the descriptions in
Table~\ref{tbl-td-swda-education}.

To do this we will use the \texttt{case\_when()} function from the
\texttt{dplyr} package. This function allows us to specify a series of
conditions and the values to return if the condition is met.
\texttt{case\_when()} evaluates the conditions and \texttt{mutate()}
writes the variable, in this case overwrites it, as seen in
Example~\ref{exm-td-swda-recoding-education}.

\begin{example}[]\protect\hypertarget{exm-td-swda-recoding-education}{}\label{exm-td-swda-recoding-education}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Recode education}
\NormalTok{swda\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{education =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{      education }\SpecialCharTok{==} \DecValTok{0} \SpecialCharTok{\textasciitilde{}} \StringTok{"Less Than High School"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\textasciitilde{}} \StringTok{"Less Than College"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \DecValTok{2} \SpecialCharTok{\textasciitilde{}} \StringTok{"College"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \DecValTok{3} \SpecialCharTok{\textasciitilde{}} \StringTok{"More Than College"}\NormalTok{,}
\NormalTok{      education }\SpecialCharTok{==} \DecValTok{9} \SpecialCharTok{\textasciitilde{}} \StringTok{"Unknown"}
\NormalTok{    )}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines}
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id speaker_id sex    education       birth_year utt_id utt_text damsl_tag
>     <dbl>      <dbl> <chr>  <chr>                <dbl>  <dbl> <chr>    <chr>    
>  1   4325       1632 Female College               1962      1 Okay.  / o        
>  2   4325       1632 Female College               1962      2 {D So, } qw       
>  3   4325       1519 Female Less Than Coll~       1971      3 [ [ I g~ qy^d     
>  4   4325       1632 Female College               1962      4 What ki~ +        
>  5   4325       1519 Female Less Than Coll~       1971      5 I think~ +        
>  6   4325       1632 Female College               1962      6 Does it~ qy       
>  7   4325       1519 Female Less Than Coll~       1971      7 I think~ sd       
>  8   4325       1519 Female Less Than Coll~       1971      8 You mig~ ad       
>  9   4325       1519 Female Less Than Coll~       1971      9 I don't~ h        
> 10   4325       1519 Female Less Than Coll~       1971     10 hold it~ ad
\end{verbatim}

\end{example}

To create the \texttt{age} variable, all we need to do is subtract the
\texttt{birth\_year} from the year of recording, 1992. Again we will use
\texttt{mutate()} to create the \texttt{age} variable. The values are
created by a subtraction operation. Since we will not need the
\texttt{birth\_year} variable afterwards, we will drop it from the
dataset. The code is seen in Example~\ref{exm-td-swda-recoding-age}.

\begin{example}[]\protect\hypertarget{exm-td-swda-recoding-age}{}\label{exm-td-swda-recoding-age}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Recode age}
\NormalTok{swda\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =} \DecValTok{1992} \SpecialCharTok{{-}}\NormalTok{ birth\_year) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{birth\_year)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id speaker_id sex    education         utt_id utt_text    damsl_tag   age
>     <dbl>      <dbl> <chr>  <chr>              <dbl> <chr>       <chr>     <dbl>
>  1   4325       1632 Female College                1 Okay.  /    o            30
>  2   4325       1632 Female College                2 {D So, }    qw           30
>  3   4325       1519 Female Less Than College      3 [ [ I gues~ qy^d         21
>  4   4325       1632 Female College                4 What kind ~ +            30
>  5   4325       1519 Female Less Than College      5 I think, ]~ +            21
>  6   4325       1632 Female College                6 Does it sa~ qy           30
>  7   4325       1519 Female Less Than College      7 I think it~ sd           21
>  8   4325       1519 Female Less Than College      8 You might ~ ad           21
>  9   4325       1519 Female Less Than College      9 I don't kn~ h            21
> 10   4325       1519 Female Less Than College     10 hold it do~ ad           21
\end{verbatim}

\end{example}

Our final recoding step is to derive the frequency of filled pauses per
utterance. In other words, we want to match the `uh' and `um' and return
the number of matches for each utterance. There are a number of ways to
do this. We could use an approach which applies the
\texttt{str\_extract\_all()} function, which returns a list of matches,
and then \texttt{unnest()} the list and count the number of matches. An
alternative approach is to use the \texttt{str\_count()} function to
count the number of matches for a pattern. The later approach is more
efficient for our purposes.

In either case, a pattern to match these annotations is needed. As
always with pattern matching, we need to craft an expression that is as
specific as possible to avoid over- or under-matching. We know from our
observation and the corpus documentation that all filled pauses are
wrapped by the \texttt{\{F\ ...\}} annotation. We can use this to our
advantage. Before we jump into counting the filled pauses, let's test a
regular expression that matches the entire \texttt{\{F\ ...\}}
annotation. An expression like \texttt{\{F.*\}} might be tempting, but
this will be problematic for two reasons. First, since the \texttt{\{}
and \texttt{\}} are regular expression operators we will need to escape
them with the \texttt{\textbackslash{}\textbackslash{}} convention.
Second, the \texttt{*} operator is greedy, meaning that it will match
the longest possible string. So if in a given utterance there are
multiple filled pauses, the \texttt{*} operator will match all of them
at once, not individually. To avoid this, we can use the \texttt{?}
operator to make the \texttt{*} operator lazy. With these considerations
in mind, we can move forward with
\texttt{\textbackslash{}\textbackslash{}\{F.*?\textbackslash{}\textbackslash{}\}}
as our expression.

We will send each utterance to the \texttt{str\_extract\_all()} function
to match the filled pauses. This function returns a list of matches, so
we will need to \texttt{unnest()} the list to get a vector of matches.
Afterwards we will apply the \texttt{count()} function to summarize the
number of matches for each match variation. The code is seen in
Example~\ref{exm-td-swda-recoding-filled-pauses-test}.

\begin{example}[]\protect\hypertarget{exm-td-swda-recoding-filled-pauses-test}{}\label{exm-td-swda-recoding-filled-pauses-test}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Test filled pause pattern}
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{matches =} \FunctionTok{str\_extract\_all}\NormalTok{(}
\NormalTok{      utt\_text, }
      \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F.*?}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(matches) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(matches)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 120 x 2
>    matches            n
>    <chr>          <int>
>  1 "{F  Oh, }"        1
>  2 "{F  uh, }"        1
>  3 "{F \"Oh, }"       1
>  4 "{F ((Oh)) }"      1
>  5 "{F ((Oh, }"       1
>  6 "{F ((Uh }"        1
>  7 "{F ((Uh)), }"     1
>  8 "{F ((Uh, }"       1
>  9 "{F + ] Um, }"     1
> 10 "{F + ] um, }"     1
> # i 110 more rows
\end{verbatim}

\end{example}

The result from our test indicates that the
\texttt{\textbackslash{}\textbackslash{}\{F\ .*?\textbackslash{}\textbackslash{}\}}
pattern matches a wide variety of filled pause annotations, 120 to be
exact! We can see that there are a number of filled pause annotations
that we are not be interested in, \emph{e.g.} \texttt{\{F\ Oh\}}.
Furthermore, the `uh' and `um' we are interested in sometimes include
more annotation structure, \emph{e.g.} \texttt{\{F\ +\ {]}\ Um,\ \}}.

A more sophisticated pattern is needed. When faced with a pattern
matching task such as this, I find it helpful to start with a simple
pattern and then add complexity as needed. This is an iterative process.
To speed up the process, I often extract the matches and use an online
tool for developing regular expressions, such as
\href{https://regex101.com/}{regex101.com}.

With a (monster) regular expression that matches each of the filled
pauses we are interested in, and only those filled pauses, we can move
forward with counting the number of matches for each utterance.

To do this we will use the \texttt{str\_count()} function from the
\texttt{stringr} package. This function counts the number of matches for
a pattern in a character vector. We will use \texttt{mutate()} to create
new variables \texttt{uh} and \texttt{um} which will contain the counts
for the filled pauses \texttt{uh} and \texttt{um}, respectively. The
code is seen in Example~\ref{exm-td-swda-recoding-filled-pauses}.

\begin{example}[]\protect\hypertarget{exm-td-swda-recoding-filled-pauses}{}\label{exm-td-swda-recoding-filled-pauses}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Recode filled pauses }
\NormalTok{swda\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{uh =} \FunctionTok{str\_count}\NormalTok{(}
\NormalTok{      utt\_text, }
      \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+[}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{]]*(u|U)h[}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s,}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{?{-}]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}
\NormalTok{    ),}
    \AttributeTok{um =} \FunctionTok{str\_count}\NormalTok{(}
\NormalTok{      utt\_text, }
      \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\{F}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s+[}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{(+}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{]]*(u|U)m[}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{)}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{s,}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{?{-}]*}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{\}"}
\NormalTok{    )}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines }
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 10
>    doc_id speaker_id sex   education utt_id utt_text damsl_tag   age    uh    um
>     <dbl>      <dbl> <chr> <chr>      <dbl> <chr>    <chr>     <dbl> <int> <int>
>  1   4325       1632 Fema~ College        1 Okay.  / o            30     0     0
>  2   4325       1632 Fema~ College        2 {D So, } qw           30     0     0
>  3   4325       1519 Fema~ Less Tha~      3 [ [ I g~ qy^d         21     0     0
>  4   4325       1632 Fema~ College        4 What ki~ +            30     0     0
>  5   4325       1519 Fema~ Less Tha~      5 I think~ +            21     1     0
>  6   4325       1632 Fema~ College        6 Does it~ qy           30     0     0
>  7   4325       1519 Fema~ Less Tha~      7 I think~ sd           21     0     0
>  8   4325       1519 Fema~ Less Tha~      8 You mig~ ad           21     1     0
>  9   4325       1519 Fema~ Less Tha~      9 I don't~ h            21     0     0
> 10   4325       1519 Fema~ Less Tha~     10 hold it~ ad           21     0     0
\end{verbatim}

\end{example}

Now we have two new columns, \texttt{uh} and \texttt{um} which indicate
how many times the relevant pattern was matched for a given utterance.
By choosing to focus on disfluencies, however, we have made a decision
to change the unit of observation from the utterance to the use of
filled pauses (\texttt{uh} and \texttt{um}). This means that as the
dataset stands, it is not in tidy format --where each observation
corresponds to the observational unit. When datasets are misaligned in
this particular way, there are in what is known as `wide' format. What
we want to do, then, is to restructure our dataset such that each row
corresponds to the unit of observation --in this case each filled pause
type.

To convert our current (wide) dataset to one where each filler type is
listed and the counts are measured for each utterance we turn to the
\texttt{pivot\_longer()} function. This function creates two new
columns, one in which the column names are listed and one for the values
for each of the column names. The code is seen in
Example~\ref{exm-td-swda-recoding-filled-pauses-longer}.

\begin{example}[]\protect\hypertarget{exm-td-swda-recoding-filled-pauses-longer}{}\label{exm-td-swda-recoding-filled-pauses-longer}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tidy filled pauses}
\NormalTok{swda\_curated\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(}\StringTok{"uh"}\NormalTok{, }\StringTok{"um"}\NormalTok{), }
    \AttributeTok{names\_to =} \StringTok{"filler"}\NormalTok{, }
    \AttributeTok{values\_to =} \StringTok{"count"}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines}
\NormalTok{swda\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 10
>    doc_id speaker_id sex    education     utt_id utt_text damsl_tag   age filler
>     <dbl>      <dbl> <chr>  <chr>          <dbl> <chr>    <chr>     <dbl> <chr> 
>  1   4325       1632 Female College            1 Okay.  / o            30 uh    
>  2   4325       1632 Female College            1 Okay.  / o            30 um    
>  3   4325       1632 Female College            2 {D So, } qw           30 uh    
>  4   4325       1632 Female College            2 {D So, } qw           30 um    
>  5   4325       1519 Female Less Than Co~      3 [ [ I g~ qy^d         21 uh    
>  6   4325       1519 Female Less Than Co~      3 [ [ I g~ qy^d         21 um    
>  7   4325       1632 Female College            4 What ki~ +            30 uh    
>  8   4325       1632 Female College            4 What ki~ +            30 um    
>  9   4325       1519 Female Less Than Co~      5 I think~ +            21 uh    
> 10   4325       1519 Female Less Than Co~      5 I think~ +            21 um    
> # i 1 more variable: count <int>
\end{verbatim}

\end{example}

Now we have a transformed dataset that is in tidy format. Each row
corresponds to a filled pause type and the number of times it was used
in a given utterance. It also includes the key socio-demographic
variables we are interested in.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

As confident as we may be in our recoding process, it is always a good
idea to perform some data checks to ensure that the recoding process was
successful. In the process, we can also gain some insight into the data.
Considering the structure in the transformed Switchboard dialog Act
Corpus dataset, what are some data checks we might want to perform? What
are some insights we might gain from these data checks?

\end{tcolorbox}

Finalize the transformation process by writing the dataset to a CSV
file, create a data dictionary, and review and comment your code in the
transformation script.

\hypertarget{sec-td-tokenization}{%
\section{Tokenization}\label{sec-td-tokenization}}

Another common transformation process that is particularly relevant for
text analysis is tokenization. Tokenization is the process of segmenting
units of language into components relevant for the research question.
This includes breaking text in curated datasets into smaller units, such
as words, \(n\)-grams, sentences, \emph{etc.} or combining text into
larger units relative to the original text.

The process of tokenization is fundamentally row-wise. By scaling the
text units up or down, we change the unit of observation. It is
important both for the research and the text processing to
operationalize our language units. For example, while it may appear
obvious to you what `word' or `sentence' means, a computer, and your
reproducible research, needs a definition. This can prove tricker than
it seems. For example, in English, we can segment text into words by
splitting on whitespace. This works fairly well but there are some cases
where this is not ideal. For example, in the case of contractions, such
as \texttt{don\textquotesingle{}t}, \texttt{won\textquotesingle{}t},
\texttt{can\textquotesingle{}t}, \emph{etc.} the apostrophe is not a
whitespace character. If we want to consider these contractions as
separate words, then we need to consider a different tokenization
strategy.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Consider the following paragraph:

\begin{quote}
``As the sun dipped below the horizon, the sky was set ablaze with
shades of orange-red, illuminating the landscape. It's a sight
Mr.~Johnson, a long-time observer, never tired of. On the lakeside, he'd
watch with friends, enjoying the ever-changing hues---especially those
around 6:30 p.m.---and reflecting on nature's grand display. Even in the
half-light, the water's glimmer, coupled with the echo of distant
laughter, created a timeless scene. The so-called `magic hour' was
indeed magical, yet fleeting, like a well-crafted poem; it was the
essence of life itself.''
\end{quote}

What text conventions would pose issues for word tokenization based on a
whitespace critieron?

\end{tcolorbox}

Furthermore, tokenization strategies can vary between languages. For
German words are often compounded together, meaning many `words' will
not be captured by the whitespace convention. Whitespace may not even be
relevant for word tokenization in written languages, such as Chinese.
The take home message is there is no one-size-fits-all tokenization
strategy.

\hypertarget{sec-td-tokenization-orientation}{%
\subsection{Orientation}\label{sec-td-tokenization-orientation}}

Let's look at a curated dataset from the CABNC Corpus to explore
tokenization. The data dictionary for this dataset appears in
Table~\ref{tbl-td-cabnc-dd}.

\hypertarget{tbl-td-cabnc-dd}{}
\begin{table}
\caption{\label{tbl-td-cabnc-dd}Data dictionary for the CABNC Corpus. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & description & variable\_type\\
\hline
doc\_id & Document ID & Unique identifier for each document & string\\
\hline
part\_id & Part ID & Identifier for the part within a document & string\\
\hline
sex & Sex & Gender of the person & string\\
\hline
age & Age & Age of the person & numeric\\
\hline
utt\_id & Utterance ID & Identifier for each utterance & numeric\\
\hline
utt\_text & Utterance Text & Text of the utterance & string\\
\hline
\end{tabular}
\end{table}

The CABNC dataset contains a number of variables describing
conversations between speakers of British English. Now let's look at the
dataset and preview the first 10 lines to get a sense of the values in
the dataset, as in Example~\ref{exm-td-cabnc-dataset-read-show}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-dataset-read-show}{}\label{exm-td-cabnc-dataset-read-show}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in the dataset}
\NormalTok{cabnc\_curated\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/derived/cabnc/cabnc\_curated.csv"}\NormalTok{)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 6
>    doc_id   part_id sex      age utt_id utt_text                                
>    <chr>    <chr>   <chr>  <dbl>  <dbl> <chr>                                   
>  1 KB0RE000 PS002   female   721      0 You enjoyed yourself in America         
>  2 KB0RE000 PS006   male     601      1 Eh                                      
>  3 KB0RE000 PS002   female   721      2 did you                                 
>  4 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes            
>  5 KB0RE000 PS002   female   721      4 Oh very good yeah                       
>  6 KB0RE000 PS006   male     601      5 Er saw Mary and Andrew and              
>  7 KB0RE000 PS002   female   721      6 Yes you did                             
>  8 KB0RE000 PS006   male     601      7 in fact the whole family was together f~
>  9 KB0RE000 PS002   female   721      8 Oh very nice very nice yes It 's horrib~
> 10 KB0RE000 PS006   male     601      9 It is horrible is n't
\end{verbatim}

\end{example}

Considering the data dictionary and the preview of the
\texttt{cabnc\_curated\_tbl} dataset, we observe a number of metadata
variables, such as \texttt{doc\_id}, \texttt{part\_id}, \texttt{sex},
\texttt{age}, and \texttt{utt\_id} and the utterances in
\texttt{utt\_text}.

Let's assume that we are performing an exploratory analysis with this
dataset and would like to consider the use of words and word sequences
(\(n\)-grams). In this case we will be deriving multiple datasets with
different units of observation.

\hypertarget{sec-td-tokenization-application}{%
\subsection{Application}\label{sec-td-tokenization-application}}

In the \texttt{cabnc\_curated\_tbl} data frame the \texttt{utt\_text}
variable contains the text we want to tokenize. We will start by
tokenizing the text into words. Abstracting away from some of the
metadata variables, if we envision what this should look like we might
imagine something like
Table~\ref{tbl-td-cabnc-tokenization-words-example}.

\hypertarget{tbl-td-cabnc-tokenization-words-example}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-td-cabnc-tokenization-words-example}Example of
tokenizing the \texttt{utt\_text} variable into words.}\tabularnewline
\toprule\noalign{}
doc\_id & utt\_id & utt\_word \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
doc\_id & utt\_id & utt\_word \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
KB0RE000 & 0 & You \\
KB0RE000 & 0 & enjoyed \\
KB0RE000 & 0 & yourself \\
KB0RE000 & 0 & in \\
KB0RE000 & 0 & America \\
\end{longtable}

Comparing Table~\ref{tbl-td-cabnc-tokenization-words-example} to the
first line of the output of Example~\ref{exm-td-swda-dataset-read-show},
we can see that we want to segment the words in the \texttt{utt\_text}
and then have each segment appear as a separate observation, retaining
the relevant metadata variables.

Before we work with tokenizing text in a data frame, let's start with a
character vector to get a sense of how tokenization works and what we
will need to do to achieve the output in
Table~\ref{tbl-td-cabnc-tokenization-words-example}. Let's start with a
character vector which contains the first three utterances from
\texttt{cabnc\_curated\_tbl}. I will use \texttt{slice\_head(n\ =\ 3)}
and then \texttt{pull()} to extract the \texttt{utt\_text} character
vector in Example~\ref{exm-td-cabnc-tokenization-words-vector}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-words-vector}{}\label{exm-td-cabnc-tokenization-words-vector}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pull a character vector}
\NormalTok{cabnc\_utts\_chr }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pull}\NormalTok{(utt\_text)}

\CommentTok{\# Preview the character vector}
\NormalTok{cabnc\_utts\_chr}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] "You enjoyed yourself in America" "Eh"                             
> [3] "did you"
\end{verbatim}

\end{example}

We have the first three utterances in \texttt{cbanc\_utts\_chr}. Now we
can tokenize the utterances into words using the
\texttt{tokenize\_words()} function from the \texttt{tokenizers} package
(\protect\hyperlink{ref-R-tokenizers}{Mullen 2022}). It's only required
argument is a character vector, as seen in
Example~\ref{exm-td-cabnc-tokenization-words-tokenize}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-words-tokenize}{}\label{exm-td-cabnc-tokenization-words-tokenize}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tokenizers)}

\CommentTok{\# Tokenize the utterances into words}
\NormalTok{cabnc\_utts\_chr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tokenize\_words}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [[1]]
> [1] "you"      "enjoyed"  "yourself" "in"       "america" 
> 
> [[2]]
> [1] "eh"
> 
> [[3]]
> [1] "did" "you"
\end{verbatim}

\end{example}

The output shows that we get a list of length 3, one for each utterance.
Each list element contains a character vector with different lengths
based on the number of tokens created from the original utterance.

Now if we add the \texttt{tokenize\_words()} function to a
\texttt{mutate()} call, we can create a new variable with the tokenized
utterances. However, the value for each observation will be a list. To
expand the character vectors within each list into separate
observations, we can use the \texttt{unnest()} function on the new
variable. I will assign the result to \texttt{cabnc\_unigrams\_tbl} as
we will have one-word tokens, as seen in
Example~\ref{exm-td-cabnc-tokenization-words}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-words}{}\label{exm-td-cabnc-tokenization-words}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tokenize the utterances into words}
\NormalTok{cabnc\_unigrams\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utt\_words =} \FunctionTok{tokenize\_words}\NormalTok{(utt\_text)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ utt\_words)}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_unigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 7
>    doc_id   part_id sex      age utt_id utt_text                       utt_words
>    <chr>    <chr>   <chr>  <dbl>  <dbl> <chr>                          <chr>    
>  1 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri~ you      
>  2 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri~ enjoyed  
>  3 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri~ yourself 
>  4 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri~ in       
>  5 KB0RE000 PS002   female   721      0 You enjoyed yourself in Ameri~ america  
>  6 KB0RE000 PS006   male     601      1 Eh                             eh       
>  7 KB0RE000 PS002   female   721      2 did you                        did      
>  8 KB0RE000 PS002   female   721      2 did you                        you      
>  9 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes   oh       
> 10 KB0RE000 PS006   male     601      3 Oh I covered a nice trip yes   i
\end{verbatim}

\end{example}

The \texttt{cabnc\_unigrams\_tbl} dataset is in the format we want, each
row corresponds to a word token and each column contains the relevant
metadata. The \texttt{tokenizers} package includes a variety of
tokenization functions. For example, we can use the
\texttt{tokenize\_ngrams()} function to create \(n\)-grams. The
\texttt{tokenize\_ngrams()} function takes a character vector and a
value for \(n\) and returns a list of \(n\)-grams. Other functions can
tokenize character \(n\)-grams, sentences, paragraphs, lines, or even
allow you to specify a custom tokenization function with a regular
expression.

As we have seen the \texttt{tokenize\_*()} set of functions take a
character vector and return a list of tokens. And if we are working with
a data frame, we can then work to expand the list into separate
observations. This is a common pattern in text analysis. So common, in
fact, that the \texttt{tidytext} package
(\protect\hyperlink{ref-R-tidytext}{Robinson and Silge 2023}) includes a
function, \texttt{unnest\_tokens()} that wraps the \texttt{tokenize\_*}
functions and expand the list of tokens into separate observations in
one step. The tokenization types available are \texttt{character},
\texttt{word}, \texttt{ngram}, \texttt{sentence}, \texttt{regex}, and
\texttt{skip\_ngram}. We will use the \texttt{word} tokenization type to
recreate the \texttt{cabnc\_unigrams\_tbl} dataset, as seen in
Example~\ref{exm-td-cabnc-tokenization-words-tidytext}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-words-tidytext}{}\label{exm-td-cabnc-tokenization-words-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Tokenize the utterances into words}
\NormalTok{cabnc\_unigrams\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ utt\_word, }
    \AttributeTok{input =}\NormalTok{ utt\_text, }
    \AttributeTok{token =} \StringTok{"words"}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_unigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 6
>    doc_id   part_id sex      age utt_id utt_word
>    <chr>    <chr>   <chr>  <dbl>  <dbl> <chr>   
>  1 KB0RE000 PS002   female   721      0 you     
>  2 KB0RE000 PS002   female   721      0 enjoyed 
>  3 KB0RE000 PS002   female   721      0 yourself
>  4 KB0RE000 PS002   female   721      0 in      
>  5 KB0RE000 PS002   female   721      0 america 
>  6 KB0RE000 PS006   male     601      1 eh      
>  7 KB0RE000 PS002   female   721      2 did     
>  8 KB0RE000 PS002   female   721      2 you     
>  9 KB0RE000 PS006   male     601      3 oh      
> 10 KB0RE000 PS006   male     601      3 i
\end{verbatim}

\end{example}

The \texttt{utt\_word} column in the outputs from both
Example~\ref{exm-td-cabnc-tokenization-words} and
Example~\ref{exm-td-cabnc-tokenization-words-tidytext} are identical.
One thing to note, however, is that the original \texttt{utt\_text}
variable is dropped in the \texttt{unnest\_tokens()} approach. This one
of a few default values for parameters which inlcude
\texttt{drop\ =\ TRUE} (dropping the original text variable) and
\texttt{to\_lower\ =\ TRUE}. In fact the \texttt{token\ =\ "words"}
parameter is not needed as it is the default. We can change these
defaults as we see fit.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

The approach using either \texttt{tokenize\_*()} or
\texttt{unnest\_tokens()} approaches tokenization from a segmentation
point of view. That is, the context separating our tokens is tarted and
is removed. In some cases it may be more feasible to turn the approach
around and instead target the tokens to extract. The
\texttt{str\_extract\_all()} can be used for this purpose. Note,
however, the former approach is often more effective and effecient. The
later requires a regular expression, or set of, which can be tricky to
develop for some tokenization tasks.

\end{tcolorbox}

As we create derived datasets to explore, let's also create bigram
tokens. We can do this by changing the \texttt{token} parameter to
\texttt{"ngrams"} and specifying the value for \(n\) with the \texttt{n}
parameter. I will assign the result to \texttt{cabnc\_bigrams\_tbl} as
we will have two-word tokens, as seen in
Example~\ref{exm-td-cabnc-tokenization-bigrams-tidytext}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-bigrams-tidytext}{}\label{exm-td-cabnc-tokenization-bigrams-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Tokenize the utterances into bigrams}
\NormalTok{cabnc\_bigrams\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ utt\_bigram, }
    \AttributeTok{input =}\NormalTok{ utt\_text, }
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
    \AttributeTok{n =} \DecValTok{2}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_bigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 6
>    doc_id   part_id sex      age utt_id utt_bigram      
>    <chr>    <chr>   <chr>  <dbl>  <dbl> <chr>           
>  1 KB0RE000 PS002   female   721      0 you enjoyed     
>  2 KB0RE000 PS002   female   721      0 enjoyed yourself
>  3 KB0RE000 PS002   female   721      0 yourself in     
>  4 KB0RE000 PS002   female   721      0 in america      
>  5 KB0RE000 PS006   male     601      1 <NA>            
>  6 KB0RE000 PS002   female   721      2 did you         
>  7 KB0RE000 PS006   male     601      3 oh i            
>  8 KB0RE000 PS006   male     601      3 i covered       
>  9 KB0RE000 PS006   male     601      3 covered a       
> 10 KB0RE000 PS006   male     601      3 a nice
\end{verbatim}

\end{example}

The two-word token sequences for each uttterance appear as observations
in the \texttt{cabnc\_bigrams\_tbl} dataset. You may notice that in row
5 the value for \texttt{utt\_bigram} is \texttt{NA}. This is because the
utterance only contains one word. The \texttt{unnest\_tokens()} function
will not create a token for a sequence that does not exist.

If we don't want to loose this information, we can modify the original
\texttt{utt\_text} variable to include a placeholder, some symbol that
we will use to denote that a single word utterance is present. Another
strategy which accomplishes the first goal and may enrich the bigram
dataset is to add a start and end token to each utterance. This will
allow us to identify the first and last word in each utterance. In
either case we can turn to the \texttt{str\_c()} function which will
allow us to concatenate strings. In
Example~\ref{exm-td-cabnc-tokenization-bigrams-tidytext-start-end}, I
will add a start and end token to each utterance and then tokenize the
utterances into bigrams.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-bigrams-tidytext-start-end}{}\label{exm-td-cabnc-tokenization-bigrams-tidytext-start-end}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prepend and append (x) char to \textasciigrave{}utt\_text\textasciigrave{}}
\NormalTok{cabnc\_derived\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{utt\_text =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"x"}\NormalTok{, utt\_text, }\StringTok{"x"}\NormalTok{, }\AttributeTok{sep =} \StringTok{" "}\NormalTok{)}
\NormalTok{  )}
\CommentTok{\# Tokenize the utterances into bigrams}
\NormalTok{cabnc\_bigrams\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_derived\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ utt\_bigram, }
    \AttributeTok{input =}\NormalTok{ utt\_text, }
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{,}
    \AttributeTok{n =} \DecValTok{2}
\NormalTok{  )}
\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_bigrams\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 6
>    doc_id   part_id sex      age utt_id utt_bigram      
>    <chr>    <chr>   <chr>  <dbl>  <dbl> <chr>           
>  1 KB0RE000 PS002   female   721      0 x you           
>  2 KB0RE000 PS002   female   721      0 you enjoyed     
>  3 KB0RE000 PS002   female   721      0 enjoyed yourself
>  4 KB0RE000 PS002   female   721      0 yourself in     
>  5 KB0RE000 PS002   female   721      0 in america      
>  6 KB0RE000 PS002   female   721      0 america x       
>  7 KB0RE000 PS006   male     601      1 x eh            
>  8 KB0RE000 PS006   male     601      1 eh x            
>  9 KB0RE000 PS002   female   721      2 x did           
> 10 KB0RE000 PS002   female   721      2 did you
\end{verbatim}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{exclamation-triangle} Warning}

In Example~\ref{exm-td-cabnc-tokenization-bigrams-tidytext-start-end} I
used \texttt{x} as the start and end token. My though process was that
\texttt{x} will be a unique token that will not be present in the
utterances. This is a good strategy, but it is not foolproof. Another,
more meaningful strategy may be to encode the start and end with
\texttt{\#} and \texttt{\$}, respectively. In this case, however, this
option is not ideal as the \texttt{unnest\_tokens()} and
\texttt{tokenize\_ngrams()} functions strip punctuation by default. This
means that the \texttt{\#} and \texttt{\$} will be removed.

\end{tcolorbox}

The most common tokenization strategy is to segment text into smaller
units, often words. However, there are times when we may want to segment
text into larger units, effectively collapsing over rows. For example,
if we are working with a curated dataset which is tokenized by words, we
may want to segment the text into sentences. A couple considerations are
in order, however. First, we need to be clear about what we mean by
`sentence' and how we will segment the text into sentences. In some
cases key cues for sentence boundaries, such as sentencial punctuation
have been stripped from the text, making a simple defnition of a
sentence difficult or impossible to be performed computationally.
Second, we also need to be clear how we will handle the metadata
variables. In other words, when we collapse over rows, we need to be
aware and intentional about how we group these new units of observation.
For example, if we collapse the \texttt{cabnc\_unigrams\_tbl} dataset
into sentences, will we group the sentences by \texttt{doc\_id} or
\texttt{part\_id} or some other combination of metadata variables?

Let's see how we might collapse text rows into larger units using the
original curated CABNC data frame \texttt{cabnc\_curated\_tbl}. Refer
back to the output in Example~\ref{exm-td-cabnc-dataset-read-show}. The
values in \texttt{utt\_text} are utterances, which may map to sentences
in some cases, but often not. Regardless, there is no sentential
punctuation to help identify sentences, even across utterance
observations. Furthermore, it may not even make sense to dicuss
sentences in the context of spoken language.

A unit that may make more sense is utterances per speaker per document.
Note this context `per speaker per document'. In effect this is a
grouping parameter for our approach to collapsing the text in
\texttt{utt\_text}. With a goal in mind we can turn to the
\texttt{group\_by()} function to group our dataset and then
\texttt{summarize()} to collapse the text in \texttt{utt\_text} with the
\texttt{str\_c()} and the parameter \texttt{collapse\ =\ "\ "}. The code
is seen in Example~\ref{exm-td-cabnc-tokenization-utterances}.

\begin{example}[]\protect\hypertarget{exm-td-cabnc-tokenization-utterances}{}\label{exm-td-cabnc-tokenization-utterances}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse utterances by speaker by document}
\NormalTok{cabnc\_utterances\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  cabnc\_curated\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id, part\_id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{utt\_text =} \FunctionTok{str\_c}\NormalTok{(utt\_text, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview the first 10 lines}
\NormalTok{cabnc\_utterances\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 3
>    doc_id   part_id utt_text                                                    
>    <chr>    <chr>   <chr>                                                       
>  1 KB0RE000 KB0PSUN Hmm Hello                                                   
>  2 KB0RE000 PS002   You enjoyed yourself in America did you Oh very good yeah Y~
>  3 KB0RE000 PS006   Eh Oh I covered a nice trip yes Er saw Mary and Andrew and ~
>  4 KB0RE001 KB0PSUN Neck of lamb 's off Hello Morning                           
>  5 KB0RE001 PS005   You can tape me by all means but they probably wo n't like ~
>  6 KB0RE001 PS007   They want to know what spoken English is like Er now what d~
>  7 KB0RE002 PS003   No Was born in Brockly I was n't born sorry about that my f~
>  8 KB0RE002 PS007   You 're not Welsh speaking at all are you Mm mm but you are~
>  9 KB0RE003 PS006   It 's all supposed to be anonymous anyway Right now do you ~
> 10 KB0RE003 PS007   I do n't think I I 'm likely to say anything use use down a~
\end{verbatim}

\end{example}

Now we have a data frame where the utterances for each speaker for each
document are collapsed into a single observation. Yet by collapsing the
utterances in this way, we have lost the speaker metadata \texttt{sex}
and \texttt{age}. We can add this information by one of two approaches.
The first is to simply add \texttt{sex} and \texttt{age} to the grouping
parameter in \texttt{group\_by()}. Since these variables are descriptors
of \texttt{part\_id} they will not change the result of our collapsing
the text, but will be retained in the resulting output. The second
approach is to use a join operation to add the metadata back to the
collapsed dataset. I won't go into this approach here as we will cover
joins head-on in Section~\ref{sec-td-merging}.

\hypertarget{sec-td-generation}{%
\section{Generation}\label{sec-td-generation}}

The process of generation involves the addition of information to a
dataset. This differs from the previous transformation procedures in
that normalization, recoding, and tokenization involve manipulating,
classifiying, and/ or deriving information based on characteristics
explicit in a dataset. Instead, generation involves deriving new
information based on characteristics implicit in a dataset.

The most common type of operation involved in the generation process is
the addition of linguistic annotation. This process can be accomplished
manually by a researcher or research team or automatically through the
use of pre-trained linguistic resources and/ or software. Ideally the
annotation of linguistic information can be conducted automatically.

There are important considerations, however, that need to be taken into
account when choosing whether linguistic annotation can be conducted
automatically. First and foremost has to do with the type of annotation
desired. Information such as part of speech (grammatical category) and
morpho-syntactic information are the the most common types of linguistic
annotation that can be conducted automatically.

Second, the degree to which the resource that will be used to annotate
the linguistic information is aligned with the language variety and/or
register is also a key consideration. As noted, automatic linguistic
annotation methods are contingent on pre-trained resources. The language
and language variety used to develop these resources may not be
available for the language under investigation, or if it does, the
language variety and/ or register may not align. The degree to which a
resource does not align with the linguistic information targeted for
annotation is directly related to the quality of the final annotations.
To be clear, no annotation method, whether manual or automatic is
guaranteed to be perfectly accurate.

\hypertarget{sec-td-generation-orientation}{%
\subsection{Orientation}\label{sec-td-generation-orientation}}

As an example, we'll posit that we are conducting translation research.
Specifically, we will set up an investigation into the effect of
translation on the syntactic simplification of text. The basic notion is
that when translators translate text from one language to another, they
subconsciously simplify the text, compared to native texts
(\protect\hyperlink{ref-Liu2021}{Liu and Afzaal 2021}).

To assess this hypothesis, we will need to identify comparable
translated and native texts. The ENNTT corpus contains native and
translated English. The texts are drawn from European Parliament
proceedings ensuring that the texts are comparable in terms of register.

The data dictionary for the curated native dataset appears in
Table~\ref{tbl-td-enntt-native-dd}.

\hypertarget{tbl-td-enntt-native-dd}{}
\begin{table}
\caption{\label{tbl-td-enntt-native-dd}Data dictionary for the curated native ENNTT dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
session\_id & Session ID & categorical & Unique identifier for each session\\
\hline
speaker\_id & Speaker ID & categorical & Unique identifier for each speaker\\
\hline
state & State & categorical & The country or region the speaker is from\\
\hline
session\_seq & Session Sequence & ordinal & The order in which the session occurred\\
\hline
text & Text & categorical & The spoken text during the session\\
\hline
type & Type & categorical & The type of speaker. Natives in this dataset.\\
\hline
\end{tabular}
\end{table}

Both the natives and translations datasets have the same variables. For
the purposes of this investigation, the \texttt{text} and \texttt{type}
variables are the most relevant. A variable to index the text, such as
\texttt{doc\_id}, will be useful as we move forward and will be added to
the dataset.

The next step is to operationalize what we mean by syntactic
simplification. There are many measures of syntactic complexity
(\protect\hyperlink{ref-Szmrecsanyi2004}{Szmrecsanyi 2004}). For our
purposes, let's focus on two: number of T-units and sentence length (in
words). Length is straightforward to calculate after word tokenization
but a T-unit is a bit more involved. A T-unit is a main clause and all
of its subordinate clauses. To calculate the number of T-units, we will
need to identify the main clauses and their subordinate clauses.

An idealized transformed dataset for this investigation would look
something like Table~\ref{tbl-td-generation-idealized}.

\hypertarget{tbl-td-generation-idealized}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0571}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.7786}}@{}}
\caption{\label{tbl-td-generation-idealized}Idealized transformed
dataset for the syntactic simplification investigation.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & translated & 1 & 5 & I am happy right now. \\
2 & translated & 3 & 11 & I think that John believes that Mary is a good
person. \\
3 & native & 2 & 8 & She thinks I am not happy right now. \\
4 & native & 4 & 21 & Although she knew that the weather was bad, Mary
decided to go for a walk, hoping that she would feel better. \\
\end{longtable}

To identify the main clauses and their subordinate clauses, we will need
to annotate the ENNTT texts with syntactic information. Specifically, we
will need to identify and count the main clauses and their subordinate
clauses.

\hypertarget{sec-td-generation-application}{%
\subsection{Application}\label{sec-td-generation-application}}

As fun as it would be to hand-annotate the ENNTT corpus, we will instead
turn to automatic linguistic annotation. Specifically, we will use the
\texttt{udpipe} package (\protect\hyperlink{ref-R-udpipe}{Wijffels
2023}) which provides an interface for annotating text using pre-trained
models from the \href{https://universaldependencies.org/}{Universal
Dependencies} (UD) project (\protect\hyperlink{ref-Nivre2020}{Nivre et
al. 2020}). The UD project is an effort to develop cross-linguistically
consistent treebank annotation for a variety of languages.

Our first step, then, is to peruse the available pre-trained models for
the languages we are interested in and selected the most
register-aligned models. The models, model names, and licensing
information are documented in the \texttt{udpipe} package and can be
accessed by running \texttt{?udpipe::udpipe\_download\_model()} in the R
console. For illustrative purposes, the \texttt{english} treebank model
from the \emph{https://github.com/bnosac/udpipe.models.ud} repository
which is released under the
\href{https://creativecommons.org/licenses/by-sa/4.0/}{CC-BY-SA
license}. This model is trained on various sources including news,
Wikipedia, and web data of various genres.

Let's set the stage by providing an overview of the annotation process.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Load the \texttt{udpipe} package.
\item
  Select the pre-trained model to use and the directory where the model
  will be stored in your local environment.
\item
  Prepare the dataset to be annotated (if necessary). This includes
  ensuring that the dataset has a column of text to be annotated and a
  grouping column. By default, the names of these columns are expected
  to be \texttt{text} and \texttt{doc\_id}, respectively. The
  \texttt{text} column needs to be a character vector and the
  \texttt{doc\_id} column needs to be a unique index for each text to be
  annotated.
\item
  Annotate the dataset. The result returns a data frame.
\end{enumerate}

Steps 3 and 4 are repeated for the \texttt{enntt\_natives\_curated} and
the \texttt{enntt\_translations\_curated} datasets. For brevity, I will
only show the code for the dataset for the natives. Additionally, I will
subset the dataset to 10,000 randomly selected lines for both dataset,
as in Example~\ref{exm-td-generation-subset-natives} for the natives.
Syntactic annotation is a computationally expensive operation and the
natives and translations datasets contain 116,341 and 738,597
observations, respectively.

\begin{example}[]\protect\hypertarget{exm-td-generation-subset-natives}{}\label{exm-td-generation-subset-natives}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Subset the natives ENNTT dataset}
\NormalTok{enntt\_natives\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_curated\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{example}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{hand-point-up} Tip} Computational Performance

In your own research computationally expensive cannot be avoided, but it
can be managed. One strategy is to work with a subset of the data until
your code is working as expected. Once you are confident that your code
is working as expected, then you can scale up to the full dataset.

If you are using Quarto, you can use the \texttt{cache:\ true} metadata
field in your code blocks to cache the results of computationally
expensive code blocks. This will allow you to run your code once and
then use the cached results for subsequent runs.

Parallel processing is another strategy for managing computationally
expensive code. Some packages, such as \texttt{udpipe}, have built-in
support for parallel processing. Other packages, such as
\texttt{tidytext}, do not. In these cases, you can use the
\texttt{future} package (\protect\hyperlink{ref-R-future}{Bengtsson
2023}) to parallelize your code.

\end{tcolorbox}

With the \texttt{enntt\_natives\_tbl} object, let's execute steps 1-4,
as seen in Example~\ref{exm-td-generation-udpipe-natives}.

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives}{}\label{exm-td-generation-udpipe-natives}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(udpipe)}

\CommentTok{\# Model and directory}
\NormalTok{model }\OtherTok{\textless{}{-}} \StringTok{"english"}
\NormalTok{model\_dir }\OtherTok{\textless{}{-}} \StringTok{"../data/"}

\CommentTok{\# Prepare the dataset to be annotated}
\NormalTok{enntt\_natives\_prepped\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, text)}

\CommentTok{\# Annotate the dataset}
\NormalTok{enntt\_natives\_ann }\OtherTok{\textless{}{-}} 
  \FunctionTok{udpipe}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_natives\_prepped\_tbl, }
    \AttributeTok{object =}\NormalTok{ model, }
    \AttributeTok{model\_dir =}\NormalTok{ model\_dir}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tibble}\NormalTok{()}

\CommentTok{\# Preview }
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_anno)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 264,124
> Columns: 17
> $ doc_id        <chr> "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "1", "~
> $ paragraph_id  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
> $ sentence_id   <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
> $ sentence      <chr> "It is extremely important that action is taken to ensur~
> $ start         <int> 1, 4, 7, 17, 27, 32, 39, 42, 48, 51, 58, 63, 66, 73, 77,~
> $ end           <int> 2, 5, 15, 25, 30, 37, 40, 46, 49, 56, 61, 64, 71, 75, 82~
> $ term_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1~
> $ token_id      <chr> "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11",~
> $ token         <chr> "It", "is", "extremely", "important", "that", "action", ~
> $ lemma         <chr> "it", "be", "extremely", "important", "that", "action", ~
> $ upos          <chr> "PRON", "AUX", "ADV", "ADJ", "SCONJ", "NOUN", "AUX", "VE~
> $ xpos          <chr> "PRP", "VBZ", "RB", "JJ", "IN", "NN", "VBZ", "VBN", "TO"~
> $ feats         <chr> "Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs"~
> $ head_token_id <chr> "4", "4", "4", "0", "8", "8", "8", "4", "10", "8", "13",~
> $ dep_rel       <chr> "expl", "cop", "advmod", "root", "mark", "nsubj:pass", "~
> $ deps          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
> $ misc          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ~
\end{verbatim}

\end{example}

There is quite a bit of information which is returned from
\texttt{udpipe()}. Note that the input lines have been tokenized by
word. Each token includes the \texttt{token}, \texttt{lemma}, part of
speech (\texttt{upos} and \texttt{xpos}), morphological features
(\texttt{feats}), and syntactic relationships (\texttt{head\_token\_id}
and \texttt{dep\_rel}). The \texttt{token\_id} keeps track of the
token's position in the sentence and the \texttt{sentence\_id} keeps
track of the sentence's position in the original text. In the case of
the Europarl dataset, most values of \texttt{lines} are just one
sentence, but there are some cases where the \texttt{lines} variable
contains multiple sentences in which the \texttt{sid} will be
incremented. Finally, the \texttt{doc\_id} column and its values
correspond to the \texttt{doc\_id} in the \texttt{enntt\_natives\_tbl}
dataset.

The number of variables in the \texttt{udpipe()} annotation output is
quite overwhelming. However, these attributes come in handy for
manipulating, extracting, and plotting information based on lexical and
syntactic patterns. See the dependency tree in
Figure~\ref{fig-td-generation-udpipe-english-plot-tree} for an example
of the syntactic information that can be extracted from the
\texttt{udpipe()} annotation output.

\begin{figure}[H]

{\centering \includegraphics{transform-datasets_files/figure-pdf/fig-td-generation-udpipe-english-plot-tree-1.pdf}

}

\caption{\label{fig-td-generation-udpipe-english-plot-tree}Plot of the
syntactic tree for a sentence in the ENNTT natives dataset.}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

The plot in Figure~\ref{fig-td-generation-udpipe-english-plot-tree} was
created using the \texttt{rsyntax} package
(\protect\hyperlink{ref-R-rsyntax}{\textbf{R-rsyntax?}}). In addition to
creating dependency tree plots, the \texttt{rsyntax} package can be used
to extract syntactic patterns from the \texttt{udpipe()} annotation
output. \href{https://github.com/vanatteveldt/rsyntax}{See the
documentation for more information}.

\end{tcolorbox}

In Figure~\ref{fig-td-generation-udpipe-english-plot-tree} we see the
syntactic tree for a sentence in the ENNTT natives dataset. Each node is
labeled with the \texttt{token\_id} which provides the linear ordering
of the sentence. Above the nodes the \texttt{dep\_relation}, or
dependency relationship label is provided. These labels are based on the
UD project's
\href{https://universaldependencies.org/u/dep/index.html}{dependency
relations}. We can see that the `ROOT' relation is at the top of the
tree and corresponds to the verb `brought'. `ROOT' relations mark
predicates in the sentence. Not seen in the example tree, `cop' relation
is a copular, or non-verbal predicate and should be included. These are
the key syntactic pattern we will use to identify main clauses for
T-units. Now we need to identify the subordinate clauses. In the UD
project's listings, the relations `ccomp' (clausal complement), `xcomp'
(open clausal complement), and `acl:relcl' (relative clause), as seen in
Figure~\ref{fig-td-generation-udpipe-english-plot-tree}) are subordinate
clauses.

To calculate the number of T-units and words per sentence we turn to the
\texttt{dplyr} package. We will use the \texttt{group\_by()} function to
group the dataset by \texttt{doc\_id} and \texttt{sentence\_id} and then
use the \texttt{summarize()} function to calculate the number of T-units
and words per sentence, where a T-unit is the combination of the sum of
main clauses and sum of subordinante clauses. The code is seen in
Example~\ref{exm-td-generation-udpipe-natives-tunits-words}.

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives-tunits-words}{}\label{exm-td-generation-udpipe-natives-tunits-words}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the number of T{-}units and words per sentence}
\NormalTok{enntt\_natives\_ann\_tunits\_words\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_ann }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id, sentence\_id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{main\_clauses =} \FunctionTok{sum}\NormalTok{(dep\_rel }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"ROOT"}\NormalTok{, }\StringTok{"cop"}\NormalTok{)),}
    \AttributeTok{subord\_clauses =} \FunctionTok{sum}\NormalTok{(dep\_rel }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"ccomp"}\NormalTok{, }\StringTok{"xcomp"}\NormalTok{, }\StringTok{"acl:relcl"}\NormalTok{)),}
    \AttributeTok{t\_units =}\NormalTok{ main\_clauses }\SpecialCharTok{+}\NormalTok{ subord\_clauses,}
    \AttributeTok{word\_len =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_ann\_tunits\_words\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,199
> Columns: 6
> $ doc_id         <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "100~
> $ sentence_id    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ main_clauses   <int> 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 1, 1~
> $ subord_clauses <int> 3, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 0, 3, 2, 0, 4, 2, 1, 1~
> $ t_units        <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2~
> $ word_len       <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, ~
\end{verbatim}

\end{example}

A quick spot check of some sentences calculations
\texttt{enntt\_natives\_ann\_tunits\_words\_tbl} dataset against the
\texttt{enntt\_natives\_ann} is good to ensure that the calculation is
working as expected. In
Figure~\ref{fig-td-generation-udpipe-natives-tunits} we see a sentence
that has a word length of 13 and a T-unit value of 5.

\begin{figure}[H]

{\centering \includegraphics{transform-datasets_files/figure-pdf/fig-td-generation-udpipe-natives-tunits-1.pdf}

}

\caption{\label{fig-td-generation-udpipe-natives-tunits}Sentence with a
word length of 13 and a T-unit value of 5.}

\end{figure}

Now we can drop the intermediate columns we created to calculate our key
syntactic complexity measures using \texttt{select()} to indicate those
that we do want to keep. I will assign the result to
\texttt{enntt\_natives\_syn\_comp} as we will be working with syntactic
complexity measures for the native texts, as seen in
Example~\ref{exm-td-generation-udpipe-natives-tunits-words-select}.

\begin{example}[]\protect\hypertarget{exm-td-generation-udpipe-natives-tunits-words-select}{}\label{exm-td-generation-udpipe-natives-tunits-words-select}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select columns}
\NormalTok{enntt\_natives\_syn\_comp }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_ann\_tunits\_words\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(doc\_id, sentence\_id, t\_units, word\_len)}
\end{Highlighting}
\end{Shaded}

\end{example}

Now we can repeat the process for the ENNTT translated dataset. I will
assign the result to \texttt{enntt\_translations\_syn\_comp}. The next
step is to join the \texttt{sentences} from the annotated data frames
into our datasets so that we have the information we set out to generate
for both datasets. Then we will concatenate both the
\texttt{enntt\_natives\_syn\_comp} and
\texttt{enntt\_translations\_syn\_comp} datasets into a single dataset.
Both the join and the contactenation will be covered in the next
section.

\hypertarget{sec-td-merging}{%
\section{Merging}\label{sec-td-merging}}

One final class of transformations that can be applied to curated
datasets to enhance their informativeness for a research project is the
process of merging two or more datasets. There are two primary types of
merging: joins and concatenation. \textbf{Joins} can be row- or
column-wise operations that combine datasets based on a common attribute
or set of attributes. \textbf{Concatenation} is exclusively a row-wise
operation that combines datasets that share the same attributes.

Of the two types of merges, joins are the most powerful and sometimes
more difficult to understand. When two datasets are joined at least one
common variable must be shared between the two datasets. The common
variable(s) are referred to as \textbf{keys}. The keys are used to match
observations in one dataset with observations in another dataset by
serving as an index.

There are a number of join types. The most common are left, full, semi,
and anti. The type of join determines which observations are retained in
the resulting dataset. Let's see this in practice. First, let's create
two datasets to join with a common variable \texttt{key}, as seen in
Example~\ref{exm-td-merging-join-dfs}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-dfs}{}\label{exm-td-merging-join-dfs}

~

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{key =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{),}
    \AttributeTok{a =}\NormalTok{ letters[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\NormalTok{  )}

\NormalTok{a\_tbl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 2
>     key a    
>   <dbl> <chr>
> 1     1 a    
> 2     2 b    
> 3     3 c    
> 4     5 d    
> 5     8 e
\end{verbatim}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b\_tbl }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}
    \AttributeTok{key =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{),}
    \AttributeTok{b =}\NormalTok{ letters[}\DecValTok{6}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\NormalTok{  )}

\NormalTok{b\_tbl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 2
>     key b    
>   <dbl> <chr>
> 1     1 f    
> 2     2 g    
> 3     4 h    
> 4     6 i    
> 5     8 j
\end{verbatim}

}

\end{minipage}%

\end{figure}

\end{example}

The \texttt{a\_tbl} and the \texttt{b\_tbl} datasets share the
\texttt{key} variable, but the values in the \texttt{key} variable are
not identical. The two datasets share values \texttt{1}, \texttt{2}, and
\texttt{8}. The \texttt{a\_tbl} dataset has values \texttt{3} and
\texttt{5} in the \texttt{key} variable and the \texttt{b\_tbl} dataset
has values \texttt{4} and \texttt{6} in the \texttt{key} variable.

If we apply a left join to the \texttt{a\_tbl} and \texttt{b\_tbl}
datasets, the result will be a dataset that retains all of the
observations in the \texttt{a\_tbl} dataset and only those observations
in the \texttt{b\_tbl} dataset that have a match in the \texttt{a\_tbl}
dataset. The result is seen in Example~\ref{exm-td-merging-join-left}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-left}{}\label{exm-td-merging-join-left}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{left\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl, }\AttributeTok{by =} \StringTok{"key"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 3
>     key a     b    
>   <dbl> <chr> <chr>
> 1     1 a     f    
> 2     2 b     g    
> 3     3 c     <NA> 
> 4     5 d     <NA> 
> 5     8 e     j
\end{verbatim}

\end{example}

Now, if the key variable has the same name, R will recognize and assume
that this is the variable to join on and we don't need the
\texttt{by\ =} argument, but if there are multiple potential key
variables, we use \texttt{by\ =} to specify which one to use.

A full join retains all observations in both datasets, as seen in
Example~\ref{exm-td-merging-join-full}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-full}{}\label{exm-td-merging-join-full}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{full\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 7 x 3
>     key a     b    
>   <dbl> <chr> <chr>
> 1     1 a     f    
> 2     2 b     g    
> 3     3 c     <NA> 
> 4     5 d     <NA> 
> 5     8 e     j    
> 6     4 <NA>  h    
> 7     6 <NA>  i
\end{verbatim}

\end{example}

Left and full joins maintain or increase the number of observations. On
the other hand, semi and anti joins aim to decrease the number of
observations. A semi join retains only those observations in the left
dataset that have a match in the right dataset, as seen in
Example~\ref{exm-td-merging-join-semi}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-semi}{}\label{exm-td-merging-join-semi}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{semi\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 2
>     key a    
>   <dbl> <chr>
> 1     1 a    
> 2     2 b    
> 3     8 e
\end{verbatim}

\end{example}

And an anti join retains only those observations in the left dataset
that do not have a match in the right dataset, as seen in
Example~\ref{exm-td-merging-join-anti}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-anti}{}\label{exm-td-merging-join-anti}

~

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anti\_join}\NormalTok{(}\AttributeTok{x =}\NormalTok{ a\_tbl, }\AttributeTok{y =}\NormalTok{ b\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 2 x 2
>     key a    
>   <dbl> <chr>
> 1     3 c    
> 2     5 d
\end{verbatim}

\end{example}

Of these join types, the left join and the anti join are some of the
most common to encounter in research projects.

::: \{.callout\} \textbf{\faIcon{lightbulb} Consider this}

In addition to datasets that are part of an acquired resource or derived
from a corpus resource, there are also a number of datasets that are
included in R packages that are particularly relevant for text analysis.
For example, the \texttt{tidytext} package includes \texttt{sentiments}
and \texttt{stop\_words} datasets. The \texttt{lexicon} package
(\protect\hyperlink{ref-R-lexicon}{Rinker 2019}) includes large number
of datasets that include sentiment lexicons, stopword lists,
contractions, and more.

Consider the \texttt{lexicon::key\_contractions} dataset. This dataset
includes a list of common contractions and their expanded forms.

What needs to be done to join these two datasets? What are the keys?
What type of join should be used? What is the resulting dataset?

\begin{table}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\hypertarget{tbl-td-merging-join-lexicon-contractions-list}{}
\caption{\label{tbl-td-merging-join-lexicon-contractions-list}Common contractions and expanded forms. }\tabularnewline

\centering
\begin{tabular}{l|l}
\hline
contraction & expanded\\
\hline
'cause & because\\
\hline
'tis & it is\\
\hline
'twas & it was\\
\hline
ain't & am not\\
\hline
aren't & are not\\
\hline
can't & can not\\
\hline
could've & could have\\
\hline
it's & it is\\
\hline
\end{tabular}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\begin{longtable}[]{@{}llll@{}}
\caption{\label{tbl-td-merging-join-lexicon-contractions}Example
tokenized dataset with contractions.}\tabularnewline
\toprule\noalign{}
doc\_id & sent\_id & token\_id & token \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
doc\_id & sent\_id & token\_id & token \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 1 & I \\
1 & 1 & 2 & can't \\
1 & 1 & 3 & believe \\
1 & 1 & 4 & that \\
1 & 1 & 5 & it's \\
1 & 1 & 6 & not \\
1 & 1 & 7 & butter \\
1 & 1 & 8 & ! \\
\end{longtable}

}

\end{minipage}%

\end{table}

\hypertarget{sec-td-merging-orientation}{%
\subsection{Orientation}\label{sec-td-merging-orientation}}

With this in mind, let's return to our syntactic simplification
investigation. Recall that we started with two curated ENNTT datasets:
the natives and translations. We manipulated these datasets subsetting
them to 10,000 randomly selected lines, prepped them for annotation by
adding a \texttt{doc\_id} column and dropping all columns except
\texttt{text}, and then annotated them using the \texttt{udpipe}
package. We then calculated the number of T-units and words per
sentence.

These steps produced two datasets for both the natives and for the
translations. The first dataset for each is the annotated data frame.
The second is the data frame with the sytactic complexity measures we
calculated. The annotated data frames are named
\texttt{enntt\_natives\_ann} and \texttt{enntt\_translations\_ann}. The
data frames with the syntactic complexity measures are named
\texttt{enntt\_natives\_syn\_comp} and
\texttt{enntt\_translations\_syn\_comp}.

In the end, we want a dataset that looks something like
Table~\ref{tbl-td-merging-idealized}.

\hypertarget{tbl-td-merging-idealized}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1279}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0814}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0930}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.6279}}@{}}
\caption{\label{tbl-td-merging-idealized}Idealized merged dataset for
the syntactic simplification investigation.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
doc\_id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t\_units
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
word\_len
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & natives & 1 & 5 & I am happy right now. \\
2 & translation & 3 & 11 & I think that John believes that Mary is a
good person. \\
\end{longtable}

To create this unified dataset, we will need to apply joins and
concatenation. First, we will join the prepped datasets with the
annotated datasets. Then we will concatenate the two resulting datasets.

\hypertarget{sec-td-merging-application}{%
\subsection{Application}\label{sec-td-merging-application}}

Let's start by joining the annotated datasets
(\texttt{enntt\_natives\_ann} and \texttt{enntt\_translations\_ann})
with the datasets with the syntactic complexity calculations
(\texttt{enntt\_natives\_syn\_comp} and
\texttt{enntt\_translations\_syn\_comp}). In these joins, we can see
that the prepped and calculated datasets share a couple variables,
\texttt{doc\_id} and \texttt{sentence\_id}, in
Example~\ref{exm-td-merging-join-prepped-syn-comp}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp}{}\label{exm-td-merging-join-prepped-syn-comp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview datasets to join}
\NormalTok{enntt\_natives\_ann }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 17
>   doc_id paragraph_id sentence_id sentence    start   end term_id token_id token
>   <chr>         <int>       <int> <chr>       <int> <int>   <int> <chr>    <chr>
> 1 1                 1           1 It is extr~     1     2       1 1        It   
> 2 1                 1           1 It is extr~     4     5       2 2        is   
> 3 1                 1           1 It is extr~     7    15       3 3        extr~
> # i 8 more variables: lemma <chr>, upos <chr>, xpos <chr>, feats <chr>,
> #   head_token_id <chr>, dep_rel <chr>, deps <chr>, misc <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{enntt\_natives\_syn\_comp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 3 x 4
>   doc_id sentence_id t_units word_len
>   <chr>        <int>   <int>    <int>
> 1 1                1       4       21
> 2 10               1       2       25
> 3 100              1       1       27
\end{verbatim}

\end{example}

The \texttt{doc\_id} and \texttt{sentence\_id} variables are both keys
that we will use to join the datasets. The reason being that if we only
use one of the two we will not align the two datasets at the sentence
level. Only the combination of \texttt{doc\_id} and
\texttt{sentence\_id} isolates the sentences for which we have syntactic
complexity measures. Beyond a having common variable (or variables in
our case), we must also ensure that join key variables are of the same
vector type in both data frames and that we are aware of any differences
in the values. From the output in
Example~\ref{exm-td-merging-join-prepped-syn-comp}, we can see that the
\texttt{doc\_id} and \texttt{sentence\_id} variables aligned in terms of
vector type; \texttt{doc\_id} is character and \texttt{sentence\_id} is
integer in both data frames. If they happened not to be, there types
would need to be adjusted.

Now, we need to check for differences in the values. We can do this by
using the \texttt{setequal()} function. This function returns
\texttt{TRUE} if the two vectors are equal and \texttt{FALSE} if they
are not. If the two vectors are not equal, the function will return the
values that are in one vector but not the other. So if one has
\texttt{10001} and the other doesn't we will get \texttt{FALSE}. Let's
see this in practice, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-check}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-check}{}\label{exm-td-merging-join-prepped-syn-comp-check}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check for differences in the values}
\FunctionTok{setequal}\NormalTok{(}
\NormalTok{  enntt\_natives\_ann}\SpecialCharTok{$}\NormalTok{doc\_id, }
\NormalTok{  enntt\_natives\_syn\_comp}\SpecialCharTok{$}\NormalTok{doc\_id}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{setequal}\NormalTok{(}
\NormalTok{  enntt\_natives\_ann}\SpecialCharTok{$}\NormalTok{sentence\_id, }
\NormalTok{  enntt\_natives\_syn\_comp}\SpecialCharTok{$}\NormalTok{sentence\_id}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] TRUE
\end{verbatim}

\end{example}

So the values are the same. The final check is to see if the vectors are
of the same length. We know the values are the same, but we don't know
if the values are repeated. We do this by simply comparing the length of
the vectors, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-length}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-length}{}\label{exm-td-merging-join-prepped-syn-comp-length}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check for differences in the length}
\FunctionTok{length}\NormalTok{(enntt\_natives\_ann}\SpecialCharTok{$}\NormalTok{doc\_id) }\SpecialCharTok{==} 
  \FunctionTok{length}\NormalTok{(enntt\_natives\_syn\_comp}\SpecialCharTok{$}\NormalTok{doc\_id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(enntt\_natives\_ann}\SpecialCharTok{$}\NormalTok{sentence\_id) }\SpecialCharTok{==} 
  \FunctionTok{length}\NormalTok{(enntt\_natives\_syn\_comp}\SpecialCharTok{$}\NormalTok{sentence\_id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] FALSE
\end{verbatim}

\end{example}

So they are not the same length. Using the \texttt{nrow()} function, I
can see that the annotated dataset has 264124 observations and the
calculated dataset has 10199 observations. The annotation data frames
will have many more observations due to the fact that the unit of
observations is word tokens. The calculated data frames' unit of
observation is the sentence.

To appreciate the difference in the number of observations, let's look
at the first 10 observations of the natives annotated frame for just the
columns of interest, as seen in
Example~\ref{exm-td-merging-join-prepped-syn-comp-ann}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-prepped-syn-comp-ann}{}\label{exm-td-merging-join-prepped-syn-comp-ann}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview the annotated dataset}
\NormalTok{enntt\_natives\_ann }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(doc\_id, sentence\_id, sentence, token) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    doc_id sentence_id sentence                                             token
>    <chr>        <int> <chr>                                                <chr>
>  1 1                1 It is extremely important that action is taken to e~ It   
>  2 1                1 It is extremely important that action is taken to e~ is   
>  3 1                1 It is extremely important that action is taken to e~ extr~
>  4 1                1 It is extremely important that action is taken to e~ impo~
>  5 1                1 It is extremely important that action is taken to e~ that 
>  6 1                1 It is extremely important that action is taken to e~ acti~
>  7 1                1 It is extremely important that action is taken to e~ is   
>  8 1                1 It is extremely important that action is taken to e~ taken
>  9 1                1 It is extremely important that action is taken to e~ to   
> 10 1                1 It is extremely important that action is taken to e~ ensu~
\end{verbatim}

\end{example}

The annotated data frames have a lot of redundancy in for the join
variables and the \texttt{sentence} variable that we want to add to the
calculated data frames. We can reduce the redundancy by using the
\texttt{distinct()} function from the \texttt{dplyr} package. In this
case we want all observations where \texttt{doc\_id},
\texttt{sentence\_id} and \texttt{sentence} are distinct. We then select
these variables with \texttt{distinct()}, as seen in
Example~\ref{exm-td-merging-annotation-distinct}.

\begin{example}[]\protect\hypertarget{exm-td-merging-annotation-distinct}{}\label{exm-td-merging-annotation-distinct}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reduce annotated data frames to unique sentences}
\NormalTok{enntt\_natives\_ann\_distinct }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_ann }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{(doc\_id, sentence\_id, sentence)}

\NormalTok{enntt\_translations\_ann\_distinct }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_translations\_ann }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{distinct}\NormalTok{(doc\_id, sentence\_id, sentence)}
\end{Highlighting}
\end{Shaded}

\end{example}

We now have two datasets that are ready to be joined with the calculated
datasets. The next step is to join the two. We will employ a left join
where the syntactic complexity data frames are on the left and the join
variables will be both the \texttt{doc\_id} and \texttt{sentence\_id}
variables. The code is seen in
Example~\ref{exm-td-merging-join-left-syn-comp}.

\begin{example}[]\protect\hypertarget{exm-td-merging-join-left-syn-comp}{}\label{exm-td-merging-join-left-syn-comp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the native datasets}
\NormalTok{enntt\_natives\_transformed\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{left\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_natives\_syn\_comp, }
    \AttributeTok{y =}\NormalTok{ enntt\_natives\_ann\_distinct, }
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"doc\_id"}\NormalTok{, }\StringTok{"sentence\_id"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Preview }
\FunctionTok{glimpse}\NormalTok{(enntt\_natives\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,199
> Columns: 5
> $ doc_id      <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "1003",~
> $ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ t_units     <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1~
> $ word_len    <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16,~
> $ sentence    <chr> "It is extremely important that action is taken to ensure ~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join the translations datasets }
\NormalTok{enntt\_translations\_transformed\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{left\_join}\NormalTok{(}
    \AttributeTok{x =}\NormalTok{ enntt\_translations\_syn\_comp,}
    \AttributeTok{y =}\NormalTok{ enntt\_translations\_ann\_distinct,}
    \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"doc\_id"}\NormalTok{, }\StringTok{"sentence\_id"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_translations\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 10,392
> Columns: 5
> $ doc_id      <chr> "1", "10", "100", "1000", "10000", "1001", "1002", "1003",~
> $ sentence_id <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~
> $ t_units     <int> 0, 2, 0, 1, 3, 0, 3, 2, 3, 3, 2, 0, 1, 0, 3, 1, 0, 1, 2, 2~
> $ word_len    <int> 24, 31, 5, 39, 44, 26, 67, 23, 46, 28, 24, 68, 19, 18, 36,~
> $ sentence    <chr> "To my great surprise , on leaving the sitting , I found t~
\end{verbatim}

\end{example}

The two data frames now have the same columns and we are closer to our
final dataset. The next step is to move toward concatenating the two
datasets. Before we do that, we need to do some preparation. First, and
most important, we need to add a \texttt{type} column to each dataset.
This column will indicate whether the sentence is a native or a
translation. The second is that our \texttt{doc\_id} does not serve as a
unique identifier for the sentences. Only in combination with
\texttt{sentence\_id} can we uniquely identify a sentence.

So our plan will be to add a \texttt{type} column to each dataset
specifying the values for all the observations in the respective
dataset. Then we will concatenate the two datasets. Note, if we combine
them before, distiguishing the type will be more difficult. After we
concatenate the two datasets, we will add a \texttt{doc\_id} column that
will serve as a unique identifier for the sentences and drop the
\texttt{sentence\_id} column. OK, that's the plan. Let's execute it in
Example~\ref{exm-td-merging-concatenation}.

\begin{example}[]\protect\hypertarget{exm-td-merging-concatenation}{}\label{exm-td-merging-concatenation}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a type column}
\NormalTok{enntt\_natives\_transformed\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_natives\_transformed\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"natives"}\NormalTok{)}

\NormalTok{enntt\_translations\_transformed\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_translations\_transformed\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{type =} \StringTok{"translations"}\NormalTok{)}

\CommentTok{\# Concatenate the datasets}
\NormalTok{enntt\_transformed\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{bind\_rows}\NormalTok{(}
\NormalTok{    enntt\_natives\_transformed\_tbl,}
\NormalTok{    enntt\_translations\_transformed\_tbl}
\NormalTok{  )}

\CommentTok{\# Overwrite the doc\_id column with a unique identifier}
\NormalTok{enntt\_transformed\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  enntt\_transformed\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{row\_number}\NormalTok{()) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(doc\_id, type, t\_units, word\_len, }\AttributeTok{text =}\NormalTok{ sentence)}

\CommentTok{\# Preview}
\FunctionTok{glimpse}\NormalTok{(enntt\_transformed\_tbl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Rows: 20,591
> Columns: 5
> $ doc_id   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18~
> $ type     <chr> "natives", "natives", "natives", "natives", "natives", "nativ~
> $ t_units  <int> 4, 2, 1, 0, 3, 2, 1, 2, 1, 2, 2, 1, 4, 2, 2, 4, 3, 2, 2, 1, 1~
> $ word_len <int> 21, 25, 27, 15, 40, 43, 29, 23, 13, 30, 33, 9, 68, 35, 16, 53~
> $ text     <chr> "It is extremely important that action is taken to ensure tha~
\end{verbatim}

\end{example}

The output of Example~\ref{exm-td-merging-concatenation} now looks like
Table~\ref{tbl-td-merging-idealized}. We have a dataset that has the
syntactic complexity measures for both the natives and the translations.
We can now write this dataset to disk and document it in the data
dictionary.

\hypertarget{summary-6}{%
\section*{Summary}\label{summary-6}}
\addcontentsline{toc}{section}{Summary}

\markright{Summary}

In this chapter we covered the process of transforming datasets. The
goal is to manipulate the curated dataset to make it align better for
analysis. We covered various types of transformation procedures from
text normalization to data frame merges. In any given research project
some or all of these steps will be employed --but not necessarily in the
order presented in this chapter. It is not uncommon to mix procedures as
well. The etiology of the transformation is as unique as the data that
you are working with.

Since you are applying techniques that have a significant factor on the
shape and contents of your dataset(s) it is important to perform data
checks to ensure that the transformations are working as expected. You
may not catch everything, and some things may not be caught until later
in the analysis process, but it is important to do as much as you can as
early as you can.

In line with the reproducible research principles, it is important to
write the transformed dataset to disk and to document it in the data
dictionary. This is especially important if you are working with
multiple datasets. Good naming conventions als come into play. Choosing
descriptive names is so easily overlooked by your present self but so
welcomed by your future self.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\faIcon{wrench} more on concluding this part and moving to the next part

This chapter concludes the section on data/ dataset preparation. The
next section we turn to analyzing datasets. This is the stage where we
interrogate the datasets to derive knowledge and insight either through
exploratory, predictive, or inferential methods.

\hypertarget{activities-5}{%
\section*{Activities}\label{activities-5}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{itemize}
\tightlist
\item[$\square$]
  \faIcon{wrench} Add description of outcomes
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_8.html}{Dataset
manipulation: tokenization and joining datasets}\\
\textbf{How}: Read Recipe 8 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To work with to primary types of transformations,
tokenization and joins. Tokenization is the process of recasting textual
units as smaller textual units. The process of joining datasets aims to
incorporate other datasets to augment or filter the dataset of interest.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/lin380/lab_8}{Dataset
manipulation: tokenization and joining datasets}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 8.\\
\textbf{Why}: To gain experience working with coding strategies for
transforming datasets using tidyverse functions and regular expressions,
practice reading/ writing data from/ to disk, and implement
organizational strategies for organizing and documenting a dataset in
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-6}{%
\section*{Questions}\label{questions-6}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Conceptual questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Technical questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{enumerate}

\end{tcolorbox}

\part{Analysis}

In this section we turn to the analysis of datasets, the evaluation of
results, and the interpretation of the findings. We will outline the
three main types of statistical analyses: Exploratory Data Analysis
(EDA), Predictive Data Analysis (PDA), and Inferential Data Analysis
(IDA). Each of these analysis types have distinct, non-overlapping aims
and therefore should be determined from the outset of the research
project and included as part of the research blueprint. The aim of this
section is to establish a clearer picture of the goals, methods, and
value of each of these approaches.

\hypertarget{sec-exploration}{%
\chapter{Exploration}\label{sec-exploration}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

\begin{quote}
The data speaks for itself, but only if we are willing to listen.

--- Nate Silver
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{list-alt} Outcomes}

\begin{itemize}
\tightlist
\item
  Identify when an exploratory data analysis approach is the best fit
  for a given research project.
\item
  Describe the fundamental methods of descriptive analysis and
  unsupervised learning, recognizing their strengths in revealing
  patterns and summarizing data.
\item
  Interpret the basic insights gained from data summarization and
  pattern recognition, considering how these insights could guide
  further questions or research.
\end{itemize}

\end{tcolorbox}

In this chapter, we examine a wide range of strategies for deriving
insight from data in cases where the researcher does not start with a
preconceived hypothesis or prediction, but rather the researcher aims to
uncover patterns and associations from data allowing the data to guide
the trajectory of the analysis. The chapter outlines two main branches
of exploratory data analysis: 1) descriptive analysis which
statistically and/ or visually summarizes a dataset and 2) unsupervised
learning which is a machine learning approach that does not assume any
particular relationship between variables in a dataset. Either through
descriptive or unsupervised learning methods, exploratory data analysis
employs quantitative methods to summarize, reduce, and sort complex
datasets and statistically and visually interrogate a dataset in order
to provide the researcher novel perspective to be qualitatively
assessed.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{terminal} Lessons}

\textbf{What}: \href{https://github.com/qtalr/lessons}{Matrices,
Exploratory Visualization}\\
\textbf{How}: In the R Console pane load \texttt{swirl}, run
\texttt{swirl()}, and follow prompts to select the lesson.\\
\textbf{Why}: Learn how to work with matrices to store and analyze
numeric data using \texttt{quanteda} and to further your understanding
of graphically representing data using \texttt{ggplot2} and other
packages for more advanced plotting.

\end{tcolorbox}

\hypertarget{sec-eda-orientation}{%
\section{Orientation}\label{sec-eda-orientation}}

The aim of this section is to provide an overview of exploratory data
analysis (EDA). We will delve into various descriptive methods, such as
frequency analysis and co-occurrence analysis, which are fundamental
tools in linguistic research. However, our exploration won't stop there.
We will also integrate modern exploratory methods from unsupervised
learning approaches, including clustering, topic modeling, and vector
space modeling. This may sound overwhelming, but I will strive to keep
explanations clear and concise, ensuring their practicality and
relevance to your linguistic inquiries is apparent. To this end, we will
provide real-world examples to exemplify the applicability of these
methodologies.

\hypertarget{sec-eda-research-goal}{%
\subsection{Research goal}\label{sec-eda-research-goal}}

As discussed in Section~\ref{sec-aa-explore} and
Section~\ref{sec-fr-aim}, the goal of exploratory data analysis is to
discover, describe, and posit new hypotheses. The researcher does not
start with a preconceived hypothesis or prediction, but rather the
researcher aims to uncover patterns and associations from data allowing
the data to guide the trajectory of the analysis. This analysis approach
is best-suited for research where the literature on a research question
is limited, or where the researcher is interested in exploring a new
research question.

Since the researcher does not start with a preconceived hypothesis, the
researcher is not able to test a hypothesis and generalize to a
population, but rather the researcher is able to describe the data and
provide a new perspective to be qualitatively assessed. This is achieved
through an iterative and inductive process of data exploration, where
the researcher uses quantitative methods to summarize, reduce, and sort
complex datasets and statistically and visually interrogate a dataset
letting the data guide the analysis.

\hypertarget{sec-eda-approach}{%
\subsection{Approach}\label{sec-eda-approach}}

The approach to exploratory data analysis is iterative and inductive. To
reign in the analysis, however, it is important to have a research
question to guide the analysis. The research question will often be
broad and exploratory in nature, but it will provide a framework for the
analysis including the unit of analysis and sometimes the units of
observation. Yet the units of observation can be modified as needed to
address the research question. Furthermore, the methods applied to the
data can evolve as the research unfolds. The researcher may start with a
descriptive analysis and then move to an unsupervised learning approach,
or vice versa. The researcher may also pivot the approach to explore new
questions and new variables. Ultimately, the researcher is guided by the
data and the research question, but the researcher is not bound by a
preconceived hypothesis or prediction.

With a research question and relevant data in hand, we can look to
conduct the analysis. The workflow for exploratory data analysis is as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify and extract the variables of interest in the dataset
\item
  Inspect the dataset to ensure the quality of the data and understand
  its characteristics
\item
  Interrogate the dataset using descriptive analysis and/ or
  unsupervised learning
\item
  Interpret the results of the analysis to determine if they are
  meaningful and if they provide a new perspective on the research
  question
\item
  (Optional) Pivot and repeat steps 1-4 to explore new questions and new
  variables
\end{enumerate}

Let's elaborate on each of these steps. First, we want to consider our
research question and identify the variables of potential interest to
provide insight to our question. Starting with a transformed dataset
means that much of the data preparation has already been done, but we
may need to further transform the data, either up front or as we explore
the data. In text analysis, this often includes identifying and
extracting the linguistic variables of interest, such as words,
\(n\)-grams, sentences, \emph{etc}. Depending on the annotation scheme,
other linguistic variables may be of interest, such as part-of-speech
tags, syntactic dependencies, semantic roles, \emph{etc}.

We may also want to consider the operational measures of the variables
derived from the text, such as frequency, dispersion, co-occurrence,
keyness, \emph{etc}. We may also want to consider the other variables in
the dataset that may be target for grouping or filtering the dataset,
such as speaker information, document information, linguistic unit
information, \emph{etc}.

During or after extracting and operationalizing the variables of
interest, we want to inspect the dataset to ensure the quality of the
data and understand its characteristics. This may include checking for
missing data, checking for outliers, checking for errors, checking for
inconsistencies, \emph{etc}. We may also want to inspect the
distribution of the variables of interest to understand their
characteristics. Summary statistics and visualizations, such as those
covered in Section~\ref{sec-aa-diagnose}, are useful for inspecting the
dataset and also provide a foundation for interrogating the dataset.

Once we have identified the variables of interest and inspected the
dataset, we can interrogate the dataset using descriptive analysis and/
or unsupervised learning. Descriptive analysis is a set of methods that
statistically and/ or visually summarizes a dataset. Descriptive
analysis can be used to describe a dataset and to identify linguistic
units (frequency analysis) or co-occuring (co-occurrence analysis) units
that are distinctive to a particular group or sub-group in the dataset.
Unsupervised learning is a machine learning approach that does not
assume any particular relationship between variables in a dataset. It
can be used to identify groupings (clustering) in the data including
patterning of linguistic units, identifying semantically similar topics
(topic modeling), and estimating word context relationships (vector
space modeling).

Exploratory methods will produce a set of statistical and/ or visual
results. The researcher must interpret these results to determine if
they are meaningful and if they provide a new perspective on the
research question. Many times the results from one method will lead to
new questions which can be explored with other methods. In some cases,
the results may not be meaningful and the researcher may need to return
to the data preparation stage to modify the dataset or the variables of
interest. As the aim of exploratory analysis is just that, to explore,
the researcher can pivot the approach to explore new questions and new
variables. Ultimately, what is meaningful is determined by the
researcher in the light of the research question and the potential
insight obtained from the results.

\hypertarget{sec-eda-analysis}{%
\section{Analysis}\label{sec-eda-analysis}}

In this section will discuss exploratory data analysis (EDA) for
linguists, with a focus on descriptive methods such as frequency
analysis and co-occurence analysis, as well as unsupervised learning
approaches such as clustering, topic modelling, and word embedding. To
ground the discussion, we will use the the Manually Annotated Sub-Corpus
(MASC) of the American National Corpus. The data dictionary for the
\texttt{masc\_transformed} dataset is shown in
Table~\ref{tbl-eda-masc-dd-show}.

\hypertarget{tbl-eda-masc-dd-show}{}
\begin{table}
\caption{\label{tbl-eda-masc-dd-show}Data dictionary for the MASC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l}
\hline
variable & name & variable\_type & description\\
\hline
doc\_id & Document ID & numeric & Unique identifier for each document\\
\hline
description & Description & categorical & Description of the content of the document\\
\hline
modality & Modality & categorical & The form in which the document is presented (written or spoken)\\
\hline
genre & Genre & categorical & The category or type of the document\\
\hline
domain & Domain & categorical & The subject or field to which the document belongs\\
\hline
term\_num & Term Number & numeric & Index number term per document\\
\hline
term & Term & categorical & Individual word forms in the document\\
\hline
lemma & Lemma & categorical & Base or dictionary form of the term\\
\hline
pos & Part of Speech & categorical & Grammatical category of the term (modified PENN Treebank tagset)\\
\hline
\end{tabular}
\end{table}

We will work with the MASC as our dataset to approach a task, more than
a question. The task will be to identify relevant materials for an
English Language Learner (ELL) textbook. This will involve multiple
research questions and allow us to illustrate some very fundamental
concepts that will emerge across text analysis research.

First, I'll read in the dataset and only keep the variables that will
pertain to our task, dropping the \texttt{description} and
\texttt{domain} variables, and preview the dataset in
Example~\ref{exm-eda-masc-read}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-read}{}\label{exm-eda-masc-read}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read and subset the MASC dataset}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}} 
  \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/masc/masc\_transformed.csv"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{description, }\SpecialCharTok{{-}}\NormalTok{domain)}

\CommentTok{\# Preview the MASC dataset}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 5 x 7
>   doc_id modality genre   term_num term         lemma        pos  
>    <dbl> <chr>    <chr>      <dbl> <chr>        <chr>        <chr>
> 1      1 Written  Letters        0 December     december     NNP  
> 2      1 Written  Letters        1 1998         1998         CD   
> 3      1 Written  Letters        2 Your         your         PRP$ 
> 4      1 Written  Letters        3 contribution contribution NN   
> 5      1 Written  Letters        4 to           to           TO
\end{verbatim}

\end{example}

From the output in Example~\ref{exm-eda-masc-read}, we should note a
couple of things. First the \texttt{doc\_id} is treated as numeric
\texttt{\textless{}dbl\textgreater{}} and it is not a quantitative
variable --we should change this vector type to
\texttt{\textless{}chr\textgreater{}}. Second, at some point in our
analysis we may need to recode some of the character variables to factor
variables as analysis methods may require this.

\begin{example}[]\protect\hypertarget{exm-eda-masc-doc-id}{}\label{exm-eda-masc-doc-id}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Change doc\_id to character}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{doc\_id =} \FunctionTok{as.character}\NormalTok{(doc\_id))}
\end{Highlighting}
\end{Shaded}

\end{example}

To get a better sense of distribution of the dataset, let's use
\texttt{skim()} from the \texttt{skimr} package to generate a summary of
the dataset. In particular, let's just focus on the character variables
by using \texttt{yank("character")}, as seen in
Example~\ref{exm-eda-masc-skim}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-skim}{}\label{exm-eda-masc-skim}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(skimr)}

\CommentTok{\# Generate summary of the MASC dataset}
\NormalTok{masc\_tbl\_skm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{skim}\NormalTok{()}

\CommentTok{\# Pull character variables}
\NormalTok{masc\_tbl\_skm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{yank}\NormalTok{(}\StringTok{"character"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-exm-eda-masc-skim}{}
\begin{table}
\caption{\label{tbl-exm-eda-masc-skim}Summary of the MASC dataset. }\tabularnewline

\centering
\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
skim\_variable & n\_missing & complete\_rate & min & max & empty & n\_unique & whitespace\\
\hline
doc\_id & 0 & 1 & 1 & 3 & 0 & 392 & 0\\
\hline
modality & 0 & 1 & 6 & 7 & 0 & 2 & 0\\
\hline
genre & 0 & 1 & 4 & 12 & 0 & 18 & 0\\
\hline
term & 25 & 1 & 1 & 99 & 0 & 39470 & 0\\
\hline
lemma & 4 & 1 & 1 & 99 & 0 & 28008 & 0\\
\hline
pos & 0 & 1 & 2 & 5 & 0 & 39 & 0\\
\hline
\end{tabular}
\end{table}

\end{example}

Looking at Table~\ref{tbl-exm-eda-masc-skim}, we see that there are 392
documents, two modalities, 18 genres, over 30k unique terms (which are
words), over 28k lemmas (word base forms), and 39 distinct
part-of-speech tags.

\hypertarget{sec-eda-descriptive}{%
\subsection{Descriptive analysis}\label{sec-eda-descriptive}}

Descriptive analysis includes common techniques such as frequency
analysis to determine the most frequent words or phrases, dispersion
analysis to see how terms or topics are distributed throughout a
document or corpus, keyword analysis to identify distinctive terms, and/
or co-occurrence analysis to see what terms tend to appear together.

Using the MASC dataset, we will entertain questions such as:

\begin{itemize}
\tightlist
\item
  What are the most common terms a beginning ELL should learn?
\item
  Are there term differences between spoken and written discourses that
  should be emphasized?
\item
  What are the most common verb particle constructions?
\end{itemize}

Along the way, we will introduce some fundamental concepts in text
analysis such as tokens and types and frequency, dispersion, and
co-occurrence measures. In addition, we will apply various descriptive
analysis techniques and visualizations to explore the dataset and
identify new questions and new variables of interest.

\hypertarget{sec-eda-frequency}{%
\subsubsection{Frequency analysis}\label{sec-eda-frequency}}

At its core, frequency analysis is a descriptive method that counts the
number of times a linguistic unit, or term, (\emph{i.e.} word,
\(n\)-gram, sentence, \emph{etc}.) occurs in a dataset. The results of
frequency analysis can be used to describe the dataset and to identify
terms that are linguistically distinctive or distinctive to a particular
group or sub-group in the dataset.

\hypertarget{sec-eda-frequency-raw}{%
\paragraph{Raw frequency}\label{sec-eda-frequency-raw}}

Let's consider what the most common words in the MASC dataset are as a
starting point to making inroads on our task by identifying relevant
vocabulary for an ELL textbook.

In the \texttt{masc\_tbl} data frame we have the linguistic unit
\texttt{term} which corresponds to the word-level annotation of the
MASC. The \texttt{lemma} corresponds to the base form of each term, for
words with inflectional morphology, the lemma is the word sans the
inflection (\emph{e.g.} is - be, are - be). For other words, the
\texttt{term} and the \texttt{lemma} will be the same (\emph{e.g.} the -
the, in - in). These two variables pose a choice point for us: do we
consider words to be the actual forms or the base forms? There is an
argument to be made for both. In this case I will operationalize our
linguistic unit as the \texttt{lemma} variable, as this will allow us to
group words with inflectional morphology together.

To perform a basic word frequency analysis, we start by using the
\texttt{count()} function from the \texttt{dplyr} package to count the
number of times each lemma occurs in the dataset. We'll sort by the most
frequent lemmas, as seen in Example~\ref{exm-eda-masc-count}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count}{}\label{exm-eda-masc-count}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Lemma count, sorted}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 28,009 x 2
>    lemma     n
>    <chr> <int>
>  1 ,     27113
>  2 .     26258
>  3 the   26137
>  4 be    19466
>  5 to    13548
>  6 and   12528
>  7 of    12005
>  8 a     10480
>  9 in     8374
> 10 i      7783
> # i 27,999 more rows
\end{verbatim}

\end{example}

The output of this frequency tabulation in
Example~\ref{exm-eda-masc-count} is a data frame with two columns:
\texttt{lemma} and \texttt{n}. The \texttt{lemma} column contains the
unique lemmas in the dataset, and the \texttt{n} column contains the
frequency of each lemma. The data frame is sorted in descending order by
the frequency of lemmas. Now the result includes over 28,000 rows
--which corresponds to the number of unique lemmas in the dataset.

\begin{itemize}
\tightlist
\item[$\square$]
  re-write this to make tokens-types make more sense with `term' and
  `lemma'
\end{itemize}

At this point, it is important to define two key concepts that are
fundamental to working with text. First, a \textbf{term} is a defined
linguistic unit extracted from a corpus. In our dataset, the terms are
words, such as `the', `houses', `are'. A lemma is an annotated recoding
of words which represent the uninflected base form of a word. In either
case, the term or lemma is an instance of a linguistic unit. These
instances are called \textbf{tokens}. When we count the number of times
a term or lemma occurs in a dataset, we are counting the number of
tokens (\texttt{n}), such as in Example~\ref{exm-eda-masc-count}. Now,
the list of unique linguistic units is a list of \textbf{types}
(\texttt{lemma}). By definition, then, there will always be at least as
many tokens as types, but more often than not (many) more tokens than
types.

Our first pass a calculating lemma frequency in
Example~\ref{exm-eda-masc-count} should bring something else to our
attention. As we can see among the most frequent lemmas are non-words
such as \texttt{,}, and \texttt{.}. As you can imagine, given the
conventions of written and transcriptional language, these types are
very frequent. For a frequency analysis focusing on words, however, we
should probably remove them. Thinking ahead, there may also be other
non-words that we want to remove, such as symbols, numbers, \emph{etc}.
Let's take a look at Table~\ref{tbl-eda-masc-pos}, where I've counted
the part-of-speech tags \texttt{pos} in the dataset to see what other
non-words we might want to remove.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos}{}\label{exm-eda-masc-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# [ ] consider how to present this better, more concisely}

\CommentTok{\# Part{-}of{-}speech tags}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(pos) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(pos) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-eda-masc-pos}{}
\begin{table}
\caption{\label{tbl-eda-masc-pos}Part-of-speech tags in the MASC dataset. }\tabularnewline

\centering
\begin{tabular}{l|r}
\hline
pos & n\\
\hline
CC & 16768\\
\hline
CD & 12786\\
\hline
DT & 48708\\
\hline
EX & 1048\\
\hline
FW & 199\\
\hline
IN & 56710\\
\hline
JJ & 31711\\
\hline
JJR & 1507\\
\hline
JJS & 776\\
\hline
LRB & 3\\
\hline
LS & 54\\
\hline
MD & 6855\\
\hline
NN & 77899\\
\hline
NNP & 46388\\
\hline
NNPS & 839\\
\hline
NNS & 25458\\
\hline
PDT & 259\\
\hline
POS & 2897\\
\hline
PP & 7\\
\hline
PRP & 29202\\
\hline
PRP\$ & 8395\\
\hline
PUNCT & 88258\\
\hline
RB & 23808\\
\hline
RBR & 600\\
\hline
RBS & 216\\
\hline
RP & 1244\\
\hline
SYM & 2973\\
\hline
TO & 13481\\
\hline
UH & 1695\\
\hline
VB & 20072\\
\hline
VBD & 15807\\
\hline
VBG & 9895\\
\hline
VBN & 10111\\
\hline
VBP & 13665\\
\hline
VBZ & 13083\\
\hline
WDT & 2388\\
\hline
WP & 2470\\
\hline
WP\$ & 73\\
\hline
WRB & 2728\\
\hline
\end{tabular}
\end{table}

\end{example}

Consulting the
\href{https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html}{PENN Tagset
online}, we can see that the \texttt{pos} variable includes a number of
non-words or other elements to exclude including:

\begin{itemize}
\tightlist
\item
  `CD' - Cardinal number
\item
  `FW' - Foreign word
\item
  `LS' - List item marker
\item
  `SYM' - Symbol
\end{itemize}

This modified tagset has grouped the punctuation tags into a single tag,
`PUNCT'.

We can use this information to remove lemmas that are tagged with either
of these values. We can do this by filtering the data frame to only
include lemmas that are not tagged with the \texttt{pos} values listed
above, as seen in Example~\ref{exm-eda-masc-count-filter}.

\begin{itemize}
\tightlist
\item[$\square$]
  think about how to name, rename objects: \texttt{masc\_words\_tbl}?
\end{itemize}

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-filter}{}\label{exm-eda-masc-count-filter}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter out lemmas with PUNCT or SYM for pos}
\NormalTok{masc\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(pos }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"CD"}\NormalTok{, }\StringTok{"FW"}\NormalTok{, }\StringTok{"LS"}\NormalTok{, }\StringTok{"SYM"}\NormalTok{, }\StringTok{"PUNCT"}\NormalTok{)))}

\CommentTok{\# Lemma count, sorted (again)}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 26,164 x 2
>    lemma     n
>    <chr> <int>
>  1 the   26137
>  2 be    19466
>  3 to    13548
>  4 and   12528
>  5 of    12005
>  6 a     10461
>  7 in     8374
>  8 i      7783
>  9 that   7082
> 10 you    5276
> # i 26,154 more rows
\end{verbatim}

\end{example}

Now we are only viewing the most frequent words in the dataset, which
reduces the number of observations to around 26k. Let's now explore the
frequency distribution of the tokens. In
Figure~\ref{fig-eda-masc-count-plots}, I've created three plots which
include: 1) all the types, 2) the top 100 types, and 3) the top 10 types
in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# [ ] consider how to present the \textquotesingle{}all types\textquotesingle{} plot better, more concisely}

\CommentTok{\# Plot lemma count for all types}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(lemma, }\FunctionTok{desc}\NormalTok{(n)), }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Types"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Token frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{())}

\CommentTok{\# Plot lemma count for top 100 types}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(lemma, }\FunctionTok{desc}\NormalTok{(n)), }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Types"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Token frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{90}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{1.3}\NormalTok{))}

\CommentTok{\# Plot lemma count for top 10 types}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(lemma, }\FunctionTok{desc}\NormalTok{(n)), }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Types"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Token frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{65}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{1.3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-count-plots-1.pdf}

}

}

\subcaption{\label{fig-eda-masc-count-plots-1}All types}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-count-plots-2.pdf}

}

}

\subcaption{\label{fig-eda-masc-count-plots-2}Top 100 types}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-count-plots-3.pdf}

}

}

\subcaption{\label{fig-eda-masc-count-plots-3}Top 10 types}
\end{minipage}%

\caption{\label{fig-eda-masc-count-plots}Frequency plots of tokens in
the MASC dataset}

\end{figure}

The distributions we see in Figure~\ref{fig-eda-masc-count-plots} are
highly right-skewed (in Figure~\ref{fig-eda-masc-count-plots-1} in a
very extreme way!). This is typical of natural language distributions,
notably documented by George Kingsley Zipf
(\protect\hyperlink{ref-Zipf1949}{Zipf 1949}). This type of distribution
approaches the theoretical Zipf distribution. A Zipf (or Zipfian)
distribution is characterized by the fact that the frequency of any word
is inversely proportional to its rank in the frequency table. In other
words, the most frequent word occurs approximately twice as often as the
second most frequent word, three times as often as the third most
frequent word, and so on.

As we can see, our distribuions to not follow the Zipf distribution
exactly. This is because the Zipf distribution is a theoretical
distribution, and the actual distribution of words in a corpus is
affected by various sampling factors, including the size of the corpus.
The larger the corpus, the closer the distribution will be to the Zipf
distribution.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{medal} Dive deeper}

As stated above, Zipfian distributions are typical of natural language
and are observed a various linguistic levels. This is because natural
language is a complex system, and complex systems tend to exhibit
Zipfian distributions. Other examples of complex systems that exhibit
Zipfian distributions include the size of cities, the frequency of
species in ecological communities, the frequency of links in the World
Wide Web, \emph{etc.}

\end{tcolorbox}

The observation captured in the Zipf distribution is key to
understanding quantitative text analysis. It demonstrates that most of
the types in a corpus occur (relatively) infrequently, while a small
number of types occur very frequently. In fact, if we calculate the
cumulative frequency of the lemmas in the \texttt{masc\_tbl} data frame,
we can see that the top 10 types account for over 20\% of the lemmas
used in the dataset --by 100 types that increases to over 40\%, as seen
in Example~\ref{exm-eda-masc-count-cumulative}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-cumulative}{}\label{exm-eda-masc-count-cumulative}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate cumulative frequency}
\NormalTok{lemma\_cumul\_freq }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cumulative =} \FunctionTok{cumsum}\NormalTok{(n)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ cumulative }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n))}

\NormalTok{lemma\_cumul\_freq }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{2000}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(lemma, }\FunctionTok{desc}\NormalTok{(n)), }\AttributeTok{y =}\NormalTok{ percent)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{10}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{100}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# annotate("text", x = 10+10, y = 0.5, label = "10 lemmas") +}
  \CommentTok{\# annotate("text", x = 100+10, y = 0.5, label = "100 lemmas") +}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{percent, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Types"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Cumulative frequency percent"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.x =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-count-cumulative-1.pdf}

}

\caption{\label{fig-eda-masc-count-cumulative}Cumulative frequency of
lemmas in the MASC dataset}

\end{figure}

\end{example}

If we look at the types that appear within the first 100 most frequent,
you can likely also appreciate another thing about language use. Let's
list the top 100 types in Example~\ref{exm-eda-masc-count-top-100}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-top-100}{}\label{exm-eda-masc-count-top-100}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Top 100 types}
\NormalTok{lemma\_cumul\_freq }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-eda-masc-count-top-100}{}
\begin{table}
\caption{\label{tbl-eda-masc-count-top-100}Top 100 lemma types in the MASC dataset. }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
the & be & to & and & of & a & in & i & that & you\\
\hline
have & it & for & on & do & with & we & as & this & not\\
\hline
at & from & he & but & by & will & my & or & n't & they\\
\hline
your & an & say & what & so & his & if & 's & can & go\\
\hline
all & there & me & would & about & know & get & make & out & up\\
\hline
think & our & she & more & time & just & no & when & their & like\\
\hline
her & who & which & other & see & people & new & s & take & now\\
\hline
work & some & year & how & them & use & come & into & well & than\\
\hline
look & its & may & right & then & could & because & only & us & these\\
\hline
want & any & also & need & way & where & back & him & here & '\\
\hline
\end{tabular}
\end{table}

\end{example}

For the most part, the most frequent words are not content words, but
rather function words (\emph{e.g.} determiners, prepositions, pronouns,
auxiliary verbs). Function words include a closed class of relatively
few words that are used to express grammatical relationships between
content words. It then is no surprise that they are the comprise many of
the most frequent words in a corpus.

Another key observation is that among the most frequency content words
(\emph{e.g.} nouns, verbs, adjectives, adverbs) are words that are quite
semantically generic --that is, they are words that are used in a wide
range of contexts and take a wide range of meanings. Take for example
the adjective `good'. It can be used to describe a wide range of nouns,
such as `good food', `good people', `good times', \emph{etc}. A
sometimes near-synonym of `good', for example `good student', is the
word `studious'. Yet, `studious' is not as frequent as `good' as it is
used to describe a narrower range of nouns, such as `studious student',
`studious scholar', `studious researcher', \emph{etc}. In this way,
`studious' is more semantically specific than `good'.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

Based on what you now know about the expected distribution of words in a
corpus, what if your were asked to predict what the most frequency
English word used is in each U.S. State? What would you predict? How
confident would you be in your prediction? What if you were asked to
predict what the most frequency word used is in the language of a given
country? What would you want to know before making your prediction?

\end{tcolorbox}

So common across corpus samples, in some analyses these usual suspects
of the most common words are considered irrelvant and are filtered out.
In our ELL materials task, however, we might exclude them for this
simple fact that it will be a given that we will teach these words given
their grammatical importance. If we want to focus on the most common
content words, we can filter out the function words.

One approach to filtering out these words is to use a pre-determined
list of \textbf{stopwords}. The \texttt{tidytext} package includes a
data frame \texttt{stop\_words} of stopword lexicons for English. We can
select a lexicon from \texttt{stop\_words} and use \texttt{anti\_join()}
to filter out the words that appear in the \texttt{word} variable from
the \texttt{lemma} variable in the \texttt{masc\_tbl} data frame. In
Example~\ref{exm-eda-masc-count-stop-words}, I perform this filtering
and then re-run the frequency analysis for the top 100 lemmas.

\begin{example}[]\protect\hypertarget{exm-eda-masc-count-stop-words}{}\label{exm-eda-masc-count-stop-words}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Select stopword lexicon}
\NormalTok{stopwords }\OtherTok{\textless{}{-}} 
\NormalTok{  stop\_words }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(lexicon }\SpecialCharTok{==} \StringTok{"SMART"}\NormalTok{)}

\CommentTok{\# Filter out stop words}
\FunctionTok{anti\_join}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ masc\_tbl,}
  \AttributeTok{y =}\NormalTok{ stopwords,}
  \AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"lemma"} \OtherTok{=} \StringTok{"word"}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(lemma, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{100}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-eda-masc-count-stop-words}{}
\begin{table}
\caption{\label{tbl-eda-masc-count-stop-words}Frequency of tokens in the MASC dataset after filtering out stopwords }\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
n't & 's & make & time & people & work & year & back & ' & find\\
\hline
give & day & thing & jack & man & yeah & good & call & world & president\\
\hline
state & question & service & change & life & leave & subject & set & long & place\\
\hline
write & show & child & end & feel & hand & system & school & information & part\\
\hline
group & follow & run & support & today & point & read & provide & uh & send\\
\hline
turn & include & talk & fact & \& & live & put & word & number & start\\
\hline
law & case & company & money & great & open & home & city & issue & woman\\
\hline
job & american & important & result & book & hear & sparrow & house & problem & um\\
\hline
america & walk & family & begin & country & date & face & friend & report & move\\
\hline
order & head & id & watch & form & program & market & week & area & figure\\
\hline
\end{tabular}
\end{table}

\end{example}

The resulting list in Table~\ref{tbl-eda-masc-count-stop-words} paints a
different picture of the most frequent words in the dataset. The most
frequent words are now content words, and included in most frequent
words are more semantically specific words.

Eliminating words in this fashion, however, may not always be the best
approach. Available lists of stopwords vary in their contents and are
determined by other researchers for other potential uses. We may instead
opt to create our own stopword list that is tailored to the task, or we
may opt to use a statistical approach based on their distribution in the
dataset using a combination of frequency and dispersion measures, as we
will see in {[}the next section.{]}

For our case, however, we have another strategy to apply. Since our task
is to identify relevant vocabulary, beyond the fundamental function
words in English, we can use the part-of-speech tags to reduce our
dataset to just the content words, that is nouns, verbs, adjectives, and
adverbs. We need to consult the Penn Tagset again, to ensure we are
selecting the correct tags. I will assign this data frame to
\texttt{masc\_content\_tbl} to keep it separate from our main data frame
\texttt{masc\_tbl}, seen in Example~\ref{exm-eda-masc-filter-pos}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-filter-pos}{}\label{exm-eda-masc-filter-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Penn Tagset for content words}
\CommentTok{\# Nouns: NN, NNS,}
\CommentTok{\# Verbs: VB, VBD, VBG, VBN, VBP, VBZ}
\CommentTok{\# Adjectives: JJ, JJR, JJS}
\CommentTok{\# Adverbs: RB, RBR, RBS}

\NormalTok{content\_pos }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"NN"}\NormalTok{, }\StringTok{"NNS"}\NormalTok{, }\StringTok{"VB"}\NormalTok{, }\StringTok{"VBD"}\NormalTok{, }\StringTok{"VBG"}\NormalTok{, }\StringTok{"VBN"}\NormalTok{, }\StringTok{"VBP"}\NormalTok{, }\StringTok{"VBZ"}\NormalTok{, }\StringTok{"JJ"}\NormalTok{, }\StringTok{"JJR"}\NormalTok{, }\StringTok{"JJS"}\NormalTok{, }\StringTok{"RB"}\NormalTok{, }\StringTok{"RBR"}\NormalTok{, }\StringTok{"RBS"}\NormalTok{)}

\CommentTok{\# Select content words}
\NormalTok{masc\_content\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(pos }\SpecialCharTok{\%in\%}\NormalTok{ content\_pos)}
\end{Highlighting}
\end{Shaded}

\end{example}

We now have reduced the number of observations by 50\% focusing on the
content words. We are getting closer to identifying the vocabulary that
we want to include in our ELL materials, but we will need some more
tools to help us identify the most relevant vocabulary.

\hypertarget{sec-eda-frequency-dispersion}{%
\paragraph{Dispersion}\label{sec-eda-frequency-dispersion}}

\textbf{Dispersion} is a measure of how evenly distributed a linguistic
unit is across a dataset. This is a key concept in text analysis, as
important as frequency. It is important to recognize that frequency and
dispersion are measures of different characteristics. We can have two
words that occur with the same frequency, but one word may be more
evenly distributed across a dataset than the other. Depending on the
researcher's aims, this may be an important distinction to make. For our
task, it is likely the case that we want to capture words that are
well-dispersed across the dataset as words that have a high frequency
and a low dispersion tend to be connected to a particular context,
whether that be a particular genre, a particular speaker, a particular
topic, \emph{etc}. In other research, aim may be the reverse; to
identify words that are highly frequent and highly concentrated in a
particular context to identify words that are distinctive to that
context.

This a wide variety of measures that can be used to estimate the
distribution of types across a dataset. Let's focus on three measures:
document frequency (\(df\)), inverse document frequency (\(idf\)), and
Gries' Deviation of Proportions (\(dp\)).

The most basic measure is \textbf{document frequency} (\(df\)). This is
the number of documents in which a type appears at least once. For
example, if a type appears in 10 documents, then the document frequency
is 10. This is a very basic measure, but it is a good starting point.

A nuanced version of document frequency is \textbf{inverse document
frequency} (\(idf\)). This measure takes the total number of documents
and divides it by the document frequency. This results in a measure that
is inversely proportional to the document frequency. That is, the higher
the document frequency, the lower the inverse document frequency. This
measure is often log-transformed to spread out the values.

One thing to consider about \(df\) and \(idf\) is that niether takes
into account the length of the documents in which the type appears nor
the spread of the type within each document. To take these factors into
account, we can use Gries' Deviation of Proportions (\(dp\)) measure
(\protect\hyperlink{ref-Gries2023}{S. T. Gries 2023, 87--88}). The
\(dp\) measure is calculated as the difference between the proportion of
a tokens in a document and tokens in the corpus. The metric can be
subtracted from 1 to create a normalized measure of dispersion ranging
between 0 and 1, with lower values being more dispersed.

Let's consider how these measures differ with three scenarios:

Imagine a type with a token frequency of 100 appears in each of the 10
documents in a corpus.

A. Each of the documents is 100 words long. The type appears 10 times in
each document. B. Each of the documents is 100 words long. But now the
type appears once in 9 documents and 91 times in 1 document. C. Nine of
the documents constitute 99\% of the corpus. The type appears once in
each of the 9 documents and 91 times in the 10th document.

Scenario A is the most dispersed, scenario B is less dispersed, and
scenario C is the least dispersed. Yet, the type's \(df\) and \(idf\)
scores will be the same. But the \(dp\) score will reflect increasing
concentration of the type from A to B to C. You may wonder why we would
want to use \(df\) or \(idf\) at all. The answer is some combination of
the fact that they are computationally less expensive to calculate, they
are widely used (especially \(idf\)), and/ or in many practical
situations they often highly correlated with \(dp\).

So for our task we will use \(dp\) as our measure of dispersion. The
\texttt{qtalrkit} package includes the \texttt{calc\_type\_metrics()}
function which calculates, among other metrics, the dispersion metrics
\(df\), \(idf\), and/ or \(dp\). Let's select \texttt{dp} and assign the
result to \texttt{masc\_lemma\_disp}, as seen in
Example~\ref{exm-eda-masc-dp}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp}{}\label{exm-eda-masc-dp}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(qtalrkit)}

\CommentTok{\# Calculate deviance of proportions (DP)}
\NormalTok{masc\_lemma\_disp }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma, }
    \AttributeTok{documents =}\NormalTok{ doc\_id, }
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(dp)}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 3
>    type      n    dp
>    <chr> <dbl> <dbl>
>  1 be    19231 0.123
>  2 have   5136 0.189
>  3 not    2279 0.240
>  4 make   1149 0.266
>  5 other   882 0.270
>  6 more   1005 0.276
>  7 take    769 0.286
>  8 only    627 0.286
>  9 time    931 0.314
> 10 see     865 0.327
\end{verbatim}

\end{example}

We would like to identify lemmas that are frequent and well-dispersed.
But an important question arises, what is the threshold for frequency
and dispersion that we should use to identify the lemmas that we want to
include in our ELL materials?

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

You may be wondering why the Inverse Document Frequency is, in fact, the
inverse of the document counts, instead of just a count of the documents
that each type appears in. The \(idf\) is a very common measure in
machine learning that is used in combination with (term) frequency to
calculate the \(tf-idf\) (term frequency-inverse document frequency)
measure. That is, the product of the frequency of a term and the inverse
document frequency of the term. This serves as a weighting measure that
lowers the \(tf-idf\) score for terms that are frequent across documents
and increases the \(tf-idf\) score for terms that are infrequent across
documents.

Consider what types will end up with a high or a low \(tf-idf\) score.
What use(s) could this measure have for distinguishing between types in
a corpus?

Hint: consider the earlier discussion of stopword lists.

\end{tcolorbox}

There are statistical approaches to identifying natural breakpoints,
including clustering, but a visual inspection is often good enough for
practical purposes. Let's create a density plot to see if there is a
natural break in the distribution of our dispersion measure, as seen in
Figure~\ref{fig-eda-masc-dp-density}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp-density}{}\label{exm-eda-masc-dp-density}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Density plot of dp}
\NormalTok{masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Deviation of Proportions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-dp-density-1.pdf}

}

\caption{\label{fig-eda-masc-dp-density}Density plot of Deviation of
Proportions for lemmas in the MASC dataset}

\end{figure}

\end{example}

What we are looking for is an elbow in the distribution of dispersion
measures. In Figure~\ref{fig-eda-masc-dp-density}, we can see that there
is distinctive bend in the distribution between .85 and .97. We can
split the difference and use this as a threshold to filter out lemmas
that are less dispersed. In Example~\ref{exm-eda-masc-dp-filter}, I
filter out lemmas that have a dispersion measure less than .91. Then in
Table~\ref{tbl-eda-masc-dp-filter}, I preview the top and bottom 50
lemmas in the dataset.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dp-filter}{}\label{exm-eda-masc-dp-filter}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for lemmas with dp \textless{}= .91}
\NormalTok{masc\_lemma\_disp\_thr }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_disp }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{91}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Preview top}
\NormalTok{masc\_lemma\_disp\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\CommentTok{\# Preview bottom}
\NormalTok{masc\_lemma\_disp\_thr }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_tail}\NormalTok{(}\AttributeTok{n =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{pull}\NormalTok{(type) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{matrix}\NormalTok{(}\AttributeTok{ncol =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kable}\NormalTok{(}\AttributeTok{col.names =} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tbl-eda-masc-dp-filter}Frequency of tokens in the MASC
dataset after filtering out lemmas with a Deviation of Proportions less
than .91}\begin{minipage}[t]{\linewidth}
\subcaption{\label{tbl-eda-masc-dp-filter-1}\textbf{?(caption)}}

{\centering 

\hypertarget{tbl-eda-masc-dp-filter-1}{}
\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
be & have & do & not & n't & say & go & know & get & make\\
\hline
think & more & just & time & so & other & see & people & take & now\\
\hline
work & year & come & use & well & look & then & right & only & want\\
\hline
also & way & need & back & here & new & find & give & thing & tell\\
\hline
t & first & help & day & many & man & ask & very & much & even\\
\hline
\end{tabular}

Top 50 lemmas 

}

\end{minipage}%
\newline
\begin{minipage}[t]{\linewidth}
\subcaption{\label{tbl-eda-masc-dp-filter-2}\textbf{?(caption)}}

{\centering 

\hypertarget{tbl-eda-masc-dp-filter-2}{}
\tabularnewline

\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}
\hline
fortunately & ignorance & liability & brave & summarize & liberty & wound & nostalgic & accidentally & wax\\
\hline
dump & sting & tuition & unleash & blur & going & devote & shy & protective & faith-based\\
\hline
instrument & mainstream & awaken & prosperous & resistance & awkward & alright & proximity & preside & decidedly\\
\hline
triumph & wildly & hook & buzz & absurd & afterwards & evolutionary & sandy & rethink & resolute\\
\hline
harsh & dismiss & fetch & presume & qualify & eve & envy & interfere & strictly & evidently\\
\hline
\end{tabular}

Bottom 50 lemmas 

}

\end{minipage}%

\end{table}

\end{example}

We now have a good candidate list of common vocabulary that is spread
well across the corpus.

\hypertarget{sec-eda-frequency-relative}{%
\paragraph{Relative frequency}\label{sec-eda-frequency-relative}}

Gauging frequency and dispersion across the entire corpus is a good
starting point for any frequency analysis, but it is often the case that
we want to compare the frequency and dispersion of linguistic units
across corpora or sub-corpora.

In the case of the MASC dataset, for example, we may want to compare
metrics across the two modalities or the various genres. Simply
comparing frequency counts across these sub-corpora is not a good
approach, and can be misleading, as the sub-corpora may vary in size.
For example, if one sub-corpus is twice as large as another sub-corpus,
then, all else being equal, the frequency counts will be twice as large
in the larger sub-corpus. This is why we use relative frequency
measures, which are normalized by the size of the sub-corpus.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{lightbulb} Consider this}

A variable in the MASC dataset that has yet to be used is the
\texttt{pos} part-of-speech variable. How could we use this variable to
refine our frequency and dispersion analysis of lemma types?

Hint: consider lemma forms that may be tagged with different
parts-of-speech.

\end{tcolorbox}

To normalize the frequency of linguistic units across sub-corpora, we
can use the \textbf{relative frequency} measure. This is the frequency
of a linguistic unit divided by the total number of linguistic units in
the sub-corpus. This bakes in the size of the sub-corpus into the
measure. The notion of relative frequency is key to all research working
with text, as it is the basis for the statistical approach to text
analysis where comparisons are made.

There are some field-specific terms that are used to refer to relative
frequency measures. For example, in information retrieval and Natural
Language Processing, the relative frequency measure is often referred to
as the \textbf{term frequency}. In corpus linguistics, the relative
frequency measure is often modified slightly to include a constant
(\emph{e.g.} \(rf * 100\)) which is known as the \textbf{observed
relative frequency}. Athough the observed relative frequency per number
of tokens is not strictly necessary, it is often used to make the values
more interpretable as we can now talk about an observed relative
frequency of 1.5 as a linguistic unit that occurs 1.5 times per 100
linguistic units.

Let's consider how we might compare the frequency and dispersion of
lemmas across the two modalities in the MASC dataset, spoken and
written. To make this a bit more interesting and more relevant, let's
add the \texttt{pos} variable to our analysis. The intent, then, will be
to identify lemmas tagged with particular parts of speech that are
particularly indicative of each of the modaliites.

We can do this by collapsing the \texttt{lemma} and \texttt{pos}
variables into a single variable, \texttt{lemma\_pos}, with the
\texttt{str\_c()} function, as seen in Example~\ref{exm-eda-masc-type}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-type}{}\label{exm-eda-masc-type}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma and pos into type}
\NormalTok{masc\_content\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{))}

\CommentTok{\# Preview}
\NormalTok{masc\_content\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
>    <chr>  <chr>    <chr>      <dbl> <chr>        <chr>        <chr> <chr>       
>  1 1      Written  Letters        3 contribution contribution NN    contributio~
>  2 1      Written  Letters        7 mean         mean         VB    mean_VB     
>  3 1      Written  Letters        8 more         more         JJR   more_JJR    
>  4 1      Written  Letters       12 know         know         VB    know_VB     
>  5 1      Written  Letters       15 help         help         VB    help_VB     
>  6 1      Written  Letters       17 see          see          VB    see_VB      
>  7 1      Written  Letters       19 much         much         JJ    much_JJ     
>  8 1      Written  Letters       21 contribution contribution NN    contributio~
>  9 1      Written  Letters       22 means        mean         VBZ   mean_VBZ    
> 10 1      Written  Letters       25 'm           be           VBP   be_VBP
\end{verbatim}

\end{example}

Now this will increase the number of lemma types in the dataset as we
are now considering lemmas where the same lemma form is tagged with
different parts-of-speech.

Getting back to calculating the frequency and dispersion of lemmas in
each modality, we can use the \texttt{calc\_type\_metrics()} function
with \texttt{lemma\_pos} as our type argument. We will, however, need to
apply this function to each sub-corpus independently and then
concatenate the two data frames. This function returns a (raw) frequency
measure by default, but we can specify the\texttt{frequency} argument to
\texttt{rf} to calculate the relative frequency of the linguistic units
as in Example~\ref{exm-eda-masc-metrics-modality}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-metrics-modality}{}\label{exm-eda-masc-metrics-modality}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate relative frequency}
\CommentTok{\# Spoken}
\NormalTok{masc\_spoken\_metrics }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Spoken"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma\_pos, }
    \AttributeTok{documents =}\NormalTok{ doc\_id, }
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{modality =} \StringTok{"Spoken"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Written }
\NormalTok{masc\_written\_metrics }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_content\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(modality }\SpecialCharTok{==} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ lemma\_pos, }
    \AttributeTok{documents =}\NormalTok{ doc\_id, }
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{modality =} \StringTok{"Written"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))}

\CommentTok{\# Concatenate spoken and written metrics}
\NormalTok{masc\_metrics }\OtherTok{\textless{}{-}}
  \FunctionTok{bind\_rows}\NormalTok{(masc\_spoken\_metrics, masc\_written\_metrics)}

\CommentTok{\# Preview}
\NormalTok{masc\_metrics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 5
>    type         n      rf     dp modality
>    <chr>    <dbl>   <dbl>  <dbl> <chr>   
>  1 be_VBZ    2612 0.0489  0.0842 Spoken  
>  2 be_VBP    1282 0.0240  0.111  Spoken  
>  3 be_VBD    1020 0.0191  0.301  Spoken  
>  4 n't_RB     829 0.0155  0.139  Spoken  
>  5 have_VBP   766 0.0143  0.152  Spoken  
>  6 do_VBP     728 0.0136  0.180  Spoken  
>  7 be_VB      655 0.0123  0.147  Spoken  
>  8 not_RB     638 0.0119  0.137  Spoken  
>  9 just_RB    404 0.00756 0.267  Spoken  
> 10 so_RB      387 0.00725 0.357  Spoken
\end{verbatim}

\end{example}

With the \texttt{rf} measure, we are now in a position to compare
`apples to apples', as you might say. We can now compare the relative
frequency of lemmas across the two modalities. Let's preview the top 10
lemmas in each modality, as seen in
Example~\ref{exm-eda-masc-relative-frequency-top}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-relative-frequency-top}{}\label{exm-eda-masc-relative-frequency-top}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview top 10 lemmas in each modality}
\NormalTok{masc\_metrics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(modality) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ rf) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 20 x 5
>    type         n      rf     dp modality
>    <chr>    <dbl>   <dbl>  <dbl> <chr>   
>  1 be_VBZ    2612 0.0489  0.0842 Spoken  
>  2 be_VBP    1282 0.0240  0.111  Spoken  
>  3 be_VBD    1020 0.0191  0.301  Spoken  
>  4 n't_RB     829 0.0155  0.139  Spoken  
>  5 have_VBP   766 0.0143  0.152  Spoken  
>  6 do_VBP     728 0.0136  0.180  Spoken  
>  7 be_VB      655 0.0123  0.147  Spoken  
>  8 not_RB     638 0.0119  0.137  Spoken  
>  9 just_RB    404 0.00756 0.267  Spoken  
> 10 so_RB      387 0.00725 0.357  Spoken  
> 11 be_VBZ    4745 0.0248  0.230  Written 
> 12 be_VBD    3317 0.0173  0.366  Written 
> 13 be_VBP    2617 0.0137  0.237  Written 
> 14 be_VB     1863 0.00974 0.218  Written 
> 15 not_RB    1640 0.00858 0.259  Written 
> 16 have_VBP  1227 0.00642 0.290  Written 
> 17 n't_RB     905 0.00473 0.540  Written 
> 18 have_VBD   859 0.00449 0.446  Written 
> 19 have_VBZ   777 0.00406 0.335  Written 
> 20 say_VBD    710 0.00371 0.609  Written
\end{verbatim}

\end{example}

We can appreciate, now, that there are similarities and a few
differences between the most frequent lemmas for each modality. First,
there are similar lemmas in written and spoken modalities, such as `be',
`have', and `not'. Second, the top 10 include verbs and adverbs. Now we
are looking at the most frequent types, so it is not surprising that we
see more in common than not. However, looking close we can see that
contracted forms are more frequent in the spoken modality, such as
`isn't', `don't', and `can't' and that ordering of the verb tenses
differs to some degree. Whether these are important distinctions for our
task is something we will need to consider.

We can further cull our results by filtering out lemmas that are not
well-dispersed across the sub-corpora. Although it may be tempting to
use the threshold we used earlier, we should consider that the size of
the sub-corpora are different and the distribution of the dispersion
measure may be different. With this in mind, we need to visualize the
distribution of the dispersion measure for each modality, as seen in
Figure~\ref{fig-eda-masc-dispersion-threshold}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Density plot of dp by modality}
\NormalTok{masc\_metrics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Deviation of Proportions"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ modality, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{, }\AttributeTok{scales =} \StringTok{"free\_x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-dispersion-threshold-1.pdf}

}

\caption{\label{fig-eda-masc-dispersion-threshold}Density plot of
Deviation of Proportions for lemmas in the MASC dataset by modality}

\end{figure}

As expected, the density plots point to different thresholds for each
modality.The written subcorpus follows closely with the previous
distribution, but the spoken subcorpus has more than one bend in the
distribution. Why are there multiple peaks in the density plot? It
points to some level of inconsistency in the spoken data, either
potentially some level of context-dependent language use (genres,
topics, speech styles) or it could be due to the fact that the spoken
subcorpus' size is too small to provide a reliable distribution.

In any case, we can estimate the threshold for the spoken corpus making
use of the largest peak in the distribution as the reference point. With
this approach in mind, lets maintain the \(.91\) threshold for the
written subcorpus and use a \(.79\) threshold for the spoken subcorpus.
Let's filter out lemmas that have a dispersion measure less than .91 for
the written subcorpus and less than .79 for the spoken subcorpus, as
seen in Example~\ref{exm-eda-masc-subcorpora-filtered}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-subcorpora-filtered}{}\label{exm-eda-masc-subcorpora-filtered}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for lemmas with}
\CommentTok{\# dp \textless{}= .91 for written and }
\CommentTok{\# dp \textless{}= .79 for spoken}
\NormalTok{masc\_metrics\_thr }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_metrics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}
\NormalTok{    (modality }\SpecialCharTok{==} \StringTok{"Written"} \SpecialCharTok{\&}\NormalTok{ dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{91}\NormalTok{) }\SpecialCharTok{|} 
\NormalTok{    (modality }\SpecialCharTok{==} \StringTok{"Spoken"} \SpecialCharTok{\&}\NormalTok{ dp }\SpecialCharTok{\textless{}=}\NormalTok{ .}\DecValTok{79}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(rf))}
\end{Highlighting}
\end{Shaded}

\end{example}

Filtering the less-dispersed types reduces the dataset from 33637 to
4860 observations. This will provide us with a more succinct list of
common and well-dispersed lemmas that are used in each modality.

As much as the frequency and dispersion measures can provide us with a
good starting point, it does not provide an understanding of what types
are more indicative of a particular sub-corpus, modality subcorpora in
our case. We can do this by calculating the log odds ratio of each lemma
in each modality.

The \textbf{log odds ratio} is a measure that quantifies the difference
between the frequencies of a type in two corpora or sub-corpora. In
spirit and in name, it compares the odds of a type occurring in one
corpus versus the other. The values range from negative to positive
infinity, with negative values indicating that the type is more frequent
in the first corpus and positive values indicating that the lemma is
more frequent in the second corpus. The magnitude of the value indicates
the strength of the association.

The \texttt{tidylo} package provides a convenient function
\texttt{bind\_log\_odds()} to calculate the log odds ratio, and a
weighed variant, for each type in each sub-corpus. Let's use this
function to calculate the log odds ratio for each lemma in each
modality, as seen in Example~\ref{exm-eda-masc-log-odds}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-log-odds}{}\label{exm-eda-masc-log-odds}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidylo)}

\CommentTok{\# Calculate log odds ratio}
\NormalTok{masc\_metrics\_thr }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{bind\_log\_odds}\NormalTok{(}
    \AttributeTok{set =}\NormalTok{ modality,}
    \AttributeTok{feature =}\NormalTok{ type,}
    \AttributeTok{n =}\NormalTok{ n, }
    \AttributeTok{unweighted =} \ConstantTok{TRUE}
\NormalTok{  )}

\CommentTok{\# Preview (ordered by log\_odds)}
\CommentTok{\# Spoken}
\NormalTok{masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ log\_odds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 7
>    type                  n       rf    dp modality log_odds log_odds_weighted
>    <chr>             <dbl>    <dbl> <dbl> <chr>       <dbl>             <dbl>
>  1 understanding_NN     45 0.000843 0.649 Spoken      0.955              6.41
>  2 meeting_NNS          42 0.000786 0.702 Spoken      0.955              6.19
>  3 testimony_NN         36 0.000674 0.785 Spoken      0.955              5.73
>  4 administration_NN    34 0.000637 0.650 Spoken      0.955              5.57
>  5 trial_NN             33 0.000618 0.769 Spoken      0.955              5.49
>  6 governor_NN          28 0.000524 0.597 Spoken      0.955              5.05
>  7 intelligent_JJ       28 0.000524 0.777 Spoken      0.955              5.05
>  8 faith_NN             27 0.000506 0.524 Spoken      0.955              4.96
>  9 walk_VBZ             27 0.000506 0.761 Spoken      0.955              4.96
> 10 gun_NNS              26 0.000487 0.577 Spoken      0.955              4.87
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Written}
\NormalTok{masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_min}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ log\_odds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 7
>    type             n        rf    dp modality log_odds log_odds_weighted
>    <chr>        <dbl>     <dbl> <dbl> <chr>       <dbl>             <dbl>
>  1 correct_JJ      13 0.0000680 0.900 Written    -0.461             -3.75
>  2 mean_VBP        38 0.000199  0.776 Written    -0.411             -5.00
>  3 president_NN    37 0.000194  0.840 Written    -0.406             -4.81
>  4 board_NN        27 0.000141  0.830 Written    -0.405             -4.10
>  5 argument_NN     24 0.000126  0.798 Written    -0.356             -3.07
>  6 meeting_NN      44 0.000230  0.852 Written    -0.343             -3.90
>  7 question_NN     80 0.000418  0.630 Written    -0.330             -4.94
>  8 say_VBN         20 0.000105  0.767 Written    -0.325             -2.42
>  9 read_VBN        10 0.0000523 0.895 Written    -0.325             -1.71
> 10 cut_NN           9 0.0000471 0.879 Written    -0.325             -1.62
\end{verbatim}

\end{example}

\begin{itemize}
\tightlist
\item[$\square$]
  not clear to me what the log\_odds are doing. I thought they would run
  from 1 to -1 with the more negative, the more indicative of writing
  (as it is alphabetically second)
\end{itemize}

{[}description of the results\ldots{]}

The second measure produced by \texttt{bind\_log\_odds()} function, is
the weighted log odds ratio. This measure provides a more robust and
interpretable measure for comparing term frequencies across corpora,
especially when term frequencies are low or when corpora are of
different sizes. The weighting (or standardization) also makes it easier
to identify terms that are particularly distinctive or characteristic of
one corpus over another. Note that the weighted measure's interpretation
is slightly different that the log odds's. The larger positive values in
each corpus indicate that the type is more indicative of that
(sub-)corpus, and the larger negative values indicate that the type is
less indicative.

Let's imagine we would like to extract the most indicative verbs for
each modality using the weighted log odds as our measure. We can do this
with a little regular expression magic. Let's use the
\texttt{str\_subset()} function to filter for lemmas that start with `V'
and then use \texttt{slice\_max()} to extract the top 10 most indicative
verb lemmas, as seen in
Example~\ref{exm-eda-masc-log-odds-weighted-verbs}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-log-odds-weighted-verbs}{}\label{exm-eda-masc-log-odds-weighted-verbs}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview (ordered by log\_odds\_weighted)}
\CommentTok{\# Spoken and written}
\NormalTok{masc\_metrics\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(modality) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(type, }\StringTok{"\_V"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_max}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{order\_by =}\NormalTok{ log\_odds\_weighted) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n, }\SpecialCharTok{{-}}\NormalTok{log\_odds) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 20 x 5
>    type                rf     dp modality log_odds_weighted
>    <chr>            <dbl>  <dbl> <chr>                <dbl>
>  1 be_VBZ        0.0489   0.0842 Spoken               14.0 
>  2 do_VBP        0.0136   0.180  Spoken               10.3 
>  3 be_VBP        0.0240   0.111  Spoken                8.66
>  4 think_VBP     0.00655  0.259  Spoken                8.32
>  5 have_VBP      0.0143   0.152  Spoken                8.00
>  6 know_VBP      0.00528  0.260  Spoken                7.03
>  7 go_VBG        0.00534  0.207  Spoken                6.47
>  8 do_VBD        0.00603  0.321  Spoken                5.97
>  9 mean_VBP      0.00247  0.543  Spoken                5.94
> 10 do_VB         0.00455  0.207  Spoken                5.61
> 11 don_VB        0.000361 0.839  Written               4.04
> 12 doe_VBZ       0.000350 0.871  Written               3.98
> 13 walk_VBD      0.000319 0.790  Written               3.80
> 14 associate_VBN 0.000303 0.777  Written               3.70
> 15 reply_VBD     0.000293 0.838  Written               3.64
> 16 develop_VBG   0.000288 0.812  Written               3.60
> 17 require_VBN   0.000272 0.793  Written               3.50
> 18 fall_VBD      0.000267 0.757  Written               3.47
> 19 meet_VB       0.000241 0.729  Written               3.30
> 20 regard_VBG    0.000225 0.823  Written               3.19
\end{verbatim}

\end{example}

Note that the log odds are larger for the spoken modality than the
written modality. This indicates that theses types are more strongly
indicative of the spoken modality than the types in the written modality
are indicative of the written modality. This is not surprising, as the
written modality is typically more diverse in terms of lexical usage
than the spoken modality, where the terms tend to be repeated more
often, including verbs.

\hypertarget{sec-eda-co-occurrence}{%
\subsubsection{Co-occurrence analysis}\label{sec-eda-co-occurrence}}

Moving forward on our task, we have a good idea of the general
vocabulary that we want to include in our ELL materials and can identify
lemma types that are particularly indicative of each modality. Another
useful approach to complement our analysis is to identify words that
co-occur with our target lemmas --in particular verbs. In English it is
common for verbs to appear with a preposition or adverb, such as `give
up', `look after'. These `phrasal verbs' form a semantic unit that is
distinct from the verb alone.

In cases such as this, we are aiming to do a co-occurrence analysis.
Co-occurrence analysis is a set of methods that are used to identify
words that appear in close proximity to a target type.

An exploratory, primarily qualitative, approach is to display the
co-occurrence of words in a Keyword in Context (KWIC). This is a table
that displays the target word in the center of the table and the words
that appear before and after the target word. This is a useful approach
for spot identifying collocations of a target word or phrase.

The \texttt{quanteda} package includes a function \texttt{kwic()} that
can be used to create a KWIC table. It does require some transformation
to the data, however. We need to collapse the \texttt{lemma} column into
a single string for each document from the original transformed dataset,
\texttt{masc\_tbl}. Then we can apply the \texttt{corpus()} and then
\texttt{tokens} function to create a quanteda tokens object. Then we can
apply the \texttt{kwic()} function to create a KWIC table.

\begin{example}[]\protect\hypertarget{exm-eda-masc-kwic}{}\label{exm-eda-masc-kwic}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma, pos into a single string}
\NormalTok{masc\_text\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(doc\_id, modality, genre) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{text =} \FunctionTok{str\_c}\NormalTok{(lemma\_pos, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(quanteda)}

\NormalTok{masc\_corpus }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_text\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{corpus}\NormalTok{(}
    \AttributeTok{text\_field =} \StringTok{"text"}\NormalTok{,}
    \AttributeTok{docid\_field =} \StringTok{"doc\_id"}
\NormalTok{  )}

\FunctionTok{summary}\NormalTok{(masc\_corpus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Corpus consisting of 392 documents, showing 100 documents:
> 
>  Text Types Tokens Sentences modality        genre
>     1   251    528         2  Written      Letters
>    10   168    343         1  Written        Email
>   100    47     50         1  Written        Email
>   101   127    210         1  Written        Email
>   102   195    351         1  Written        Email
>   103   102    124         1  Written        Email
>   104   123    166         2  Written        Email
>   105   163    296         5  Written        Email
>   106    52     55         1  Written        Email
>   107   120    181         3  Written        Email
>   108   153    259         1  Written        Email
>   109   107    138         3  Written        Email
>    11   217    407         1  Written        Email
>   110    47     54         4  Written        Email
>   111   182    287         1  Written        Email
>   112  2452  16668        20   Spoken   Transcript
>   113  2449  16198         9   Spoken   Transcript
>   114   178    306         1  Written        Email
>   115   139    220         1  Written        Email
>   116   102    134         1  Written        Email
>   117   147    247         1  Written        Email
>   118    54     64         1  Written        Email
>   119   197    365         1  Written        Email
>    12   326    746         1  Written        Email
>   120   172    291         1  Written        Email
>   121   234    479         1  Written        Email
>   122    97    149         1  Written        Email
>   123   184    349         1  Written        Email
>   124   186    348         1  Written        Email
>   125   174    290         1  Written        Email
>   126   314    651         5  Written        Email
>   127    76     99         1  Written        Email
>   128   209    330         6  Written      Letters
>   129   172    287         2  Written        Email
>    13   183    330         2  Written        Email
>   130   145    204         1  Written        Email
>   131   150    219         1  Written        Email
>   132   175    267         3  Written        Email
>   133    82     99         1  Written        Email
>   134   116    190         1  Written    Newspaper
>   135   135    231         2  Written    Newspaper
>   136   266    558         1  Written      Letters
>   137  1490   4991         3  Written      Journal
>   138   330    675         2  Written         Blog
>   139   355    793         4  Written         Blog
>    14   204    358         2  Written        Email
>   140  1542   7188        50  Written        Essay
>   141   409    858         2  Written         Blog
>   142   131    188         3  Written        Email
>   143   363    826         3  Written      Journal
>   144   270    533         2  Written      Journal
>   145   356    723         2  Written      Journal
>   146   412    735         4  Written      Journal
>   147   428    752         1  Written      Journal
>   148   359    673         4  Written      Journal
>   149   447    913         1  Written      Journal
>    15   112    160         1  Written        Email
>   150   413    974         1  Written      Journal
>   151   980   4121         8  Written        Essay
>   152  1031   6961         8   Spoken Face-to-face
>   153   934   2968        16  Written        Essay
>   154  1181   7128        58   Spoken Face-to-face
>   155  1363   6010        20  Written  Non-fiction
>   156  1616   5582        49  Written  Non-fiction
>   157   141    222         1  Written        Email
>   158  2334  21868        35   Spoken   Transcript
>   159    70     80         1  Written        Email
>    16   184    288         2  Written        Email
>   160    54     78         1  Written    Newspaper
>   161   126    175         1  Written        Email
>   162  1457   5532         5  Written         Blog
>   163  1254   4823        27  Written   Government
>   164   352    743         8  Written        Email
>   165   347    671         2  Written        Email
>   166   821   2365         1  Written         Blog
>   167   908   2445        24  Written         Blog
>   168   222    409         2  Written        Email
>   169   981   2761        23  Written Travel Guide
>    17   102    151         1  Written        Email
>   170  1004   2687        17  Written Travel Guide
>   171  1116   2746         6  Written Travel Guide
>   172   855   2601         9  Written        Essay
>   173   324    724         1  Written         Blog
>   174   302    584         1  Written         Blog
>   175   361    824         1  Written      Letters
>   176   541   1165         3  Written Travel Guide
>   177   428    844         1  Written Travel Guide
>   178   452    994         2  Written         Blog
>   179   443   1191        11   Spoken Movie Script
>    18   104    141         1  Written        Email
>   180   280    563         7   Spoken Movie Script
>   181   656   1880        18   Spoken Movie Script
>   182    82    132         1   Spoken Movie Script
>   183   795   3047        67   Spoken Movie Script
>   184   538   2092         3  Written   Government
>   185   292    576         1  Written        Email
>   186  1454   9317        12   Spoken   Transcript
>   187   667   1732        15  Written        Essay
>   188    66    101         1  Written        Email
>   189   379    863         2  Written      Letters
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_corpus }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tokens}\NormalTok{() }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kwic}\NormalTok{(}
    \AttributeTok{pattern =} \FunctionTok{phrase}\NormalTok{(}\StringTok{"*\_V* *\_IN*"}\NormalTok{),}
    \AttributeTok{window =} \DecValTok{3}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> Keyword-in-context with 10 matches.                                                                            
>      [231, 536:537]     the_DT bio-sphere_JJ there_RB |    be_VBZ in_IN    |
>    [169, 2220:2221]        ataturk_NNP rise_VBD to_TO |   power_VB on_IN   |
>      [219, 648:649]       1980s_NN warren_NNP this_DT |  result_VBD in_IN  |
>    [183, 1183:1184]           well_RB since_IN it_PRP |  look_VBZ like_IN  |
>      [332, 745:746]                    . _NN when_WRB | register_VBG by_IN |
>      [150, 699:700]        allow_VBP doctor_NNS to_TO |  decide_VB for_IN  |
>        [5, 111:112]             ' s_POS counselor_NNS |  feel_VBD that_IN  |
>      [266, 680:681]       transfer_NN agent_NNS to_TO |   stay_VB out_IN   |
>  [158, 16560:16561] censorship_NN process_NN that_WDT |   go_VBZ into_IN   |
>      [181, 456:457]          and_CC be_VBZ clearly_RB | recline_VBG in_IN  |
>                                       
>  this_DT whole_JJ self-constructing_JJ
>  a_DT wave_NN of_IN                   
>  persecution_NN against_IN the_DT     
>  i_PRP be_VBP already_RB              
>  phone_NN please_VB refer_VBP         
>  themselves_PRP if_IN the_DT          
>  his_PRP $ personable_JJ              
>  of_IN it_PRP that_DT                 
>  put_VBG book_NNS in_IN               
>  a_DT hospital_NN bed_NN
\end{verbatim}

\end{example}

A straightforward quantitative way to explore co-occurrence is to set
the unit of observation to an \(n-gram\) of terms. An \(n-gram\) is a
sequence of \(n\) words. For example, a 2-gram is a sequence of two
words, a 3-gram is a sequence of three words, and so on. Then, the
frequency and dispersion metrics can be calculated for each \(n-gram\).

In general, deriving \(n-grams\) from a corpus is a straightforward
process. The \texttt{tidytext} package includes a function
\texttt{unnest\_tokens()} that can be used to create \(n-grams\) from a
corpus. The function can take a single column of untokenized text or a
tokenized column in combination with a variable to use as the grouping
variable. In the \texttt{masc\_tbl} dataset, we have tokenized text in
the \texttt{lemma} column and a grouping variable in the
\texttt{doc\_id} column. We can use the \texttt{unnest\_tokens()}
function to create a new data frame with a row for each \(n-gram\) in
each document, as seen in Example~\ref{exm-eda-masc-bigrams-tidytext}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-tidytext}{}\label{exm-eda-masc-bigrams-tidytext}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Create bigrams}
\NormalTok{masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ bigrams, }
    \AttributeTok{input =}\NormalTok{ lemma, }
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }
    \AttributeTok{n =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{collapse =} \StringTok{"doc\_id"}
\NormalTok{    ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 2
>    doc_id bigrams          
>    <chr>  <chr>            
>  1 1      december your    
>  2 1      your contribution
>  3 1      contribution to  
>  4 1      to goodwill      
>  5 1      goodwill will    
>  6 1      will mean        
>  7 1      mean more        
>  8 1      more than        
>  9 1      than you         
> 10 1      you may
\end{verbatim}

\end{example}

The result of this operation can be joined with the original dataset
using the \texttt{doc\_id} as the key variable. Then we can calculate
the frequency and dispersion metrics for each \(n-gram\) in each
modality. However, this approach is not ideal for our task. The reason
is that we are interested in identifying \(n-grams\) that include verbs.
The \texttt{unnest\_tokens()} function does not allow us to filter the
\(n-grams\) by part-of-speech.

Another, more informative approach is to create a new variable that
combines the lemma and part-of-speech for each observation before using
\texttt{unnest\_tokens()} to generate the bigrams. We can use the
\texttt{str\_c()} function from the \texttt{stringr} package to join the
\texttt{lemma} and \texttt{pos} columns into a single string, so that we
have a variable \texttt{lemma\_pos} with the lemma and part-of-speech
joined by an underscore.

One consideration that we need to take for our goal to identify verb
particle constructions, is how we ultimately want to group our
\texttt{lemma\_pos} values. This is particularly important given the
fact that our \texttt{pos} tags for verbs include information about the
verb's tense and person. This means that a verb in a verb particle
bigram, such as `look after', will be represented by multiple
\texttt{lemma\_pos} values, such as `look\_VB', `look\_VBP',
`look\_VBD', and `look\_VBG'. If we want this level of detail, we just
proceed as described above. However, if we want to group the verb
particle bigrams by a single verb value, we need to recode the
\texttt{pos} values for verbs. We can do this with the
\texttt{case\_match()} function from the \texttt{dplyr} package.

In Example~\ref{exm-eda-masc-lemma-pos}, I recode the \texttt{pos}
values for verbs to `V' and then join the \texttt{lemma} and
\texttt{pos} columns into a single string.

\begin{example}[]\protect\hypertarget{exm-eda-masc-lemma-pos}{}\label{exm-eda-masc-lemma-pos}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Collapse lemma into a single string}
\NormalTok{masc\_lemma\_pos\_tbl }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pos =} \FunctionTok{case\_when}\NormalTok{(}
    \FunctionTok{str\_detect}\NormalTok{(pos, }\StringTok{"\^{}V"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"V"}\NormalTok{,}
    \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}}\NormalTok{ pos}
\NormalTok{  )) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(doc\_id) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{lemma\_pos =} \FunctionTok{str\_c}\NormalTok{(lemma, pos, }\AttributeTok{sep =} \StringTok{"\_"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{()}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 8
>    doc_id modality genre   term_num term         lemma        pos   lemma_pos   
>    <chr>  <chr>    <chr>      <dbl> <chr>        <chr>        <chr> <chr>       
>  1 1      Written  Letters        0 December     december     NNP   december_NNP
>  2 1      Written  Letters        2 Your         your         PRP$  your_PRP$   
>  3 1      Written  Letters        3 contribution contribution NN    contributio~
>  4 1      Written  Letters        4 to           to           TO    to_TO       
>  5 1      Written  Letters        5 Goodwill     goodwill     NNP   goodwill_NNP
>  6 1      Written  Letters        6 will         will         MD    will_MD     
>  7 1      Written  Letters        7 mean         mean         V     mean_V      
>  8 1      Written  Letters        8 more         more         JJR   more_JJR    
>  9 1      Written  Letters        9 than         than         IN    than_IN     
> 10 1      Written  Letters       10 you          you          PRP   you_PRP
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{unnest\_tokens}\NormalTok{(}
    \AttributeTok{output =}\NormalTok{ bigrams, }
    \AttributeTok{input =}\NormalTok{ lemma\_pos, }
    \AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }
    \AttributeTok{n =} \DecValTok{2}\NormalTok{, }
    \AttributeTok{to\_lower =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{collapse =} \StringTok{"doc\_id"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(bigrams, }\StringTok{"\_V.*\_IN"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{calc\_type\_metrics}\NormalTok{(}
    \AttributeTok{type =}\NormalTok{ bigrams, }
    \AttributeTok{documents =}\NormalTok{ doc\_id, }
    \AttributeTok{frequency =} \StringTok{"rf"}\NormalTok{,}
    \AttributeTok{dispersion =} \StringTok{"dp"}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(rf))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 4,512 x 4
>    type                n      rf    dp
>    <chr>           <dbl>   <dbl> <dbl>
>  1 be_V in_IN        359 0.0259  0.366
>  2 be_V that_IN      210 0.0151  0.504
>  3 look_V at_IN      157 0.0113  0.528
>  4 say_V that_IN     134 0.00966 0.580
>  5 be_V on_IN        120 0.00865 0.523
>  6 talk_V about_IN   114 0.00821 0.622
>  7 be_V of_IN        110 0.00793 0.503
>  8 know_V that_IN     98 0.00706 0.605
>  9 think_V that_IN    90 0.00649 0.643
> 10 be_V about_IN      76 0.00548 0.608
> # i 4,502 more rows
\end{verbatim}

\end{example}

We have identified and derived frequency and dispersion metrics for
\(n-grams\) that include verb particle construction candidates. Yet,
there is a problem with this approach. The problem is that the
\(n-grams\) are not necessarily verb particle constructions in the sense
that they form a semantic unit. Second, frequency and dispersion metrics
are not necessarily the best measures for identifying the co-occurrence
relationship between the verb and the particle. In other words, just
because a two-word sequence is frequent and well-dispersed does not mean
that the two words form a semantic unit.

To address these issues, we can use a statistical measures to estimate
collocational strength between two words. A \textbf{collocation} is a
sequence of words that co-occur more often than would be expected by
chance. The most common measure of collocation is the \textbf{pointwise
mutual information} (PMI) measure. The PMI measure is calculated as the
log ratio of the observed frequency of two words co-occurring to the
expected frequency of the two words co-occurring. The expected frequency
is calculated as the product of the frequency of each word. The PMI
measure is a log ratio, so the values range from negative to positive
infinity, with negative values indicating that the two words co-occur
less often than would be expected by chance and positive values
indicating that the two words co-occur more often than would be expected
by chance. The magnitude of the value indicates the strength of the
association.

Let's calculate the PMI for all the bigrams in the MASC dataset. We can
use the \texttt{calc\_assoc\_metrics()} function from \texttt{qtalrkit}.
We need to specify the \texttt{association} argument to \texttt{pmi} and
the \texttt{type} argument to \texttt{bigrams}, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi}{}\label{exm-eda-masc-bigrams-pmi}

~

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_pos\_assoc }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pos\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{calc\_assoc\_metrics}\NormalTok{(}
    \AttributeTok{doc\_index =}\NormalTok{ doc\_id, }
    \AttributeTok{token\_index =}\NormalTok{ term\_num, }
    \AttributeTok{type =}\NormalTok{ lemma\_pos, }
    \AttributeTok{association =} \StringTok{"pmi"}
\NormalTok{  )}

\CommentTok{\# Preview }
\NormalTok{masc\_lemma\_pos\_assoc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pmi)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x               y                   n   pmi
>    <chr>           <chr>           <dbl> <dbl>
>  1 #Christian_NN   bigot_NN            1  12.4
>  2 #FAIL_NN        phenomenally_RB     1  12.4
>  3 #NASCAR_NN      #indycar_NN         1  12.4
>  4 #PALM_NN        merchan_NN          1  12.4
>  5 #Twitter_NN     #growth_NN          1  12.4
>  6 #college_NN     #jobs_NN            1  12.4
>  7 #education_NN   #teaching_NN        1  12.4
>  8 #faculty_NN     #cites_NN           1  12.4
>  9 #fb_NN          siebel_NNP          1  12.4
> 10 #glitchmyass_NN reps_NNP            1  12.4
\end{verbatim}

\end{example}

One caveat to using the PMI measure is that it is sensitive to the
frequency of the words. If the words in a bigram pair are infrequent,
and especially if they only occur once, then the PMI measure will be
inflated. To mitigate this issue, we can apply a frequency threshold to
the bigrams before calculating the PMI measure. Let's filter out bigrams
that occur less than 10 times, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi-filtered}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi-filtered}{}\label{exm-eda-masc-bigrams-pmi-filtered}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for bigrams that occur \textgreater{}= 10 times}
\NormalTok{masc\_lemma\_pos\_assoc\_thr }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pos\_assoc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(n }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pmi))}

\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_pos\_assoc\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x             y                n   pmi
>    <chr>         <chr>        <dbl> <dbl>
>  1 pianista_NN   irlandesa_NN    10 10.0 
>  2 costa_NNP     rica_NNP        10  9.95
>  3 nanowrimo_NNP novel_NNP       12  9.87
>  4 bin_NN        laden_NNP       11  9.79
>  5 osama_NNP     bin_NN          11  9.79
>  6 bin_NNP       ladin_NNP       11  9.70
>  7 los_NNP       angeles_NNP     11  9.64
>  8 chilean_JJ    seabass_NNS     13  9.64
>  9 novel_NNP     ch_NNP          12  9.58
> 10 st_NNP        zip_NNP         10  9.52
\end{verbatim}

\end{example}

Now we are in a position to identify verb particle constructions. We can
filter for bigrams that include a verb and a particle and that have a
PMI measure greater than 0, as seen in
Example~\ref{exm-eda-masc-bigrams-pmi-filtered-vpc}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-bigrams-pmi-filtered-vpc}{}\label{exm-eda-masc-bigrams-pmi-filtered-vpc}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter for bigrams that include a verb and a particle}
\CommentTok{\# and that have a PMI measure greater than 0}
\NormalTok{masc\_verb\_part\_assoc }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pos\_assoc\_thr }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(x, }\StringTok{"\_V"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(y, }\StringTok{"\_IN"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(pmi }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(x)}

\CommentTok{\# Preview }
\NormalTok{masc\_verb\_part\_assoc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> # A tibble: 10 x 4
>    x             y           n   pmi
>    <chr>         <chr>   <dbl> <dbl>
>  1 account_V     for_IN     17  3.74
>  2 acknowledge_V that_IN    13  3.62
>  3 act_V         as_IN      14  2.96
>  4 agree_V       with_IN    45  3.21
>  5 agree_V       that_IN    14  1.94
>  6 appear_V      on_IN      12  1.98
>  7 appear_V      in_IN      24  1.76
>  8 argue_V       that_IN    20  3.26
>  9 arrive_V      at_IN      18  3.39
> 10 arrive_V      in_IN      10  1.48
\end{verbatim}

\end{example}

We can clean up the results a bit by removing the part-of-speech tags
from the \texttt{x} and \texttt{y} variables, up our minimum PMI value,
and create a network plot to visualize the results, as seen in
Figure~\ref{fig-eda-masc-verb-part-network}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Clean up results}
\NormalTok{masc\_verb\_part\_assoc\_plot }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_verb\_part\_assoc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(pmi }\SpecialCharTok{\textgreater{}=} \DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{x =} \FunctionTok{str\_remove}\NormalTok{(x, }\StringTok{"\_V.*"}\NormalTok{),}
    \AttributeTok{y =} \FunctionTok{str\_remove}\NormalTok{(y, }\StringTok{"\_IN"}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Create an association network plot}
\CommentTok{\# \textasciigrave{}x\textasciigrave{} and \textasciigrave{}y\textasciigrave{} are the nodes}
\CommentTok{\# \textasciigrave{}pmi\textasciigrave{} is the edge weight}

\FunctionTok{library}\NormalTok{(igraph)}
\FunctionTok{library}\NormalTok{(ggraph)}

\NormalTok{masc\_verb\_part\_assoc\_plot }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{graph\_from\_data\_frame}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggraph}\NormalTok{(}\AttributeTok{layout =} \StringTok{"nicely"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ pmi),}
    \AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{,}
    \AttributeTok{edge\_width =} \FloatTok{0.8}\NormalTok{,}
    \AttributeTok{arrow =}\NormalTok{ grid}\SpecialCharTok{::}\FunctionTok{arrow}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_edge\_color\_gradient}\NormalTok{(}\AttributeTok{low =} \StringTok{"grey90"}\NormalTok{, }\AttributeTok{high =} \StringTok{"grey20"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-verb-part-network-1.pdf}

}

\caption{\label{fig-eda-masc-verb-part-network}Network plot of verb
particle constructions in the MASC dataset}

\end{figure}

From this plot, and from the underlying data, we can explore verb
particle constructions. We could go further and apply our co-occurrence
methods to each modality separately, if we wanted to identify verb
particle constructions that are distinctive to each modality. We could
also apply our co-occurrence methods to other parts-of-speech, such as
adjectives and nouns, to identify collocations of these parts-of-speech.
There is much more to explore with co-occurrence analysis, but this
should give you a good idea of the types of questions that can be
addressed with co-occurrence analysis.

\hypertarget{sec-eda-unsupervised}{%
\subsection{Unsupervised learning}\label{sec-eda-unsupervised}}

Aligned in purpose with descriptive approaches, unsupervised learning
approaches to exploratory data analysis are used to identify patterns in
the data from an algorithmic perspective. Common methods in text
analysis include principle component analysis, clustering, topic
modeling, and word embedding.

We will continue to use the MASC dataset as we develop materials for our
ELL textbook to illustrate unsupervised learning methods. In the
process, we will explore the following questions:

\begin{itemize}
\tightlist
\item
  Can we identify genres based on linguistic features or co-occurrence
  patterns of the data itself?
\item
  Are there discernible topics that separate spoken from written
  discourses based on linguistic features or co-occurrence patterns?
\item
  Do certain words or phrases have different meanings or uses depending
  on the modality in which they appear?
\end{itemize}

Through these questions we will build on our knowledge of frequency,
dispersion, and co-occurrence analysis and introduce concepts and
methods associated with machine learning.

\hypertarget{sec-eda-clustering}{%
\subsubsection{Clustering}\label{sec-eda-clustering}}

\textbf{Clustering} is a unsupervised learning technique that can be
used to group similar items in the text data, helping to organize the
data into distinct categories and discover relationships between
different elements in the text. The main steps in the procedure includes
identifying the relevant linguistic features to use for clustering,
representing the features in a way that can be used for clustering, and
applying a clustering algorithm to the data. However, it is important to
consider the strengths and weaknesses of the clustering algorithm for a
particular task and how the results will be evaluated.

In our ELL textbook task, we may very well want to identify terms that
are particularly indicative of each genre. This will help us design a
textbook with relevant communicative context in which terms are used
most naturally. However,we may have questions about how well the labeled
genres in the dataset map to real distinctions in language use. For
example, are the genres `Letters' and `Journal' really distinct genres?
Or, are they more similar than different? If they are more similar than
different, then we may want to consider combining them into a single
genre.

Enter clustering. Instead of relying entirely on the labels in the MASC
dataset, we can let the data itself say something about how related the
genres are. Yet, a pivotal question is what features should we use,
otherwise known as \textbf{feature selection}. We could use terms or
lemmas, but we may want to consider other features, such as
parts-of-speech or some co-occurrence patterns. We are not locked into
using one criterion, and we can perform clustering multiple times with
different features, but we should consider the implications of our
feature selection for our interpretation of the results.

Another key question is what clustering algorithm to use. Again, we are
not married to one algorithm, and we can perform clustering multiple
times with different algorithms, but not all algorithms are created
equal. Some algorithms are better suited for certain types of data and
certain types of tasks. For example, \textbf{Hierarchical clustering} is
a good choice when we are not sure how many clusters we want to
identify, as it does not require us to specify the number of clusters
from the outset. However, it is not a good choice when we have a large
dataset, as it can be computationally expensive compared to some other
algorithms. \textbf{K-means clustering}, on the other hand, is a good
choice when we want to identify a pre-defined number of clusters, and
the aim is to gauge how well the data fit the clusters. These two
clustering techniques, therefore complement each other with Hierarchical
clustering being a good choice for initial exploration and K-means
clustering being a good choice for targeted evaluation.

With these considerations in mind, let's start by identifying the
linguistic features that we want to use for clustering. Imagine that
among the various features that we are interested in associating with
genres, we consider lemma use and part-of-speech use. However, we need
to operationalize what we mean by `use' and then transform the data into
a format that can be used for clustering. In machine learning, this
process is known as \textbf{feature engineering}. Since we aim to
compare documents it is logical for us to use the document-normalized
features. So in both lemma and part-of-speech tags, we will use the
relative frequency. An additional operation that we can apply to the
lemma feature is to weight the relative frequency by the dispersion of
the lemma. This will give us a measure of the distinctiveness of the
lemma in the document. A common implementation of this approach is to
use the \(tf-idf\) measure, which is the product of the relative
frequency and the inverse document frequency.

Each of these engineered feature sets represents a different aspect of
the lexical nature of the documents. The relative frequency of lemmas
represents the lexical diversity of the documents, the
dispersion-weighted \(tf-idf\) of lemmas represents the distinctiveness
of the lemmas in the documents, and the relative frequency of
part-of-speech tags represents the grammatical diversity of the
documents (\protect\hyperlink{ref-Petrenz2011}{Petrenz and Webber
2011}). //FIXME CITATIONS

The next question to address in any analysis is how to represent the
features. In our case, we want to represent the features in each
document. In machine learning, the most common way to represent features
is in a matrix. In our case, we want to create a matrix with the
documents in the rows and the features in the columns. The values in the
matrix will be the operationalization of lexical use in each document
for each of our three candidate measures. This configuration is known as
a \textbf{document-term matrix} (DTM).

To recast a data frame into a DTM, we can use the \texttt{cast\_dtm()}
function from the \texttt{tidytext} package. This function takes a data
frame with a document identifier, a feature identifier, and a value for
each observation and casts it into a matrix. Operations such as
normalization are easily and efficiently performed in R on matrices, so
initially we can cast a frequency table of lemmas and part-of-speech
tags into a matrix and then normalize the matrix by documents. For the
\(tf-idf\) measure we use the \texttt{bind\_tf\_idf()} function from the
\texttt{tidytext} package. This function takes a DTM and calculates the
\(tf-idf\) measure for each feature in each document. This is a
normalized measure, so we do not need to normalize the matrix by
documents. Let's see how this works with the MASC dataset in
Example~\ref{exm-eda-masc-dtms}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms}{}\label{exm-eda-masc-dtms}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load packages}
\FunctionTok{library}\NormalTok{(tidytext)}

\CommentTok{\# Create a document{-}term matrix of lemmas}
\NormalTok{masc\_lemma\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(doc\_id, lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, lemma, n) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Create a document{-}term matrix of part{-}of{-}speech tags}
\NormalTok{masc\_pos\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(doc\_id, pos) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, pos, n) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.matrix}\NormalTok{()}

\CommentTok{\# Create a document{-}term matrix of tf{-}idf weighted lemmas}
\NormalTok{masc\_lemma\_tfidf\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(doc\_id, lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{bind\_tf\_idf}\NormalTok{(doc\_id, lemma, n) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cast\_dtm}\NormalTok{(doc\_id, lemma, tf\_idf) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\end{example}

Note preview the a subset of the contents of a matrix, such as in
Example~\ref{exm-eda-masc-dtms}, we use bracket syntax \texttt{{[}{]}}
instead of the \texttt{head()} function. Let's take a look at the first
5 rows and 5 columns of the matrices, as seen in
Example~\ref{exm-eda-masc-dtms-preview}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-preview}{}\label{exm-eda-masc-dtms-preview}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Preview}
\NormalTok{masc\_lemma\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs  'd 's M.  a account
>   1    1  1  1 15       1
>   10   0  0  0  7       0
>   100  0  0  0  0       0
>   101  0  0  0  2       0
>   102  0  0  0  1       0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_pos\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs  CC DT EX IN JJ
>   1   14 35  1 44 27
>   10  11 38  0 39 18
>   100  0  2  0  2  3
>   101  3 16  0 23  7
>   102 20 29  0 34 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{masc\_lemma\_tfidf\_dtm[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>      Terms
> Docs      'd      's    M.        a account
>   1   0.0547 0.00341 0.241 0.006898  0.0391
>   10  0.0000 0.00000 0.000 0.003467  0.0000
>   100 0.0000 0.00000 0.000 0.000000  0.0000
>   101 0.0000 0.00000 0.000 0.001050  0.0000
>   102 0.0000 0.00000 0.000 0.000479  0.0000
\end{verbatim}

\end{example}

Now we can normalize the lemma and pos matrices by documents. We can do
this by dividing each feature count by the total count in each document.
This is a row-wise transformation, so we can use the \texttt{rowSums()}
function from base R to calculate the total count in each document. Then
each count divided by its row's total count, as seen in
Example~\ref{exm-eda-masc-dtms-normalized}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-normalized}{}\label{exm-eda-masc-dtms-normalized}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Normalize lemma and pos matrices by documents}
\NormalTok{masc\_lemma\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_dtm }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(masc\_lemma\_dtm)}

\NormalTok{masc\_pos\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_pos\_dtm }\SpecialCharTok{/} \FunctionTok{rowSums}\NormalTok{(masc\_pos\_dtm)}
\end{Highlighting}
\end{Shaded}

\end{example}

There are two concerns to address before we can proceed with clustering.
First, clustering algorithm performance tends to degrade with the number
of features. If we consider either the relative frequency of lemmas or
the dispersion-weighted \(tf-idf\) of lemmas, we are looking at over 25k
features! Second, clustering algorithms perform better with more
informative features. That is to say, features that are more distinct
across the documents provide better information for deriving useful
clusters.

We can address both of these concerns by reducing the number of features
and increasing the informativeness of the features. To accomplish this
is to use \textbf{dimensionality reduction}. Dimensionality reduction is
a set of methods that are used to reduce the number of features in a
dataset while retaining as much information as possible. The most common
method for dimensionality reduction is \textbf{principle component
analysis} (PCA). PCA is a method that transforms a set of correlated
variables into a set of uncorrelated variables, known as principle
components. The principle components are ordered by the amount of
variance that they explain in the data. The first principle component
explains the most variance, the second principle component explains the
second most variance, and so on.

We can apply PCA to each of these features and assess how well the
features account for the variation in the data. We can then use the
features that account for the most variation in the data for clustering.
The \texttt{prcomp()} function from base R can be used to perform PCA.
Let's apply PCA to each of our candidate feature matrices, as seen in
Example~\ref{exm-eda-masc-dtms-pca}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-pca}{}\label{exm-eda-masc-dtms-pca}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }

\CommentTok{\# Apply PCA to each feature matrix}
\NormalTok{masc\_lemma\_pca }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_dtm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{prcomp}\NormalTok{()}

\NormalTok{masc\_pos\_pca }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_pos\_dtm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{prcomp}\NormalTok{()}

\NormalTok{masc\_lemma\_tfidf\_pca }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_tfidf\_dtm }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{prcomp}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\end{example}

We can visualize the amount of variance explained by each principle
component with a scree plot. The \texttt{fviz\_eig()} function from the
\texttt{factoextra} package can be used to create a scree plot. The
\texttt{fviz\_eig()} function takes the output of the \texttt{prcomp()}
function as its argument and an argument \texttt{ncp\ =\ 10} to specify
the number of principle components to include in the plot. Let's create
a scree plot for each of our candidate feature matrices, as seen in
Figure~\ref{fig-eda-masc-dtms-pca-scree}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(factoextra)}

\CommentTok{\# Scree plot: lemma relative frequency}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_lemma\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Scree plot: lemma dispersion{-}weighted tf{-}idf}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_lemma\_tfidf\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}

\CommentTok{\# Scree plot: part{-}of{-}speech relative frequency}
\FunctionTok{fviz\_eig}\NormalTok{(masc\_pos\_pca, }\AttributeTok{ncp =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-1.pdf}

}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-1}Lemma relative
frequency}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-2.pdf}

}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-2}Lemma
dispersion-weighted tf-idf}
\end{minipage}%
%
\begin{minipage}[t]{0.33\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-scree-3.pdf}

}

}

\subcaption{\label{fig-eda-masc-dtms-pca-scree-3}Part-of-speech relative
frequency}
\end{minipage}%

\caption{\label{fig-eda-masc-dtms-pca-scree}Scree plot of the principle
components of the MASC dataset}

\end{figure}

{[}describe differences between the three plots{]}

To calculate the amount of variance explained by each principle
component we can square the standard deviations of the principle
components and divide by the sum of the squared standard deviations.
Let's calculate the amount of variance explained by each principle
component for each of our candidate feature matrices, as seen in
Example~\ref{exm-eda-masc-dtms-pca-variance}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-dtms-pca-variance}{}\label{exm-eda-masc-dtms-pca-variance}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate variance explained for first 3 principle components}
\CommentTok{\# lemma}
\NormalTok{masc\_lemma\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_lemma\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pos}
\NormalTok{masc\_pos\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_pos\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_pos\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 78.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# lemma tf{-}idf}
\NormalTok{masc\_lemma\_tfidf\_pca\_var }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_tfidf\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(masc\_lemma\_tfidf\_pca}\SpecialCharTok{$}\NormalTok{sdev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*} \DecValTok{100}

\FunctionTok{sum}\NormalTok{(masc\_lemma\_tfidf\_pca\_var[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
> [1] 6.11
\end{verbatim}

\end{example}

Combining the findings in the Scree plots and the variance explained
calculations, we can see that the first three principle components of
the part-of-speech features account for a very high proportion of the
variance. Therefore, all else being equal, we should use the
part-of-speech features. As with all things exploratory, however, it is
important to consider the implications of our feature selection for our
interpretation of the results. In this case, the part-of-speech features
approximate grammatical diversity of the documents, more so than lexical
diversity. This means that the clusters that we identify will be based
on a particular measure of grammatical diversity of the documents. If,
for example, we want to identify clusters based on the lexical diversity
of the documents, we may opt to use the lemma features, or some other
operationalized measure of lexical diversity.

Before we leave PCA, let's also take a look at the principle components
themselves. The \texttt{get\_pca\_var()} function from the
\texttt{factoextra} package can be used to extract the principle
components from the output of the \texttt{prcomp()} function. The
\texttt{get\_pca\_var()} function takes the output of the
\texttt{prcomp()} function as its argument and an argument
\texttt{ncp\ =\ 10} to specify the number of principle components to
include in the plot. Let's create a plot of the first two principle
components for each of our candidate feature matrices, as seen in
Figure~\ref{fig-eda-masc-dtms-pca-pc}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(factoextra)}

\CommentTok{\# Plot principle components: pos}
\NormalTok{masc\_pos\_pca }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{fviz\_contrib}\NormalTok{(}
    \AttributeTok{choice =} \StringTok{"var"}\NormalTok{,}
    \AttributeTok{axes =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}
    \AttributeTok{top =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-dtms-pca-pc-1.pdf}

}

\caption{\label{fig-eda-masc-dtms-pca-pc}Feature contributions to the
PCA of the MASC dataset}

\end{figure}

From Figure~\ref{fig-eda-masc-dtms-pca-pc}, we can see that the three
principle components are dominated by the relative frequency of nouns,
prepositions, and determiners. This information can help us better
understand the results of the clustering algorithm.

Now that we have identified the features that we want to use for
clustering and we have represented the features in a way that can be
used for clustering, we can apply a clustering algorithm to the data.
For Hiearchical clustering, we can use the \texttt{hclust()} function
from base R. The \texttt{hclust()} function takes a distance matrix as
its argument and an argument \texttt{method\ =\ "average"} to specify
the average linkage method. The average linkage method takes the average
of the dissimilarities between all pairs in two clusters. It is less
sensitive to outliers compared to other methods. Let's apply the
clustering algorithm to the part-of-speech features, as seen in
Example~\ref{exm-eda-masc-pos-hclust}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-hclust}{}\label{exm-eda-masc-pos-hclust}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract first 3 principle components}
\NormalTok{masc\_lemma\_pca\_pc }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}

\CommentTok{\# Create distance matrix}
\NormalTok{masc\_pos\_dist }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pca\_pc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{dist}\NormalTok{(}\AttributeTok{method =} \StringTok{"manhattan"}\NormalTok{)}

\CommentTok{\# Apply the clustering algorithm}
\NormalTok{masc\_pos\_hc }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_pos\_dist }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{hclust}\NormalTok{(}\AttributeTok{method =} \StringTok{"average"}\NormalTok{)}

\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_hc }\SpecialCharTok{|\textgreater{}} \FunctionTok{fviz\_dend}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/eda-masc-pos-hclust-1.pdf}

}

\end{figure}

\end{example}

Since we are exploring the usefulness of the 18 genre labels used in the
MASC dataset we have a good idea of how many clusters we want to start
with. This is a good case to employ the K-means clustering algorithm. In
K-means clustering, we specify the number of clusters that we want to
identify. For each cluster number, a random center is generated. Then
each observation is assigned to the cluster with the nearest center. The
center of each cluster is then recalculated based on the distribution of
the observations in the cluster. This process is iterates either a
pre-defined number of times, or until the centers converge (\emph{i.e}
observations stop switching clusters).

We can use the \texttt{kmeans()} function from base R to apply the
K-means clustering algorithm. The \texttt{kmeans()} function takes the
matrix of features as its first argument and the number of clusters as
its second argument. We can specify the number of clusters with the
\texttt{centers} argument. The \texttt{kmeans()} function also takes an
argument \texttt{nstart} to specify the number of random starts. The
K-means algorithm is sensitive to the initial starting points, so it is
a good idea to run the algorithm multiple times with different starting
points. The \texttt{nstart} argument specifies the number of random
starts. The default value is 1, but we can increase this to 10 or 20 to
increase the likelihood of finding a good solution.

Our goal, then, will be to assess how well this number of clusters fits
the data. If it does not fit the data well, we can try a different
number of clusters. We can then compare the results of the clustering
with the genre labels to see how well the clusters map to the labels and
make ajustments to the way we group the labels as necessary.

Let's start with 18 clusters, assuming the target of the number of
genres in the MASC dataset. We can apply the K-means clustering
algorithm to the part-of-speech features, as seen in
Example~\ref{exm-eda-masc-pos-kmeans}.

\begin{example}[]\protect\hypertarget{exm-eda-masc-pos-kmeans}{}\label{exm-eda-masc-pos-kmeans}

~

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract first 3 principle components}
\NormalTok{masc\_lemma\_pca\_pc }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pca}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }

\CommentTok{\# K{-}means clustering}
\NormalTok{masc\_pos\_kmeans }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_pca\_pc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{kmeans}\NormalTok{(}
    \AttributeTok{centers =} \DecValTok{18}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\end{example}

The \texttt{factoextra} package provides \texttt{fviz\_cluster()}
function for visualizing the results of clustering algorithms. The
\texttt{fviz\_cluster()} function takes the output of the
\texttt{kmeans()} function as its first argument and the matrix of
features as its second argument. The \texttt{fviz\_cluster()} function
can be used to visualize the clusters in the data, as seen in
Figure~\ref{fig-eda-masc-pos-kmeans}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_kmeans }\SpecialCharTok{|\textgreater{}}  \CommentTok{\# output of kmeans()}
  \FunctionTok{fviz\_cluster}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ masc\_lemma\_pca\_pc, }\CommentTok{\# matrix of features}
    \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{,}
    \AttributeTok{ellipse.level =} \FloatTok{0.95}\NormalTok{,}
    \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,}
    \AttributeTok{pointsize =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{palette =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{ggtheme =} \FunctionTok{theme\_qtalr}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-kmeans-1.pdf}

}

\caption{\label{fig-eda-masc-pos-kmeans}K-means clustering of the MASC
dataset}

\end{figure}

The ellipses in a k-means plot represent the 95\% confidence interval
for each cluster. The ellipses are based on the multivariate normal
distribution of the data in each cluster. The size and shape of the
ellipses tell us about the variance of the data in each cluster. The
larger the ellipse the greater the dispersion of values. A wide ellipse
suggests high within cluster variability. The distance between the
clusters also tells us about the similarity between the clusters. The
closer the clusters are to each other, the more similar they are. The
further the clusters are from each other, the more dissimilar they are.

So in Figure~\ref{fig-eda-masc-pos-kmeans}, we see a mix sizes and
shapes. This suggests that some clusters are more homogeneous than
others. We also see separation between the large wide clusters towards
the top of the plot and the smaller, more circular clusters towards the
bottom. With 18 clusters, we have a lot of clusters, so it is difficult
to interpret the results as there is a large amount of overlap. In sum,
18 clusters is likely not an optimal number for this dataset.

We could run the code in Example~\ref{exm-eda-masc-pos-kmeans} for
different values for \(k\) and plot each in turn. But a more effective
way to determine the optimal number of clusters is to plot the within
cluster sum of squares (WSS) for a range of values for \(k\). The WSS is
the sum of the squared distance between each observation and its cluster
center. With a plot of the WSS for a range of values for \(k\), we can
identify the value for \(k\) where the WSS begins to level off. This is
known as the \textbf{elbow method}. The elbow method is a heuristic, so
it is not always clear where the elbow is. However, it is a good
starting point for identifying the optimal number of clusters.

Again, the \texttt{factoextra} package has us covered. The
\texttt{fviz\_nbclust()} function can be used to plot the WSS for a
range of values for \(k\). The \texttt{fviz\_nbclust()} function takes
the \texttt{kmeans()} function as its first argument and the matrix of
features as its second argument. The \texttt{fviz\_nbclust()} function
also takes arguments \texttt{method\ =\ "wss"} to specify the WSS method
and \texttt{k.max\ =\ 20} to specify the maximum number of clusters to
plot. Let's plot the WSS for a range of values for \(k\), as seen in
Figure~\ref{fig-eda-masc-pos-kmeans-elbow}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Determine the optimal number of clusters}
\NormalTok{masc\_lemma\_pca\_pc }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{fviz\_nbclust}\NormalTok{(}
    \AttributeTok{FUNcluster =}\NormalTok{ kmeans,}
    \AttributeTok{method =} \StringTok{"wss"}\NormalTok{, }\CommentTok{\# method}
    \AttributeTok{k.max =} \DecValTok{20}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-pos-kmeans-elbow-1.pdf}

}

\caption{\label{fig-eda-masc-pos-kmeans-elbow}Elbow method for k-means
clustering of the MASC dataset}

\end{figure}

It is clear that there is significant gains in cluster fit from 1 to 4
clusters, but the gains begin to level off after 4-5 clusters. Now we
can skip ahead and try 4 clusters, as seen in
\textbf{?@exm-eda-masc-pos-kmeans-4}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# K{-}means: for 4 clusters}
\NormalTok{masc\_pos\_kmeans\_4 }\OtherTok{\textless{}{-}}
\NormalTok{  masc\_lemma\_pca\_pc }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{kmeans}\NormalTok{(}
    \AttributeTok{centers =} \DecValTok{4}\NormalTok{,}
    \AttributeTok{nstart =} \DecValTok{25}\NormalTok{,}
    \AttributeTok{iter.max =} \DecValTok{20}
\NormalTok{  )}

\CommentTok{\# Visualize}
\NormalTok{masc\_pos\_kmeans\_4  }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{fviz\_cluster}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ masc\_lemma\_pca\_pc,}
  \AttributeTok{ellipse.type =} \StringTok{"norm"}\NormalTok{,}
  \AttributeTok{ellipse.level =} \FloatTok{0.95}\NormalTok{,}
  \AttributeTok{geom =} \StringTok{"point"}\NormalTok{,}
  \AttributeTok{pointsize =} \DecValTok{1}\NormalTok{,}
  \AttributeTok{palette =} \StringTok{"grey"}\NormalTok{,}
  \AttributeTok{ggtheme =} \FunctionTok{theme\_qtalr}\NormalTok{()}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/eda-masc-pos-kmeans-4-1.pdf}

}

\end{figure}

\hypertarget{sec-eda-topic-modeling}{%
\subsubsection{Topic modeling}\label{sec-eda-topic-modeling}}

ELL: we want to identify themes or topics that are distinctive to genres
or a particular genre. This will help us design a textbook with relevant
topics for each genre and the most relevant vocabulary for each topic.

This section will discuss topic modeling techniques, which are used to
identify and group semantically similar topics in unstructured data. It
will discuss various approaches to topic modelling, such as Latent
Dirichlet Allocation (LDA), and discuss their applications in
linguistics.

\begin{itemize}
\tightlist
\item
  LDA (latent Dirichlet allocation)
\end{itemize}

Let's load the \texttt{topicmodels} package. Then we will clean the
lemmas removing stop words.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load package}
\FunctionTok{library}\NormalTok{(topicmodels) }\CommentTok{\# for topic modeling}

\CommentTok{\# Clean lemmas}
\NormalTok{masc\_lemma\_clean }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_tbl }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{lemma }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(doc\_id, lemma)}
\end{Highlighting}
\end{Shaded}

Next, we will create a document-term matrix (DTM) of the lemmas. We will
use the \texttt{cast\_dtm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a dtm of lemmas}

\NormalTok{masc\_lemma\_dtm }\OtherTok{\textless{}{-}} 
\NormalTok{  masc\_lemma\_clean }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(doc\_id, lemma) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{cast\_dtm}\NormalTok{(}
    \AttributeTok{document =}\NormalTok{ doc\_id,}
    \AttributeTok{term =}\NormalTok{ lemma,}
    \AttributeTok{value =}\NormalTok{ n}
\NormalTok{  ) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We can fit a LDA model to our DTM.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit a LDA model}
\NormalTok{lemma\_lda\_model }\OtherTok{\textless{}{-}} 
  \FunctionTok{LDA}\NormalTok{(}
\NormalTok{    masc\_lemma\_dtm,}
    \AttributeTok{k =} \DecValTok{10}\NormalTok{,}
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{seed =} \DecValTok{1234}\NormalTok{)}
\NormalTok{  )}

\CommentTok{\# Extract the topics}
\NormalTok{lemma\_topics }\OtherTok{\textless{}{-}} 
\NormalTok{  lemma\_lda\_model }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{tidy}\NormalTok{(}\AttributeTok{matrix =} \StringTok{"beta"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# beta: distribution of words in each topic. Gamma: distribution of topics in each document.}
  \FunctionTok{group\_by}\NormalTok{(topic) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_max}\NormalTok{(beta, }\AttributeTok{n =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(topic, }\FunctionTok{desc}\NormalTok{(beta))}
\end{Highlighting}
\end{Shaded}

So, if I want to see the words associated with the topics, use the
`beta' matrix. For the topics associated with each document, use the
`gamma' matrix.

View the top words in each topic in a bar plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# View the topics in a bar plot}
\NormalTok{lemma\_topics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{term =} \FunctionTok{reorder\_within}\NormalTok{(term, beta, topic)) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(term, beta)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ topic, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_reordered}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{exploration_files/figure-pdf/fig-eda-masc-topic-modeling-lda-1.pdf}

}

\caption{\label{fig-eda-masc-topic-modeling-lda}\textbf{?(caption)}}

\end{figure}

\hypertarget{sec-eda-word-embedding}{%
\subsubsection{Word embedding}\label{sec-eda-word-embedding}}

ELL: we want to meaning similarities and potential differences between
spoken and written discourses. This will help provide students with a
more nuanced understanding of potential synonyms within and differences
between spoken and written discourses.

This section will discuss word embedding techniques, which are used to
represent words in a vector space. It will discuss various approaches to
word embedding, such as Word2Vec and GloVe, and discuss their
applications in linguistics.

\begin{itemize}
\tightlist
\item
  Word2vec (skip-gram)
\item
  GloVe (global vectors for word representation)
\end{itemize}

\hypertarget{summary-7}{%
\section{Summary}\label{summary-7}}

Exploratory data analysis is a set of methods that can be used to
explore a dataset and to identify new questions and new variables of
interest. The methods can be used to describe a dataset and to identify
linguistic units that are distinctive to a particular group or sub-group
in the dataset. The methods can also be used to identify semantically
similar topics in unstructured data. The results of exploratory analysis
can be used to inform the development of a hypothesis or to inform the
design of a machine learning model.

\hypertarget{activities-6}{%
\section*{Activities}\label{activities-6}}
\addcontentsline{toc}{section}{Activities}

\markright{Activities}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{file-code} Recipe}

\textbf{What}:
\href{https://lin380.github.io/tadr/articles/recipe_11.html}{Exploratory
methods: descriptive and unsupervised learning analysis methods}\\
\textbf{How}: Read Recipe 10 and participate in the Hypothes.is online
social annotation.\\
\textbf{Why}: To illustrate how to prepare a dataset for descriptive and
unsupervised machine learning methods and evaluate the results for
exploratory data analysis.

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\textbf{\faIcon{flask} Lab}

\textbf{What}: \href{https://github.com/lin380/lab_11}{Exploratory Data
Analysis}\\
\textbf{How}: Clone, fork, and complete the steps in Lab 9.\\
\textbf{Why}: To gain experience working with coding strategies to
prepare, feature engineer, explore, and evaluate results from
exploratory data analyses, practice transforming datasets into new
object formats and visualizing relationships, and implement
organizational strategies for organizing and reporting results in a
reproducible fashion.

\end{tcolorbox}

\hypertarget{questions-7}{%
\section*{Questions}\label{questions-7}}
\addcontentsline{toc}{section}{Questions}

\markright{Questions}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Conceptual questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is exploratory data analysis?
\item
  How can exploratory data analysis be used to uncover patterns and
  associations?
\item
  Describe the workflow of exploratory data analysis?
\item
  What are the advantages and disadvantages of descriptive analysis?
\item
  What are the advantages and disadvantages of unsupervised learning?
\item
  What is the difference between supervised and unsupervised learning?
\item
  How does exploratory data analysis differ from traditional hypothesis
  testing?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, rightrule=.15mm, arc=.35mm, left=2mm, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

\faIcon{wrench} \textbf{Technical questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Write a function in R to conduct a hierarchical cluster analysis on a
  dataset.
\item
  Implement a k-means algorithm in R to identify clusters within a
  dataset.
\item
  Implement a Principal Component Analysis (PCA) algorithm in R to
  identify patterns and associations within a dataset.
\item
  Write a function in R to produce a descriptive summary of a dataset.
\item
  Conduct a correlation analysis in R to identify relationships between
  variables in a dataset.
\item
  Load a dataset into R and conduct a frequency analysis on the dataset.
\item
  Load a dataset into R and conduct a keyword in context analysis on the
  dataset.
\item
  Load a dataset into R and conduct a keyword analysis on the dataset.
\item
  Load a dataset into R and conduct a sentiment analysis on the dataset.
\item
  Load a dataset into R and conduct a topic modelling analysis on the
  dataset.
\end{enumerate}

\end{tcolorbox}

\hypertarget{sec-prediction}{%
\chapter{Prediction}\label{sec-prediction}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

\hypertarget{sec-inference}{%
\chapter{Inference}\label{sec-inference}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

\part{Communication}

In this section I cover the steps in presenting the findings of the
research both as a research document and as a reproducible research
project. Both research documents and reproducible projects are
fundamental components of modern scientific inquiry. On the one hand a
research document provides readers a detailed summary of the main import
of the research study. On the other hand making the research project
available to interested readers ensures that the scientific community
can gain insight into the process implemented in the research and thus
enables researchers to vet and extend this research to build a more
robust and verifiable research base.

\hypertarget{sec-reports}{%
\chapter{Reports}\label{sec-reports}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

\hypertarget{sec-collaboration}{%
\chapter{Collaboration}\label{sec-collaboration}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

\bookmarksetup{startatroot}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Ackoff1989}{}}%
Ackoff, Russell L. 1989. {``From Data to Wisdom.''} \emph{Journal of
Applied Systems Analysis} 16 (1): 3--9.

\leavevmode\vadjust pre{\hypertarget{ref-Adel2020}{}}%
del, Annelie. 2020. {``Corpus Compilation.''} In \emph{A Practical
Handbook of Corpus Linguistics}, edited by Magali Paquot and Stefan Th.
Gries, 3--24. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Albert2015}{}}%
Albert, Saul, Laura E. de Ruiter, and J. P. de Ruiter. 2015. {``CABNC:
The Jeffersonian Transcription of the Spoken British National Corpus.''}
TalkBank.

\leavevmode\vadjust pre{\hypertarget{ref-R-quarto}{}}%
Allaire, JJ. 2022. \emph{Quarto: R Interface to Quarto Markdown
Publishing System}. \url{https://github.com/quarto-dev/quarto-r}.

\leavevmode\vadjust pre{\hypertarget{ref-R-rmarkdown}{}}%
Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier
Luraschi, Kevin Ushey, Aron Atkins, et al. 2023. \emph{Rmarkdown:
Dynamic Documents for r}. \url{https://github.com/rstudio/rmarkdown}.

\leavevmode\vadjust pre{\hypertarget{ref-Baayen2006}{}}%
Baayen, R. Harald, L. B. Feldman, and R. Schreuder. 2006.
{``Morphological Influences on the Recognition of Monosyllabic
Monomorphemic Words.''} \emph{Journal of Memory and Language} 55:
290--313. \url{https://doi.org/10.1016/j.jml.2006.03.008}.

\leavevmode\vadjust pre{\hypertarget{ref-Bao2019}{}}%
Bao, Wang, Ning Lianju, and Kong Yue. 2019. {``Integration of
Unsupervised and Supervised Machine Learning Algorithms for Credit Risk
Assessment.''} \emph{Expert Systems with Applications} 128 (August):
301--15. \url{https://doi.org/10.1016/j.eswa.2019.02.033}.

\leavevmode\vadjust pre{\hypertarget{ref-R-future}{}}%
Bengtsson, Henrik. 2023. \emph{Future: Unified Parallel and Distributed
Processing in r for Everyone}. \url{https://future.futureverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-quanteda.corpora}{}}%
Benoit, Kenneth. 2020. \emph{Quanteda.corpora: A Collection of Corpora
for Quanteda}. \url{http://github.com/quanteda/quanteda.corpora}.

\leavevmode\vadjust pre{\hypertarget{ref-R-stopwords}{}}%
Benoit, Kenneth, David Muhr, and Kohei Watanabe. 2021. \emph{Stopwords:
Multilingual Stopword Lists}.
\url{https://github.com/quanteda/stopwords}.

\leavevmode\vadjust pre{\hypertarget{ref-R-readtext}{}}%
Benoit, Kenneth, and Adam Obeng. 2023. \emph{Readtext: Import and
Handling for Plain and Formatted Text Files}.
\url{https://github.com/quanteda/readtext}.

\leavevmode\vadjust pre{\hypertarget{ref-R-workflowr}{}}%
Blischak, John, Peter Carbonetto, and Matthew Stephens. 2023.
\emph{Workflowr: A Framework for Reproducible and Collaborative Data
Science}. \url{https://github.com/workflowr/workflowr}.

\leavevmode\vadjust pre{\hypertarget{ref-R-wordbankr}{}}%
Braginsky, Mika. 2022. \emph{Wordbankr: Accessing the Wordbank
Database}. \url{https://langcog.github.io/wordbankr/}.

\leavevmode\vadjust pre{\hypertarget{ref-Bresnan2007a}{}}%
Bresnan, Joan. 2007. {``A Few Lessons from Typology.''} \emph{Linguistic
Typology} 11 (1): 297--306.

\leavevmode\vadjust pre{\hypertarget{ref-Brown2005}{}}%
Brown, Keith. 2005. \emph{Encyclopedia of Language and Linguistics}.
Vol. 1. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Buckheit1995}{}}%
Buckheit, Jonathan B., and David L. Donoho. 1995. {``Wavelab and
Reproducible Research.''} In \emph{Wavelets and Statistics}, 55--81.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Bychkovska2017}{}}%
Bychkovska, Tetyana, and Joseph J. Lee. 2017. {``At the Same Time:
Lexical Bundles in L1 and L2 University Student Argumentative
Writing.''} \emph{Journal of English for Academic Purposes} 30
(November): 38--52. \url{https://doi.org/10.1016/j.jeap.2017.10.008}.

\leavevmode\vadjust pre{\hypertarget{ref-Campbell2001}{}}%
Campbell, Lyle. 2001. {``The History of Linguistics.''} In \emph{The
Handbook of Linguistics}, edited by Mark Aronoff and Janie Rees-Miller,
81--104. Blackwell Handbooks in Linguistics. Blackwell Publishers.

\leavevmode\vadjust pre{\hypertarget{ref-Carmi2020}{}}%
Carmi, Elinor, Simeon J. Yates, Eleanor Lockley, and Alicja Pawluczuk.
2020. {``Data Citizenship: Rethinking Data Literacy in the Age of
Disinformation, Misinformation, and Malinformation.''} \emph{Internet
Policy Review} 9 (2).

\leavevmode\vadjust pre{\hypertarget{ref-Chambers2020}{}}%
Chambers, John M. 2020. {``S, r, and Data Science.''} \emph{Proceedings
of the ACM on Programming Languages} 4 (HOPL): 1--17.
\url{https://doi.org/10.1145/3386334}.

\leavevmode\vadjust pre{\hypertarget{ref-Chan2014}{}}%
Chan, Sin-wai. 2014. \emph{Routledge Encyclopedia of Translation
Technology}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-Conway2012}{}}%
Conway, Lucian Gideon, Laura Janelle Gornick, Chelsea Burfeind, Paul
Mandella, Andrea Kuenzli, Shannon C. Houck, and Deven Theresa Fullerton.
2012. {``Does Complex or Simple Rhetoric Win Elections? An Integrative
Complexity Analysis of u.s. Presidential Campaigns.''} \emph{Political
Psychology} 33 (5): 599--618.
\url{https://doi.org/10.1111/j.1467-9221.2012.00910.x}.

\leavevmode\vadjust pre{\hypertarget{ref-Cross2006}{}}%
Cross, Nigel. 2006. {``Design as a Discipline.''} \emph{Designerly Ways
of Knowing}, 95--103.

\leavevmode\vadjust pre{\hypertarget{ref-DataNeverSleeps08-2021}{}}%
{``Data Never Sleeps 7.0 Infographic.''} 2019.
https://www.domo.com/learn/infographic/data-never-sleeps-7.

\leavevmode\vadjust pre{\hypertarget{ref-Desjardins2019}{}}%
Desjardins, Jeff. 2019. {``How Much Data Is Generated Each Day?''}
\emph{Visual Capitalist}.

\leavevmode\vadjust pre{\hypertarget{ref-Donoho2017}{}}%
Donoho, David. 2017. {``50 Years of Data Science.''} \emph{Journal of
Computational and Graphical Statistics} 26 (4): 745--66.
\url{https://doi.org/10.1080/10618600.2017.1384734}.

\leavevmode\vadjust pre{\hypertarget{ref-Dubnjakovic2010}{}}%
Dubnjakovic, Ana, and Patrick Tomlin. 2010. \emph{A Practical Guide to
Electronic Resources in the Humanities}. Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-Eisenstein2012}{}}%
Eisenstein, Jacob, Brendan O'Connor, Noah A Smith, and Eric P Xing.
2012. {``Mapping the Geographical Diffusion of New Words.''}
\emph{Computation and Language}, 1--13.
\url{https://doi.org/10.1371/journal.pone.0113114}.

\leavevmode\vadjust pre{\hypertarget{ref-Francom2022}{}}%
Francom, Jerid. 2022. {``Corpus Studies of Syntax.''} In \emph{The
Cambridge Handbook of Experimental Syntax}, edited by Grant Goodall,
687--713. Cambridge Handbooks in Language and Linguistics. Cambridge
University Press.

\leavevmode\vadjust pre{\hypertarget{ref-R-qtalrkit}{}}%
---------. 2023. \emph{Qtalrkit: Quantitative Text Analysis for
Linguists Resource Kit}. \url{https://github.com/qtalr/qtalrkit}.

\leavevmode\vadjust pre{\hypertarget{ref-Gandrud2015}{}}%
Gandrud, Christopher. 2015.
\emph{\href{https://www.ncbi.nlm.nih.gov/pubmed/17811671}{Reproducible
Research with r and r Studio}}. Second edition. CRC Press.

\leavevmode\vadjust pre{\hypertarget{ref-Gentleman2007}{}}%
Gentleman, Robert, and Duncan Temple Lang. 2007. {``Statistical Analyses
and Reproducible Research.''} \emph{Journal of Computational and
Graphical Statistics} 16 (1): 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-Gilquin2009}{}}%
Gilquin, Gatanelle, and Stefan Th Gries. 2009. {``Corpora and
Experimental Methods: A State-of-the-Art Review.''} \emph{Corpus
Linguistics and Linguistic Theory} 5 (1): 1--26.
\url{https://doi.org/10.1515/CLLT.2009.001}.

\leavevmode\vadjust pre{\hypertarget{ref-Gomez-Uribe2015}{}}%
Gomez-Uribe, Carlos A., and Neil Hunt. 2015. {``The Netflix Recommender
System: Algorithms, Business Value, and Innovation.''} \emph{ACM
Transactions on Management Information Systems (TMIS)} 6 (4): 1--19.

\leavevmode\vadjust pre{\hypertarget{ref-Gries2021a}{}}%
Gries, Stefan Th. 2021. \emph{Statistics for Linguistics with r}. De
Gruyter Mouton.

\leavevmode\vadjust pre{\hypertarget{ref-Gries2023}{}}%
---------. 2023. {``Statistical Methods in Corpus Linguistics.''} In
\emph{Readings in Corpus Linguistics: A Teaching and Research Guide for
Scholars in Nigeria and Beyond,} 78--114.

\leavevmode\vadjust pre{\hypertarget{ref-Gries2013a}{}}%
Gries, Stefan Th. 2013. \emph{Statistics for Linguistics with r. A
Practical Introduction}. 2nd revise.

\leavevmode\vadjust pre{\hypertarget{ref-Grieve2018}{}}%
Grieve, Jack, Andrea Nini, and Diansheng Guo. 2018. {``Mapping Lexical
Innovation on American Social Media.''} \emph{Journal of English
Linguistics} 46 (4): 293--319.

\leavevmode\vadjust pre{\hypertarget{ref-Hay2002}{}}%
Hay, Jennifer. 2002. {``From Speech Perception to Morphology: Affix
Ordering Revisited.''} \emph{Language} 78 (3): 527--55.

\leavevmode\vadjust pre{\hypertarget{ref-R-fs}{}}%
Hester, Jim, Hadley Wickham, and Gbor Csrdi. 2023. \emph{Fs:
Cross-Platform File System Operations Based on Libuv}.
\url{https://fs.r-lib.org}.

\leavevmode\vadjust pre{\hypertarget{ref-Hicks2019}{}}%
Hicks, Stephanie C., and Roger D. Peng. 2019. {``Elements and Principles
for Characterizing Variation Between Data Analyses.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1903.07639}.

\leavevmode\vadjust pre{\hypertarget{ref-Ide2008}{}}%
Ide, Nancy, Collin Baker, Christiane Fellbaum, Charles Fillmore, and
Rebecca Passonneau. 2008. {``MASC: The Manually Annotated Sub-Corpus of
American English.''} In \emph{6th International Conference on Language
Resources and Evaluation, LREC 2008}, 2455--60. European Language
Resources Association (ELRA).

\leavevmode\vadjust pre{\hypertarget{ref-Ignatow2017}{}}%
Ignatow, Gabe, and Rada Mihalcea. 2017. \emph{An Introduction to Text
Mining: Research Design, Data Collection, and Analysis}. Sage
Publications.

\leavevmode\vadjust pre{\hypertarget{ref-Jaeger2007}{}}%
Jaeger, T Florian, and Neal Snider. 2007. {``Implicit Learning and
Syntactic Persistence: Surprisal and Cumulativity.''} \emph{University
of Rochester Working Papers in the Language Sciences} 3 (1).

\leavevmode\vadjust pre{\hypertarget{ref-Kaur2018}{}}%
Kaur, Jashanjot, and P. Kaur Buttar. 2018. {``A Systematic Review on
Stopword Removal Algorithms.''} \emph{International Journal on Future
Revolution in Computer Science \& Communication Engineering} 4 (4):
207--10.

\leavevmode\vadjust pre{\hypertarget{ref-Kloumann2012}{}}%
Kloumann, IM, CM Danforth, KD Harris, and CA Bliss. 2012. {``Positivity
of the English Language.''} \emph{PloS One}.

\leavevmode\vadjust pre{\hypertarget{ref-Koehn2005}{}}%
Koehn, P. 2005. {``Europarl: A Parallel Corpus for Statistical Machine
Translation.''} \emph{MT Summit X}, 12--16.

\leavevmode\vadjust pre{\hypertarget{ref-Kostic2003}{}}%
Kosti, Aleksandar, Tanja Markovi, and Aleksandar Baucal. 2003.
{``Inflectional Morphology and Word Meaning: Orthogonal or
Co-Implicative Cognitive Domains?''} In \emph{Morphological Structure in
Language Processing}, edited by R. Harald Baayen and Robert Schreuder,
1--44. De Gruyter Mouton. \url{https://doi.org/10.1515/9783110910186.1}.

\leavevmode\vadjust pre{\hypertarget{ref-R-TBDBr}{}}%
Kowalski, John, and Rob Cavanaugh. 2022. \emph{TBDBr: Easy Access to
TalkBankDB via r API}. \url{https://github.com/TalkBank/TalkBankDB-R}.

\leavevmode\vadjust pre{\hypertarget{ref-Krathwohl2002}{}}%
Krathwohl, David R. 2002. {``A Revision of Bloom's Taxonomy: An
Overview.''} \emph{Theory into Practice} 41 (4): 212--18.

\leavevmode\vadjust pre{\hypertarget{ref-R-swirl}{}}%
Kross, Sean, Nick Carchedi, Bill Bauer, and Gina Grdina. 2020.
\emph{Swirl: Learn r, in r}. \url{http://swirlstats.com}.

\leavevmode\vadjust pre{\hypertarget{ref-Kucera1967}{}}%
Kucera, H, and W N Francis. 1967. \emph{Computational Analysis of
Present Day American English}. Brown University Press Providence.

\leavevmode\vadjust pre{\hypertarget{ref-R-targets}{}}%
Landau, William Michael. 2023. \emph{Targets: Dynamic Function-Oriented
Make-Like Declarative Pipelines}.
\url{https://docs.ropensci.org/targets/}.

\leavevmode\vadjust pre{\hypertarget{ref-Lantz2013}{}}%
Lantz, Brett. 2013. \emph{Machine Learning with r}. Birmingham: Packt
Publishing.

\leavevmode\vadjust pre{\hypertarget{ref-Leech1992}{}}%
Leech, Geoffrey. 1992. {``100 Million Words of English: The British
National Corpus (BNC),''} no. 1991: 1--13.

\leavevmode\vadjust pre{\hypertarget{ref-Lewis2004}{}}%
Lewis, Michael. 2004. \emph{Moneyball: The Art of Winning an Unfair
Game}. WW Norton \& Company.

\leavevmode\vadjust pre{\hypertarget{ref-Liu2021}{}}%
Liu, Kanglong, and Muhammad Afzaal. 2021. {``Syntactic Complexity in
Translated and Non-Translated Texts: A Corpus-Based Study of
Simplification.''} Edited by Diego Raphael Amancio. \emph{PLOS ONE} 16
(6): e0253454. \url{https://doi.org/10.1371/journal.pone.0253454}.

\leavevmode\vadjust pre{\hypertarget{ref-Lozano2009}{}}%
Lozano, Cristbal. 2009. {``CEDEL2: Corpus Escrito Del Espaol L2.''}
\emph{Applied Linguistics Now: Understanding Language and Mind/La
Lingstica Aplicada Hoy: Comprendiendo El Lenguaje y La Mente. Almera:
Universidad de Almera}, 197--212.

\leavevmode\vadjust pre{\hypertarget{ref-Magueresse2020}{}}%
Magueresse, Alexandre, Vincent Carles, and Evan Heetderks. 2020.
{``Low-Resource Languages: A Review of Past Work and Future
Challenges.''} arXiv. \url{https://arxiv.org/abs/2006.07264}.

\leavevmode\vadjust pre{\hypertarget{ref-Manning2003}{}}%
Manning, Christopher. 2003. {``Probabilistic Syntax.''} In
\emph{Probabilistic Linguistics}, edited by Bod, Jennifer Hay, and
Jannedy, 289--341. Cambridge, MA: MIT Press.

\leavevmode\vadjust pre{\hypertarget{ref-Marcus1993}{}}%
Marcus, Mitchell P, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993. {``Building a Large Annotated Corpus of English: The Penn
Treebank.''} \emph{Computational Linguistics} 19 (2): 313--30.

\leavevmode\vadjust pre{\hypertarget{ref-Marwick2018}{}}%
Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. {``Packaging
Data Analytical Work Reproducibly Using r (and Friends).''} \emph{The
American Statistician} 72 (1): 80--88.

\leavevmode\vadjust pre{\hypertarget{ref-R-lingtypology}{}}%
Moroz, George. 2023. \emph{Lingtypology: Linguistic Typology and
Mapping}. \url{https://CRAN.R-project.org/package=lingtypology}.

\leavevmode\vadjust pre{\hypertarget{ref-Mosteller1963}{}}%
Mosteller, Frederick, and David L Wallace. 1963. {``Inference in an
Authorship Problem.''} \emph{Journal of the American Statistical
Association} 58 (302): 275--309.
\url{https://www.jstor.org/stable/2283270}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tokenizers}{}}%
Mullen, Lincoln. 2022. \emph{Tokenizers: Fast, Consistent Tokenization
of Natural Language Text}. \url{https://docs.ropensci.org/tokenizers/}.

\leavevmode\vadjust pre{\hypertarget{ref-Munoz2006}{}}%
Muoz, Carmen, ed. 2006. \emph{Age and the Rate of Foreign Language
Learning}. 1st ed. Vol. 19. Second Language Acquisition Series.
Clevedon: Multilingual Matters.

\leavevmode\vadjust pre{\hypertarget{ref-Nisioi2016}{}}%
Nisioi, Sergiu, Ella Rabinovich, Liviu P. Dinu, and Shuly Wintner. 2016.
{``A Corpus of Native, Non-Native and Translated Texts.''} In
\emph{Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC 2016)}. Portoro{z}, Slovenia: European
Language Resources Association (ELRA).

\leavevmode\vadjust pre{\hypertarget{ref-Nivre2016}{}}%
Nivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg,
Jan Haji, Christopher D Manning, Ryan McDonald, et al. 2016.
{``Universal Dependencies V1: A Multilingual Treebank Collection.''}
\emph{Proceedings of the Tenth International Conference on Language
Resources and Evaluation (LREC'16)}, 1659--66. \url{https://doi.org/?}

\leavevmode\vadjust pre{\hypertarget{ref-Nivre2020}{}}%
Nivre, Joakim, Marie-Catherine De Marneffe, Filip Ginter, Jan Haji,
Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis
Tyers, and Daniel Zeman. 2020. {``Universal Dependencies V2: An
Evergrowing Multilingual Treebank Collection.''} \emph{arXiv Preprint
arXiv:2004.10643}. \url{https://arxiv.org/abs/2004.10643}.

\leavevmode\vadjust pre{\hypertarget{ref-Olohan2008}{}}%
Olohan, Maeve. 2008. {``Leave It Out! Using a Comparable Corpus to
Investigate Aspects of Explicitation in Translation.''} \emph{Cadernos
de Traduo}, 153--69.

\leavevmode\vadjust pre{\hypertarget{ref-R-jsonlite}{}}%
Ooms, Jeroen. 2023. \emph{Jsonlite: A Simple and Robust JSON Parser and
Generator for r}.
\href{https://jeroen.r-universe.dev/jsonlite\%0Ahttps://arxiv.org/abs/1403.2805}{https://jeroen.r-universe.dev/jsonlite
https://arxiv.org/abs/1403.2805}.

\leavevmode\vadjust pre{\hypertarget{ref-Paquot2020a}{}}%
Paquot, Magali, and Stefan Th. Gries, eds. 2020. \emph{A Practical
Handbook of Corpus Linguistics}. Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Petrenz2011}{}}%
Petrenz, Philipp, and Bonnie Webber. 2011. {``Stable Classification of
Text Genres.''} \emph{Computational Linguistics} 37 (2): 385--93.
\url{https://doi.org/10.1162/COLI_a_00052}.

\leavevmode\vadjust pre{\hypertarget{ref-R-DBI}{}}%
R Special Interest Group on Databases (R-SIG-DB), Hadley Wickham, and
Kirill Mller. 2022. \emph{DBI: R Database Interface}.
\url{https://dbi.r-dbi.org}.

\leavevmode\vadjust pre{\hypertarget{ref-Riehemann2001}{}}%
Riehemann, Susanne Z. 2001. {``A Constructional Approach to Idioms and
Word Formation.''} PhD thesis, Stanford.

\leavevmode\vadjust pre{\hypertarget{ref-R-lexicon}{}}%
Rinker, Tyler. 2019. \emph{Lexicon: Lexicons for Text Analysis}.
\url{https://github.com/trinker/lexicon}.

\leavevmode\vadjust pre{\hypertarget{ref-R-pacman}{}}%
Rinker, Tyler, and Dason Kurkiewicz. 2019. \emph{Pacman: Package
Management Tool}. \url{https://github.com/trinker/pacman}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tidytext}{}}%
Robinson, David, and Julia Silge. 2023. \emph{Tidytext: Text Mining
Using Dplyr, Ggplot2, and Other Tidy Tools}.
\url{https://github.com/juliasilge/tidytext}.

\leavevmode\vadjust pre{\hypertarget{ref-Roediger2000}{}}%
Roediger, H. L. L, and K. B. B McDermott. 2000. {``Distortions of
Memory.''} \emph{The Oxford Handbook of Memory}, 149--62.

\leavevmode\vadjust pre{\hypertarget{ref-Rowley2007}{}}%
Rowley, Jennifer. 2007. {``The Wisdom Hierarchy: Representations of the
DIKW Hierarchy.''} \emph{Journal of Information Science} 33 (2):
163--80. \url{https://doi.org/10.1177/0165551506070706}.

\leavevmode\vadjust pre{\hypertarget{ref-Saxena2020}{}}%
Saxena, Shweta, and Manasi Gyanchandani. 2020. {``Machine Learning
Methods for Computer-Aided Breast Cancer Diagnosis Using Histopathology:
A Narrative Review.''} \emph{Journal of Medical Imaging and Radiation
Sciences} 51 (1): 182--93.

\leavevmode\vadjust pre{\hypertarget{ref-Sedgwick2015}{}}%
Sedgwick, Philip. 2015. {``Units of Sampling, Observation, and
Analysis.''} \emph{BMJ (Online)} 351 (October): h5396.
\url{https://doi.org/10.1136/bmj.h5396}.

\leavevmode\vadjust pre{\hypertarget{ref-R-janeaustenr}{}}%
Silge, Julia. 2022. \emph{Janeaustenr: Jane Austen's Complete Novels}.
\url{https://github.com/juliasilge/janeaustenr}.

\leavevmode\vadjust pre{\hypertarget{ref-Szmrecsanyi2004}{}}%
Szmrecsanyi, Benedikt. 2004. {``On Operationalizing Syntactic
Complexity.''} In \emph{Le Poids Des Mots. Proceedings of the 7th
International Conference on Textual Data Statistical Analysis.
Louvain-La-Neuve}, 2:1032--39.

\leavevmode\vadjust pre{\hypertarget{ref-Talarico2003}{}}%
Talarico, Jennifer M., and David C. Rubin. 2003. {``Confidence, Not
Consistency, Characterizes Flashbulb Memories.''} \emph{Psychological
Science} 14 (5): 455--61. \url{https://doi.org/10.1111/1467-9280.02453}.

\leavevmode\vadjust pre{\hypertarget{ref-Tottie2011}{}}%
Tottie, Gunnel. 2011. {``Uh and Um as Sociolinguistic Markers in British
English.''} \emph{International Journal of Corpus Linguistics} 16 (2):
173--97.

\leavevmode\vadjust pre{\hypertarget{ref-SWDA2008}{}}%
University of Colorado Boulder. 2008. {``Switchboard Dialog Act Corpus.
Web Download.''} Linguistic Data Consortium.

\leavevmode\vadjust pre{\hypertarget{ref-Voigt2017}{}}%
Voigt, Rob, Nicholas P. Camp, Vinodkumar Prabhakaran, William L.
Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan
Jurafsky, and Jennifer L. Eberhardt. 2017. {``Language from Police Body
Camera Footage Shows Racial Disparities in Officer Respect.''}
\emph{Proceedings of the National Academy of Sciences} 114 (25):
6521--26.

\leavevmode\vadjust pre{\hypertarget{ref-R-skimr}{}}%
Waring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,
Hao Zhu, and Shannon Ellis. 2022. \emph{Skimr: Compact and Flexible
Summaries of Data}. \url{https://docs.ropensci.org/skimr/}.

\leavevmode\vadjust pre{\hypertarget{ref-R-ProjectTemplate}{}}%
White, John Myles. 2023. \emph{ProjectTemplate: Automates the Creation
of New Statistical Analysis Projects}. \url{http://projecttemplate.net}.

\leavevmode\vadjust pre{\hypertarget{ref-Wickham2014a}{}}%
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (10). \url{https://doi.org/10.18637/jss.v059.i10}.

\leavevmode\vadjust pre{\hypertarget{ref-R-stringr}{}}%
---------. 2022. \emph{Stringr: Simple, Consistent Wrappers for Common
String Operations}. \url{https://stringr.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tidyverse}{}}%
---------. 2023. \emph{Tidyverse: Easily Install and Load the
Tidyverse}. \url{https://tidyverse.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-usethis}{}}%
Wickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher.
2023. \emph{Usethis: Automate Package and Project Setup}.
\url{https://usethis.r-lib.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-dplyr}{}}%
Wickham, Hadley, Romain Franois, Lionel Henry, Kirill Mller, and Davis
Vaughan. 2023. \emph{Dplyr: A Grammar of Data Manipulation}.
\url{https://dplyr.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-dbplyr}{}}%
Wickham, Hadley, Maximilian Girlich, and Edgar Ruiz. 2023. \emph{Dbplyr:
A Dplyr Back End for Databases}. \url{https://dbplyr.tidyverse.org/}.

\leavevmode\vadjust pre{\hypertarget{ref-R-readr}{}}%
Wickham, Hadley, Jim Hester, and Jennifer Bryan. 2023. \emph{Readr: Read
Rectangular Text Data}. \url{https://readr.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-devtools}{}}%
Wickham, Hadley, Jim Hester, Winston Chang, and Jennifer Bryan. 2022.
\emph{Devtools: Tools to Make Developing r Packages Easier}.
\url{https://devtools.r-lib.org/}.

\leavevmode\vadjust pre{\hypertarget{ref-R-haven}{}}%
Wickham, Hadley, Evan Miller, and Danny Smith. 2023. \emph{Haven: Import
and Export SPSS, Stata and SAS Files}.
\url{https://haven.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tidyr}{}}%
Wickham, Hadley, Davis Vaughan, and Maximilian Girlich. 2023.
\emph{Tidyr: Tidy Messy Data}. \url{https://tidyr.tidyverse.org}.

\leavevmode\vadjust pre{\hypertarget{ref-R-udpipe}{}}%
Wijffels, Jan. 2023. \emph{Udpipe: Tokenization, Parts of Speech
Tagging, Lemmatization and Dependency Parsing with the UDPipe 'NLP'
Toolkit}. \url{https://bnosac.github.io/udpipe/en/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-Wulff2007}{}}%
Wulff, S, A Stefanowitsch, and Stefan Th. Gries. 2007. {``Brutal Brits
and Persuasive Americans.''} \emph{Aspects of Meaning}.

\leavevmode\vadjust pre{\hypertarget{ref-R-bookdown}{}}%
Xie, Yihui. 2023a. \emph{Bookdown: Authoring Books and Technical
Documents with r Markdown}. \url{https://github.com/rstudio/bookdown}.

\leavevmode\vadjust pre{\hypertarget{ref-R-tinytex}{}}%
---------. 2023b. \emph{Tinytex: Helper Functions to Install and
Maintain TeX Live, and Compile LaTeX Documents}.
\url{https://github.com/rstudio/tinytex}.

\leavevmode\vadjust pre{\hypertarget{ref-Zipf1949}{}}%
Zipf, George Kingsley. 1949. \emph{Human Behavior and the Principle of
Least Effort}. Oxford, England: Addison-Wesley Press.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{data-appendix}{%
\chapter{Data}\label{data-appendix}}

\begin{itemize}
\tightlist
\item
  CEDEL2
\item
  SOTU
\item
  CABNC
\item
  Federalist Papers (LOC)
\item
  SWDA
\item
  Brown Corpus
\item
  ANC
\item
  Europarl
\item
  ENNTT
\item
  \ldots{}
\end{itemize}

\hypertarget{feedback-appendix}{%
\chapter{\texorpdfstring{Feedback
\faIcon{comment}}{Feedback }}\label{feedback-appendix}}

Thank you for taking the time to read through this book . I really
value your opinion and I would love to hear your thoughts as I continue
to make progress.

\hypertarget{where-to-review}{%
\subsection*{Where to review}\label{where-to-review}}
\addcontentsline{toc}{subsection}{Where to review}

Chapters that are ready for feedback will appear with the following
callout.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Draft}, rightrule=.15mm, colframe=quarto-callout-tip-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Ready for review.

\end{tcolorbox}

Those that are not ready will appear with the following callout.

\begin{tcolorbox}[enhanced jigsaw, breakable, colback=white, opacitybacktitle=0.6, coltitle=black, left=2mm, toptitle=1mm, titlerule=0mm, arc=.35mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, rightrule=.15mm, colframe=quarto-callout-caution-color-frame, bottomtitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, toprule=.15mm, leftrule=.75mm, bottomrule=.15mm, opacityback=0]

Under development.

\end{tcolorbox}

In a few cases a chapter will be ready for review, but I'll still be
working on the exercises, callouts, \emph{etc.} In those cases, the
items that are still under development will be marked with the
\faIcon{wrench} icon.

\hypertarget{what-to-look-for}{%
\subsection*{What to look for}\label{what-to-look-for}}
\addcontentsline{toc}{subsection}{What to look for}

As you read over the draft, I'd appreciate your feedback in the
following areas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Clarity and Comprehensibility\\
  I'd love to know if you think the content is clear and easy to
  understand. Do you think the concepts are broken down enough? Are the
  examples helpful? If anything seems too jargon-y or confusing,
  definitely let me know.
\item
  Consistency\\
  It's pretty important to keep things smooth. So, keep an eye out for
  any inconsistent writing styles, terminology, or layout. If something
  seems off, I'd appreciate it if you point it out.
\item
  Relevance\\
  Does the material match the current standards and knowledge? Will it
  the topics and questions be of interest to linguists? If something
  feels outdated or irrelevant, don't hesitate to mention it.
\item
  Engagement\\
  \hspace{0pt}I\hspace{0pt}'m not looking to drop a boring read on
  people. So, as you're going through it, think about whether it holds
  your interest. Maybe \hspace{0pt}the prose needs more life, the
  examples need to be more diverse, or the exercises could be more or
  less challenging. If you have any ideas, I'm all ears.
\end{enumerate}

\hypertarget{how-to-submit-feedback}{%
\subsection*{How to submit feedback}\label{how-to-submit-feedback}}
\addcontentsline{toc}{subsection}{How to submit feedback}

Depending on your preference, you can submit feedback in one of three
ways:

\begin{itemize}
\item
  \href{https://hypothes.is/groups/wppaYKxy/qtal-feedback}{hypothes.is}\\
  This is the easiest way to submit feedback. Join the
  ``qtal\_feedback'' annotation group and just highlight the text you
  want to comment on and click the ``Annotate'' button. You can also add
  comments to the right sidebar.
\item
  \href{https://github.com/qtalr/book/issues}{GitHub issues}\\
  This book is hosted on GitHub, so you can submit feedback directly
  through the issues page for the repository. Just click the ``New
  issue'' button and fill out the form. You'll need a GitHub account to
  do this.
\item
  Email me at
  \href{mailto:francojc@wfu.edu}{\nolinkurl{francojc@wfu.edu}}\\
  If you'd rather not use the other options, you can always email me
  directly. Just make sure to try to include references to the specific
  parts of the book you're referring to. A link or section number will
  do.
\end{itemize}

\hypertarget{thank-yous}{%
\subsection*{Thank yous!}\label{thank-yous}}
\addcontentsline{toc}{subsection}{Thank yous!}

I want to thank you beforehand for your willingness to help me out. I
really appreciate it. I also want to thank you in print. Please give me
the name you would like to appear in the
\protect\hyperlink{acknowledgements}{Acknowledgements} section. If you'd
rather not be acknowledged in the final version of the book, please let
me know.



\printindex

\end{document}
